{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9851c963-3e39-4191-afb6-e86195bc4b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'issue_type': 'other'}, {'issue_type': 'error'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Extract JSON-like content from a string, handling extra or mismatched braces, \n",
    "    and parse it into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): The string containing JSON-like content.\n",
    "\n",
    "    Returns:\n",
    "    dict: Parsed JSON content as a dictionary, or an empty dictionary if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex to find JSON-like content\n",
    "        json_pattern = re.compile(r'{.*?}', re.DOTALL)\n",
    "        match = json_pattern.search(input_string)\n",
    "\n",
    "        if match:\n",
    "            # Handle potential double braces\n",
    "            json_content = match.group()\n",
    "            json_content = json_content.replace('{{', '{').replace('}}', '}')\n",
    "            \n",
    "            # Try parsing the adjusted JSON content\n",
    "            return json.loads(json_content)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Return empty dictionary if no valid JSON is found\n",
    "    return {}\n",
    "\n",
    "# Example usage\n",
    "input_string_1 = \"\"\"\n",
    "Some random text\n",
    "{\n",
    "    \"issue_type\": \"other\"\n",
    "}\n",
    "More text here\n",
    "\"\"\"\n",
    "\n",
    "input_string_2 = \"\"\"\n",
    "Some text with extra braces {{\n",
    "    \"issue_type\": \"error\"\n",
    "}}\n",
    "End of text\n",
    "\"\"\"\n",
    "\n",
    "parsed_json_1 = extract_json_from_string(input_string_1)\n",
    "parsed_json_2 = extract_json_from_string(input_string_2)\n",
    "\n",
    "parsed_json_1, parsed_json_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe1b8d6-fa15-43f7-8944-6f8a285b1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL_DIR = \"/data/luomingkai/issue/models/Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "BASE_MODEL_DIR = \"/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bb1c1-eff0-4691-8f39-5a06046eedc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 19:15:55 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 04-08 19:15:55 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 04-08 19:15:56 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-08 19:15:56 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:15:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:15:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 04-08 19:15:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 04-08 19:15:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 04-08 19:15:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:15:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 04-08 19:15:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:15:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m INFO 04-08 19:15:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:15:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 04-08 19:15:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 04-08 19:15:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m WARNING 04-08 19:15:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 04-08 19:15:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 04-08 19:15:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 04-08 19:15:59 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f664874da00>, local_subscribe_port=35793, remote_subscribe_port=None)\n",
      "INFO 04-08 19:15:59 model_runner.py:1056] Starting to load model /mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:15:59 model_runner.py:1056] Starting to load model /mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct...\n",
      "INFO 04-08 19:15:59 model_runner.py:1056] Starting to load model /mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct...\n",
      "INFO 04-08 19:15:59 model_runner.py:1056] Starting to load model /mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/Qwen2.5-14B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565dbb861b840f1b5ae999179cfe825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m INFO 04-08 19:17:50 model_runner.py:1067] Loading model weights took 6.9459 GB\n",
      "INFO 04-08 19:17:50 model_runner.py:1067] Loading model weights took 6.9459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:17:50 model_runner.py:1067] Loading model weights took 6.9459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:17:50 model_runner.py:1067] Loading model weights took 6.9459 GB\n",
      "INFO 04-08 19:18:14 distributed_gpu_executor.py:57] # GPU blocks: 15551, # CPU blocks: 5461\n",
      "INFO 04-08 19:18:14 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 7.59x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:18:25 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-08 19:18:26 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-08 19:18:26 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-08 19:19:16 model_runner.py:1523] Graph capturing finished in 51 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m INFO 04-08 19:19:16 model_runner.py:1523] Graph capturing finished in 51 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m INFO 04-08 19:19:16 model_runner.py:1523] Graph capturing finished in 51 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-08 19:19:16 model_runner.py:1523] Graph capturing finished in 51 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████| 1/1 [00:03<00:00,  3.59s/it, est. speed input: 10.31 toks/s, output: 61.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nTell me something about large language models.<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"Large language models, like the one you're interacting with now, are sophisticated artificial intelligence systems designed to understand and generate human-like text based on the vast amount of text data they've been trained on. These models are characterized by their enormous size, typically measured in billions of parameters, which allows them to capture complex patterns and nuances in natural language.\\n\\nOne key aspect of these models is their ability to perform a wide range of tasks without being specifically programmed for each one. This versatility comes from their training process, where they learn from diverse datasets that include books, articles, websites, and more. As a result, they can generate text, answer questions, translate languages, write stories, and even provide explanations or summaries of topics.\\n\\nHowever, large language models also come with challenges, such as ensuring the accuracy and fairness of their outputs, managing computational resources required for their operation, and addressing privacy concerns related to the data used in their training.\\n\\nOverall, large language models represent a significant advancement in the field of artificial intelligence and continue to push the boundaries of what machines can do in understanding and generating human language.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1992350)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992349)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1992351)\u001b[0;0m INFO 04-09 01:19:52 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 04-09 01:19:52 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 04-09 01:19:52 multiproc_worker_utils.py:240] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7,4,5,6\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "\n",
    "# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "\n",
    "# Input the model name or path. Can be GPTQ or AWQ models.\n",
    "model = LLM(model=BASE_MODEL_DIR, tensor_parallel_size=4)\n",
    "\n",
    "# Prepare your prompts\n",
    "prompt = \"Tell me something about large language models.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# generate outputs\n",
    "outputs = model.generate([text], sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8793b6bd-5b1e-4a06-8df9-f2b87d742dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                           | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████| 1/1 [00:02<00:00,  2.02s/it, est. speed input: 19.29 toks/s, output: 63.32 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A large language model is an artificial intelligence system designed to understand and generate human-like text based on the patterns it has learned from vast amounts of textual data. These models are typically trained using deep learning techniques on diverse datasets that can include books, articles, websites, and other written content. By processing this extensive corpus, they learn to predict the likelihood of certain words following others, which allows them to generate coherent responses to a wide variety of prompts or questions. Large language models are used in many applications such as chatbots, automated customer service, content creation, and more, providing capabilities that range from simple text completion to complex dialogue systems.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qwen_output(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages_list,\n",
    "    max_input_length=4096,\n",
    "    max_tokens=512,\n",
    "):\n",
    "    text_list = []\n",
    "    for messages in messages_list:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        text_list.append(text)\n",
    "        \n",
    "    # sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "    # sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "    sampling_params = SamplingParams(temperature=0.3, top_p=1.0, repetition_penalty=1.05, max_tokens=max_tokens)\n",
    "    \n",
    "\n",
    "    outputs = model.generate(text_list, sampling_params)\n",
    "    \n",
    "    # Print the outputs.\n",
    "    responses = []\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "        responses.append(generated_text)\n",
    "\n",
    "    return responses\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model.\"}\n",
    "]\n",
    "get_qwen_output(model, tokenizer, [messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795181ed-b35a-4a43-89c6-8a8e5d2d69f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uesr: RainBoltz\n",
      "Title: No output displayed or it gets stuck - OpenCV issue\n",
      "Body: my terminal isn't responding after executed the command below: `./build/examples/openpose/rtpose.bin --imagelevel 1 --netcaffeopenpose.sh` , which was provided by official\n",
      "label: deployment\n",
      "author_association: NONE\n",
      "comment_list: [('bigmoumou', 'NONE', 'I have the same problem too\\r\\n\\r\\nIs there any solution ?\\r\\n\\r\\n![help](https://cloud.githubusercontent.com/assets/16252975/25697753/cddcfe9e-30ee-11e7-82a2-2af97eb8460b.png)\\r\\n\\r\\n'), ('Shawnroom', 'NONE', 'I have the same issue, and wonder if there is any solution.'), ('gineshidalgo99', 'MEMBER', 'Sorry to hear that, we are working on fixing that error. We think it is due to OpenCV compiled with Qt or different visualization support.\\r\\n\\r\\nMeanwhile, you can make it work by:\\r\\n1. Completely uninstalling your current OpenCV version.\\r\\n2. Installing the default OpenCV from the Ubuntu repository: `apt-get install libopencv-dev`, or alternatively compiling OpenCV without Qt support.\\r\\nLet us know if any of these solutions do not work either. Note that you must manually remove any existing OpenCV version and run `make clean` in both 3rdparty/caffe and the OpenPose main folder\\r\\n\\r\\nA couple quick questions:\\r\\n1. Does the same happen when processing video?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480`\\r\\n2. Does it work if no output is displayed?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480 --write_images results_folder/ --no_display`\\r\\n\\r\\nWe will notify this thread once we solve this issue with OpenCV. Thanks!\\r\\n\\r\\nUPDATED: This issue should be fixed now. See my next response on this issue thread to see solution.'), ('RainBoltz', 'NONE', 'thanks!!! i solved it by switching the opencv version to 2.4.13\\r\\n(btw, simply execute `sudo apt-get install libopencv-dev` doesnt work for me,\\r\\nso i recompiled and install the total opencv then it worked)\\r\\n\\r\\nand for the questions above, the same issue DO occurs when processing video...\\r\\n\\r\\ni have to say this really works amazingly!! great work:)'), ('gineshidalgo99', 'MEMBER', 'This issue has finally being solved (at least for some people after our last commit).\\r\\n\\r\\nYou can `git pull` or re-download the latest version of the library and re-compile it.\\r\\n\\r\\nTo re-compile it in case you just do `git pull`:\\r\\n`make clean && cd 3rdparty/caffe && make clean && make distribute -j8 && cd ../.. && make all -j8`\\r\\n\\r\\nIn case you re-download it, just delete the old version and follow the installation steps again.\\r\\n\\r\\nPlease, reopen this post and post again if it keeps happening (this message is for everybody).'), ('taxuezcy', 'NONE', 'how can i build without display instead of giving arg when running @gineshidalgo99 '), ('saumyasaxenaa', 'NONE', 'Hi\\r\\nI am facing the same issue on Ubuntu 18.04 with Opencv 4.1.1. The screen opens up but blacks out with no output. I am facing this for images and videos.\\r\\nAny help would be appreciated '), ('MichaelGMoore', 'NONE', 'This is happening to me on Ubuntu 16.04 using sudo apt-get install libopencv-dev. Any help appreciated.'), ('shash29-dev', 'NONE', 'same problem. On Linux.\\r\\nwindow dont respond and crashes with following output:\\r\\nF0803 21:38:10.637179 20637 cudnn.hpp:128] Check failed: status == CUDNN_STATUS_SUCCESS (3 vs. 0)  CUDNN_STATUS_BAD_PARAM\\r\\n*** Check failure stack trace: ***\\r\\n    @     0x7f9bd7b455cd  google::LogMessage::Fail()\\r\\n    @     0x7f9bd7b47433  google::LogMessage::SendToLog()\\r\\n    @     0x7f9bd7b4515b  google::LogMessage::Flush()\\r\\n    @     0x7f9bd7b47e1e  google::LogMessageFatal::~LogMessageFatal()\\r\\n    @     0x7f9bd7115ec0  caffe::CuDNNConvolutionLayer<>::Reshape()\\r\\n    @     0x7f9bd71eab62  caffe::Net<>::Init()\\r\\n    @     0x7f9bd71edb60  caffe::Net<>::Net()\\r\\n    @     0x7f9bd9102468  op::NetCaffe::initializationOnThread()\\r\\n    @     0x7f9bd91974cc  op::addCaffeNetOnThread()\\r\\n    @     0x7f9bd919879f  op::PoseExtractorCaffe::netInitializationOnThread()\\r\\n    @     0x7f9bd919db00  op::PoseExtractorNet::initializationOnThread()\\r\\n    @     0x7f9bd9194111  op::PoseExtractor::initializationOnThread()\\r\\n    @     0x7f9bd918e9b1  op::WPoseExtractor<>::initializationOnThread()\\r\\n    @     0x7f9bd9119aa1  op::Worker<>::initializationOnThreadNoException()\\r\\n    @     0x7f9bd9119be0  op::SubThread<>::initializationOnThread()\\r\\n    @     0x7f9bd911caf8  op::Thread<>::initializationOnThread()\\r\\n    @     0x7f9bd911ccfd  op::Thread<>::threadFunction()\\r\\n    @     0x7f9bd8a44b8e  (unknown)\\r\\n    @     0x7f9bd817c6ba  start_thread\\r\\n    @     0x7f9bd849951d  clone\\r\\n    @              (nil)  (unknown)\\r\\nAborted (core dumped)\\r\\n')]\n",
      "uesr: gineshidalgo99\n",
      "Title: Release version works but debug version does not - CUDA (7 vs. 0): too many resources requested\n",
      "Body: issue summary @zhaishengfu issue #13: > when i compile using debug mode, there are still errors with: terminate called after throwing an instance of 'std::runtimerelease -a` on ubuntu): distributor id: ubuntu description: ubuntu 14.04.3 lts release: 14.04 codename: trusty ** (`nvidia-smi`): gtx-1070 compiler (`gcc --version` on ubuntu): and my cpu is 4 core\n",
      "label: Error\n",
      "author_association: MEMBER\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', '@zhaishengfu It should be fixed now. Please post again if the error persists. Thanks!')]\n",
      "uesr: ShihanWang\n",
      "Title: How can I use OpenPose in another project?\n",
      "Body: issue summary my test project: i just copy the file rtpose.cpp to the project.then i write a cmakelists.txt: cmakerequired(version 2.8) project(test) set(cmakecompiler \"g++\") set(cmaketype debug) set(cmakeflags \"-std=c++0x\") finddirectories( ${cudadirs} /home/wsh/projects/openpose/include /home/wsh/projects/openpose/3rdparty/caffe/include ) addlinklibs} /home/wsh/projects/openpose/build/lib/libopenpose.so /home/wsh/projects/openpose/3rdparty/caffe/build/lib/libcaffe.so ) i can run the examples successfully in openpose, but in my test project, it can not. type of issue help wanted openpose output (if any) /home/wsh/projects/openpose/include/openpose/experimental/face/faceextractor.hpp:48:29: error: ‘resizeandmergecaffe’ was not declared in this scope std::sharedptr spnmscaffe; ......\n",
      "label: question\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'This is from a Makefile file where I used the OpenPose binaries in a different project, I think you forgot to add the USE_CAFFE define in your cmake, you can get the required defines from my Makefile (there might be some duplication since I did it quickly, but it is working):\\r\\n\\r\\nPerform `make distribute -j8` on the OpenPose folder to get in `./distribute` the `include` and `lib` folders. The Caffe `lib` and `include` are already in `3rdparty/caffe/distribute` (used by OpenPose).\\r\\n\\r\\n### OpenPose (assuming its `include` and `lib` are located in ./3rdparty/openpose)\\r\\n`-Wl,-rpath=./3rdparty/openpose/lib -Wl,-Bdynamic -L./3rdparty/openpose/lib/ -lopenpose -DUSE_CAFFE`\\r\\n\\r\\n### Caffe (assuming its `include` and `lib` are located in ./3rdparty/caffe/distribute)\\r\\n`-Wl,-rpath=./3rdparty/caffe/distribute/lib -Wl,-Bdynamic -L./3rdparty/caffe/distribute/lib/ -lcaffe -DUSE_CUDNN`\\r\\n\\r\\n### OpenCV (add more OpenCV flags if you need them)\\r\\n`-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_contrib -lopencv_calib3d`\\r\\n\\r\\n### CUDA, something like:\\r\\n`-I/usr/local/cuda-8.0/include/ -L/usr/local/cuda-8.0/lib64 -lcudart -lcublas -lcurand`\\r\\n\\r\\n### 3rdparty that Caffe uses\\r\\n`-lcudnn -lglog -lgflags -lboost_system -lboost_filesystem -lm -lboost_thread`\\r\\n\\r\\n### Other required\\r\\n`-pthread -fPIC -std=c++11 -fopenmp`\\r\\n\\r\\n### Optimization flags (optional)\\r\\n`-DNDEBUG -O3 -march=native`\\r\\n\\r\\nLet me know whether it works'), ('ShihanWang', 'NONE', 'Thank you very much.  I have solved the problem and I found that:\\r\\n1. In my CMakeLists.txt,  I need to change the Caffe directory:  SET(caffe_DIR ${OpenPose_DIR}/3rdparty/caffe/distribute);\\r\\n2. I delete \"#ifdef USE_CAFFE\"  in some files named as xxCaffe.hpp in openpose/include/.\\r\\nThen I make openpose again and everything is OK.  Without the 2nd step, I will have the same problem.But I do not know what is the effect of my change, maybe I just need to add USE_CAFFE in my CMakeLists.txt.'), ('gineshidalgo99', 'MEMBER', \"Thank you for posting it! Closing this then.\\r\\n\\r\\nEDITED: As an extra detail, instead of using this CMake example to construct the cmake file, I'd rather consider file example in the doc/installation_cmake.md document itself.\"), ('yorgosk', 'NONE', 'Hello!\\r\\n\\r\\nI am trying to use my current OpenPose installation for the compilation of another project (for the https://github.com/stevenjj/openpose_ros to be specific). I have build and tested my OpenPose installation as the documentation instructs me and everything seems fine, however I cannot do ```make distribute```. I have searched the documentation for a solution, but I am probably missing something.\\r\\nSince the instructions above are about one and a half year old @gineshidalgo99 can you please give me an update?\\r\\n\\r\\nThanks!'), ('gineshidalgo99', 'MEMBER', 'Hi, I believe doc/install.md has an explanation on how to include the library through CMake\\r\\n\\r\\nYou talk about `make distribute`, could you quickly send me a link to the instructions (in current master OP) where it says `make distribute`? So I can update that part.'), ('yorgosk', 'NONE', \"Okay, I'll check doc/install.md again.\\r\\n\\r\\nThe instructions in current master do not mention anything about make distribute, as far as I have read them. I first read about it here and I figured that since it used to be an option but apparently it is no more, I better ask you for a pointer to the thing that you substituted it with.\"), ('gineshidalgo99', 'MEMBER', 'OK yeah, this is based on the old basic installer, the new installer way (using CMake) provides much higher configuration settings, following how big libraries do it (e.g., OpenCV, Caffe, etc.).\\r\\n\\r\\nSo please, check https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac for all the details.'), ('burnzzz', 'NONE', 'https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac  is dead link.')]\n",
      "uesr: ufohuang98\n",
      "Title: Can this model tracking people or hand movement\n",
      "Body: i want to use this to make a gesture demo to control application or other iot. does it possible?\n",
      "label: other\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'Not for now, but we are planing to (as a LONG-term goal).\\r\\n\\r\\nWe have them both as pending extensions in our wait-list (hands should be added within 1-2 months, tracking we do not have an idea yet).\\r\\n\\r\\nThere are some methods for tracking, in case you are interested. You can take a look to #15.\\r\\n\\r\\nLet me know if you have any other question.'), ('ufohuang98', 'NONE', 'Thank you for your reply！'), ('heurainbow', 'NONE', 'I am going to implement a multi-person pose tracker. I find the hand tracker already implemented is quite helpful. However, I wonder how to acquire (or update) frame ID in class HandDetector. Can I simply make mCurrentId++ when calling updateTracker()? I am not familiar with the multi-threading framework, and I am not sure if this is the right way. @gineshidalgo99 '), ('gineshidalgo99', 'MEMBER', 'The hand tracker is not finished yet, but anyway it is meant for different purposes than tracking the same person across frames.\\r\\nI think the easiest way is to get the frame ID is by checking the (include/core/) Datum that is shared among the multi-threads. In particular, its `id` field. Since you will have to use the shared Datum anyway, using that ID should be the easiest. @heurainbow '), ('heurainbow', 'NONE', \"@gineshidalgo99 I wonder how a webcam producer or a video producer keep the frame order sequentially in a multi-gpu case. Since each frame is processed by one gpu, there is no guarantee that each frame is processed in order. But the gui does show images in order and I couldn't find the reason in the code.\"), ('gineshidalgo99', 'MEMBER', 'There is a internal buffer to keep the order'), ('heurainbow', 'NONE', '@gineshidalgo99 If possible, please show me which file or code I should refer to, and please give me a brief explanation.'), ('gineshidalgo99', 'MEMBER', 'This is the file, it simply uses the Datum ID to sort the frames\\r\\nhttps://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/include/openpose/thread/wQueueOrderer.hpp'), ('seovchinnikov', 'NONE', '@gineshidalgo99 what methods for tracking did you look at? \\r\\n'), ('gineshidalgo99', 'MEMBER', '@seovchinnikov We are currently working in an expensive but accurate body recognition algorithm and in a basic bust fast LK tracking algorithm. We are trying to add them by the end of the year. Thanks'), ('seovchinnikov', 'NONE', \"@gineshidalgo99 thank you,\\r\\nAs far as I understand, will body recognition algorithm' be based on basic body keypoints detector or will it be inderpendent end-to-end part to complement openpose library (smth like https://github.com/bendidi/Tracking-with-darkflow)?\\r\\n\\r\\n\"), ('gineshidalgo99', 'MEMBER', \"It'd be an extra component, that can be enabled (slower but with temporary information) or disabled (to keep the current OpenPose behavior)\"), ('superying', 'NONE', '@heurainbow Are there any progress in your multi-person pose tracker? Do you implement it base on the key points from openpose?')]\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/home/luomingkai/workspace/issue_llm/issue_classify/issue_with_comments_framework/matched_results_test.json\"\n",
    "data_path = \"/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/matched_results_test_modify_other_update.json\"\n",
    "with open(data_path, encoding=\"utf-8\") as fp:\n",
    "    issue_data = json.load(fp)\n",
    "    for idx, data in enumerate(issue_data):\n",
    "        # print(issue_data)\n",
    "        uesr, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "        comment_list = data[\"comments_list\"]\n",
    "        if len(comment_list) > 0:\n",
    "            comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "\n",
    "        print(f\"uesr: {uesr}\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Body: {body}\")\n",
    "        print(f\"label: {label}\")\n",
    "        print(f\"author_association: {author_association}\")\n",
    "        print(f\"comment_list: {comment_list}\")\n",
    "        \n",
    "        \n",
    "        # print()\n",
    "        if idx > 2:\n",
    "            break\n",
    "        # print(title, description, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3f081-f81e-4b27-a382-785a7e6805d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48483e6-5db1-473e-a7e6-48a835b628c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_issue_prompt = r\"\"\"\n",
    "### **Role**  \n",
    "You are an expert in GitHub repository analysis. Your task is to classify a given GitHub Issue into one of the following categories: **error**, **performance**, **deployment**, **question**, or **other**.\n",
    "\n",
    "Analyze the conversation context thoroughly and determine the correct classification. If the issue is unrelated to the repository’s functionality or purpose, it must be categorized as **other**.\n",
    "\n",
    "### **Issue Categories**  \n",
    "\n",
    "- **error**:  \n",
    "  Problems directly caused by the repository’s code, configuration, or inherent incompatibilities within the repository. For example:\n",
    "  - Runtime errors.\n",
    "  - Exceptions.\n",
    "  - Failures in repository code execution.  \n",
    "\n",
    "- **performance**:  \n",
    "  Issues where the repository’s code or configuration leads to:\n",
    "  - Slow execution times.\n",
    "  - Resource bottlenecks (e.g., CPU, GPU, memory, or storage).\n",
    "  - Inefficient resource usage (e.g., excessive memory or storage consumption).\n",
    "\n",
    "- **deployment**:  \n",
    "  Issues arising during the installation or deployment process that are specifically caused by:\n",
    "  - Defects in the repository code.\n",
    "  - Incomplete or inadequate documentation.\n",
    "  - Configuration problems stemming from the repository.  \n",
    "\n",
    "  *Note*: If the issue is caused by user errors (e.g., outdated dependencies, incorrect tools) or hardware/environmental limitations, it should be categorized as **question**.\n",
    "\n",
    "- **question**:  \n",
    "  Issues originating from:\n",
    "  - Misunderstandings or failure to follow documentation.\n",
    "  - Incorrect usage of the repository’s features.\n",
    "  - Local environment misconfigurations not caused by the repository code or documentation.  \n",
    "\n",
    "  *Note*: This category also includes:\n",
    "  - User questions about usage scenarios.\n",
    "  - Discussions seeking clarification or additional guidance.\n",
    "\n",
    "- **other**:  \n",
    "  Issues outside the predefined categories, including:\n",
    "  1. Topics unrelated to the repository’s functionality.\n",
    "  2. Feature requests or discussions beyond the repository’s purpose.\n",
    "  3. Suggestions for improving documentation, usability, or community processes.\n",
    "\n",
    "\n",
    "### **Process**  \n",
    "**The process must be strictly executed, and the output must adhere to the defined JSON format.**\n",
    "\n",
    "1. **Analyze the Conversation**:  \n",
    "   Review the entire conversation context and evaluate the issue based on the evidence provided.\n",
    "\n",
    "2. **Determine the Final Classification**:  \n",
    "   Assign the issue to one of the five categories:\n",
    "   - **error**\n",
    "   - **performance**\n",
    "   - **deployment**\n",
    "   - **question**\n",
    "   - **other**\n",
    "\n",
    "3. **Output**:  \n",
    "   Generate a single JSON object as the result, formatted as follows:  \n",
    "   ```json\n",
    "   {\"issue_type\": \"<error|performance|deployment|question|other>\"}\n",
    "   ```\n",
    "\n",
    "### **Examples**  \n",
    "\n",
    "#### Example 1: Error\n",
    "*Conversation:*  \n",
    "- \"Running model.fit() raises a KeyError related to missing labels in the dataset.\"  \n",
    "- \"We’ll patch data_loader.py to handle missing labels.\"  \n",
    "- \"The fix is merged. Let us know if it resolves the problem.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"error\"}\n",
    "```\n",
    "\n",
    "#### Example 2: Question\n",
    "*Conversation:*  \n",
    "- \"I tried running the code, but the output looks weird. Is this a bug?\"  \n",
    "- \"Have you verified if the input data matches the format described in the README?\"  \n",
    "- \"I missed the formatting instructions. After fixing the input, it works fine.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"question\"}\n",
    "```\n",
    "\n",
    "#### Example 3: Performance\n",
    "*Conversation:*  \n",
    "- \"The code runs much slower than expected for large datasets. Is there a way to optimize?\"  \n",
    "- \"We could optimize the data processing step using multi-threading.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"performance\"}\n",
    "```  \n",
    "\n",
    "#### Example 4: Deployment\n",
    "*Conversation:*  \n",
    "- \"The installation instructions don't mention the CUDA version required. The code fails on my setup.\"  \n",
    "- \"We’ll update the documentation to specify the supported CUDA versions.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"deployment\"}\n",
    "```  \n",
    "\n",
    "#### Example 5: Other\n",
    "*Conversation:*  \n",
    "- \"It would be great if this repository supported visualization tools for monitoring model training.\"  \n",
    "- \"Thanks for the suggestion. We’ll consider this for future updates.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"other\"}\n",
    "```  \n",
    "\"\"\"\n",
    "\n",
    "# 仅使用第一条issue的模版\n",
    "question_prompt = \"\"\"\n",
    "````\n",
    "*Conversation*:\n",
    "- {}: ### Title: \"{}\" ### Body: \"{}\"\n",
    "````\n",
    "\"\"\"\n",
    "\n",
    "# 使用comment的版本\n",
    "question_comment_prompt = \"\"\"\n",
    "*Conversation*:\n",
    "- \"{} {}\"\n",
    "\"\"\"\n",
    "\n",
    "comment_prompt = \"\"\"\n",
    "- \"{}\"\n",
    "\"\"\"\n",
    "\n",
    "ROLE_MAP_BEGIN = {\n",
    "    \"NONE\": \"ISSUE RAISER\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}\n",
    "\n",
    "ROLE_MAP_COMMENT = {\n",
    "    \"NONE\": \"Commenter\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9993b69-c200-4ea9-bdf4-c610e1ba7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DESC = {\n",
    "    \"CMU-Perceptual-Computing-Lab/openpose\": {\n",
    "        \"description\": \"OpenPose is a real-time multi-person keypoint detection library developed by the Carnegie Mellon Perceptual Computing Lab. It estimates human body, face, hands, and foot keypoints from single images, providing 2D real-time multi-person keypoint detection, including 15, 18, or 25-keypoint body/foot keypoint estimation, 2x21-keypoint hand keypoint estimation, and 70-keypoint face keypoint estimation. Additionally, it offers 3D real-time single-person keypoint detection and a calibration toolbox. OpenPose is compatible with various operating systems, including Ubuntu, Windows, and macOS, and supports CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only versions.\",\n",
    "        \"url\": \"https://github.com/CMU-Perceptual-Computing-Lab/openpose\"\n",
    "    },\n",
    "    \"CorentinJ/Real-Time-Voice-Cloning\": {\n",
    "        \"description\": \"Real-Time Voice Cloning is a Python-based tool that enables the cloning of voices in real-time. It utilizes deep learning models to synthesize speech that mimics a target voice, requiring only a few seconds of audio from the desired speaker. The repository provides code and instructions to train and use the voice cloning system.\",\n",
    "        \"url\": \"https://github.com/CorentinJ/Real-Time-Voice-Cloning\"\n",
    "    },\n",
    "    \"JaidedAI/EasyOCR\": {\n",
    "        \"description\": \"EasyOCR is an open-source Optical Character Recognition (OCR) library that supports over 80 languages. It is designed to be easy to use and provides accurate text recognition from images. The library is built on PyTorch and offers pre-trained models for various languages and scripts.\",\n",
    "        \"url\": \"https://github.com/JaidedAI/EasyOCR\"\n",
    "    },\n",
    "    \"deepfakes/faceswap\": {\n",
    "        \"description\": \"Faceswap is a deep learning-based tool for face-swapping in images and videos. It allows users to train models to swap faces between different subjects, providing a platform for experimenting with deepfake technology. The repository includes code for training and using the face-swapping models.\",\n",
    "        \"url\": \"https://github.com/deepfakes/faceswap\"\n",
    "    },\n",
    "    \"deezer/spleeter\": {\n",
    "        \"description\": \"Spleeter is an open-source tool developed by Deezer for source separation in music tracks. It uses deep learning models to separate audio into stems, such as vocals and accompaniment, enabling applications like karaoke and remixing. The repository provides pre-trained models and code for audio source separation.\",\n",
    "        \"url\": \"https://github.com/deezer/spleeter\"\n",
    "    },\n",
    "    \"dusty-nv/jetson-inference\": {\n",
    "        \"description\": \"Jetson Inference is a collection of deep learning inference samples and models for NVIDIA Jetson devices. It includes code for image classification, object detection, and segmentation, optimized for Jetson hardware. The repository provides pre-trained models and examples to demonstrate the capabilities of Jetson devices in AI applications.\",\n",
    "        \"url\": \"https://github.com/dusty-nv/jetson-inference\"\n",
    "    },\n",
    "    \"iperov/DeepFaceLab\": {\n",
    "        \"description\": \"DeepFaceLab is a deep learning tool for creating deepfakes, focusing on face-swapping in videos. It provides a comprehensive set of tools for training and applying deep learning models to perform face-swapping tasks. The repository includes code for data preparation, model training, and face-swapping applications.\",\n",
    "        \"url\": \"https://github.com/iperov/DeepFaceLab\"\n",
    "    },\n",
    "    \"junyanz/pytorch-CycleGAN-and-pix2pix\": {\n",
    "        \"description\": \"This repository provides PyTorch implementations of CycleGAN and pix2pix, two popular models for image-to-image translation tasks. CycleGAN enables image translation without paired examples, while pix2pix requires paired images for training. The repository includes code and pre-trained models for various image translation tasks.\",\n",
    "        \"url\": \"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\"\n",
    "    },\n",
    "    \"mozilla/TTS\": {\n",
    "        \"description\": \"Mozilla TTS is an open-source Text-to-Speech (TTS) engine that aims to make speech synthesis more accessible. It provides implementations of state-of-the-art TTS models, including Tacotron and FastSpeech, and supports training on custom datasets. The repository includes code for training and using TTS models.\",\n",
    "        \"url\": \"https://github.com/mozilla/TTS\"\n",
    "    },\n",
    "    \"streamlit/streamlit\": {\n",
    "        \"description\": \"Streamlit is an open-source app framework for Machine Learning and Data Science projects. It allows users to create interactive web applications for data analysis and visualization with minimal code. The repository provides the core framework and examples for building Streamlit applications.\",\n",
    "        \"url\": \"https://github.com/streamlit/streamlit\"\n",
    "    },\n",
    "    \"microsoft/recommenders\": {\n",
    "        \"description\": \"Recommenders is a project under the Linux Foundation of AI and Data. This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail learnings on five key tasks: preparing and loading data for each recommendation algorithm, building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares (ALS) or eXtreme Deep Factorization Machines (xDeepFM), evaluating algorithms with offline metrics, tuning and optimizing hyperparameters for recommendation models, and operationalizing models in a production environment on Azure. Several utilities are provided to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications.\",\n",
    "        \"url\": \"https://github.com/recommenders-team/recommenders\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a23fcb-bb7c-4a19-9e4e-337671f8f7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 808/1933 [17:49<23:32,  1.26s/it, est. speed input: 1140.05 toks/s, output: 56.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 19:37:34 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1336/1933 [27:24<10:10,  1.02s/it, est. speed input: 1233.76 toks/s, output: 59.95 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 19:47:09 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|██████▉  | 1477/1933 [29:38<07:58,  1.05s/it, est. speed input: 1268.83 toks/s, output: 61.49 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 19:49:23 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:44<00:00,  1.11s/it, est. speed input: 1389.06 toks/s, output: 70.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.4994122231327932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.88      0.61       251\n",
      " performance       0.32      0.65      0.43        60\n",
      "  deployment       0.36      0.75      0.48       165\n",
      "    question       0.78      0.57      0.66       921\n",
      "       other       0.88      0.53      0.66       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.56      0.67      0.57      1933\n",
      "weighted avg       0.72      0.62      0.63      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 814/1933 [17:44<21:19,  1.14s/it, est. speed input: 1145.50 toks/s, output: 56.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 20:13:33 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1341/1933 [27:23<10:26,  1.06s/it, est. speed input: 1234.37 toks/s, output: 59.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 20:23:12 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1479/1933 [29:36<07:03,  1.07it/s, est. speed input: 1270.59 toks/s, output: 60.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 20:25:25 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:47<00:00,  1.11s/it, est. speed input: 1387.35 toks/s, output: 68.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.4977815817862815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.46      0.85      0.59       251\n",
      " performance       0.33      0.68      0.45        60\n",
      "  deployment       0.35      0.73      0.48       165\n",
      "    question       0.78      0.59      0.67       921\n",
      "       other       0.90      0.51      0.66       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.67      0.57      1933\n",
      "weighted avg       0.72      0.62      0.63      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 807/1933 [17:44<23:37,  1.26s/it, est. speed input: 1143.49 toks/s, output: 55.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 20:49:42 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1342/1933 [27:18<08:51,  1.11it/s, est. speed input: 1236.63 toks/s, output: 60.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 20:59:16 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1485/1933 [29:38<10:43,  1.44s/it, est. speed input: 1269.93 toks/s, output: 61.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 21:01:36 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:32<00:00,  1.10s/it, est. speed input: 1397.19 toks/s, output: 69.95 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.5046034576890172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.88      0.61       251\n",
      " performance       0.33      0.67      0.44        60\n",
      "  deployment       0.37      0.77      0.50       165\n",
      "    question       0.78      0.59      0.67       921\n",
      "       other       0.89      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.68      0.57      1933\n",
      "weighted avg       0.72      0.62      0.64      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1%|▏          | 22/1933 [03:23<3:47:32,  7.14s/it, est. speed input: 245.06 toks/s, output: 1.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 21:11:21 scheduler.py:1483] Sequence group 5984 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 816/1933 [17:47<22:31,  1.21s/it, est. speed input: 1136.02 toks/s, output: 55.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 21:25:42 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1340/1933 [27:28<09:53,  1.00s/it, est. speed input: 1229.50 toks/s, output: 59.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 21:35:23 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1484/1933 [29:47<06:13,  1.20it/s, est. speed input: 1262.83 toks/s, output: 61.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 21:37:42 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [36:02<00:00,  1.12s/it, est. speed input: 1377.81 toks/s, output: 69.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.5020944275879793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.88      0.61       251\n",
      " performance       0.33      0.65      0.44        60\n",
      "  deployment       0.37      0.73      0.49       165\n",
      "    question       0.78      0.60      0.68       921\n",
      "       other       0.88      0.51      0.64       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.56      0.67      0.57      1933\n",
      "weighted avg       0.72      0.62      0.64      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|███▎    | 807/1933 [17:35<1:14:16,  3.96s/it, est. speed input: 1153.04 toks/s, output: 56.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:01:56 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▎  | 1347/1933 [27:10<09:31,  1.02it/s, est. speed input: 1245.79 toks/s, output: 61.12 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:11:31 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1487/1933 [29:26<07:39,  1.03s/it, est. speed input: 1277.67 toks/s, output: 62.30 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:13:47 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:19<00:00,  1.10s/it, est. speed input: 1405.20 toks/s, output: 70.33 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.5014125225316147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.87      0.61       251\n",
      " performance       0.33      0.63      0.43        60\n",
      "  deployment       0.37      0.76      0.49       165\n",
      "    question       0.78      0.59      0.67       921\n",
      "       other       0.89      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.67      0.57      1933\n",
      "weighted avg       0.72      0.62      0.64      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 804/1933 [17:38<30:09,  1.60s/it, est. speed input: 1146.65 toks/s, output: 54.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:37:42 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1339/1933 [27:24<10:29,  1.06s/it, est. speed input: 1232.98 toks/s, output: 60.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:47:28 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1480/1933 [29:39<07:24,  1.02it/s, est. speed input: 1267.10 toks/s, output: 61.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 22:49:43 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:25<00:00,  1.10s/it, est. speed input: 1401.28 toks/s, output: 69.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.49392614559774495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.46      0.86      0.60       251\n",
      " performance       0.34      0.70      0.46        60\n",
      "  deployment       0.36      0.73      0.48       165\n",
      "    question       0.77      0.58      0.66       921\n",
      "       other       0.89      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.56      0.68      0.57      1933\n",
      "weighted avg       0.71      0.62      0.63      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|███▎    | 806/1933 [17:40<1:03:45,  3.39s/it, est. speed input: 1147.50 toks/s, output: 55.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 23:13:31 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▎  | 1344/1933 [27:15<08:55,  1.10it/s, est. speed input: 1240.77 toks/s, output: 60.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 23:23:07 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1481/1933 [29:28<11:32,  1.53s/it, est. speed input: 1275.58 toks/s, output: 61.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 23:25:19 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:32<00:00,  1.10s/it, est. speed input: 1396.60 toks/s, output: 70.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.49991211463176727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.87      0.61       251\n",
      " performance       0.34      0.67      0.45        60\n",
      "  deployment       0.36      0.73      0.48       165\n",
      "    question       0.77      0.59      0.67       921\n",
      "       other       0.89      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.68      0.57      1933\n",
      "weighted avg       0.72      0.62      0.63      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 804/1933 [17:45<24:50,  1.32s/it, est. speed input: 1138.99 toks/s, output: 55.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 23:49:33 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1337/1933 [27:22<09:54,  1.00it/s, est. speed input: 1236.51 toks/s, output: 59.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-08 23:59:09 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|██████▉  | 1478/1933 [29:39<07:59,  1.05s/it, est. speed input: 1269.12 toks/s, output: 61.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 00:01:27 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:41<00:00,  1.11s/it, est. speed input: 1390.74 toks/s, output: 69.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.4950562575698578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.48      0.88      0.62       251\n",
      " performance       0.31      0.67      0.43        60\n",
      "  deployment       0.36      0.75      0.48       165\n",
      "    question       0.77      0.57      0.66       921\n",
      "       other       0.88      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.61      1933\n",
      "   macro avg       0.56      0.68      0.57      1933\n",
      "weighted avg       0.71      0.61      0.63      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 816/1933 [17:39<22:49,  1.23s/it, est. speed input: 1144.30 toks/s, output: 56.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 00:25:35 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▎  | 1344/1933 [27:23<09:37,  1.02it/s, est. speed input: 1235.57 toks/s, output: 60.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 00:35:18 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1481/1933 [29:32<06:54,  1.09it/s, est. speed input: 1271.03 toks/s, output: 61.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 00:37:28 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:55<00:00,  1.11s/it, est. speed input: 1382.28 toks/s, output: 70.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.5051055305455062\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.48      0.87      0.62       251\n",
      " performance       0.33      0.67      0.44        60\n",
      "  deployment       0.36      0.75      0.49       165\n",
      "    question       0.78      0.59      0.67       921\n",
      "       other       0.89      0.52      0.66       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.68      0.58      1933\n",
      "weighted avg       0.72      0.62      0.64      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 805/1933 [17:46<25:22,  1.35s/it, est. speed input: 1137.85 toks/s, output: 55.47 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 01:02:00 scheduler.py:895] Input prompt (34100 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▏  | 1336/1933 [27:25<12:25,  1.25s/it, est. speed input: 1233.53 toks/s, output: 60.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 01:11:39 scheduler.py:895] Input prompt (46503 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████▉  | 1479/1933 [29:40<07:08,  1.06it/s, est. speed input: 1267.97 toks/s, output: 61.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 01:13:54 scheduler.py:895] Input prompt (40618 tokens) is too long and exceeds limit of 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 1933/1933 [35:37<00:00,  1.11s/it, est. speed input: 1393.37 toks/s, output: 69.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.4991873051465983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.47      0.88      0.61       251\n",
      " performance       0.33      0.68      0.44        60\n",
      "  deployment       0.36      0.73      0.48       165\n",
      "    question       0.77      0.58      0.66       921\n",
      "       other       0.90      0.51      0.65       536\n",
      "\n",
      "    accuracy                           0.62      1933\n",
      "   macro avg       0.57      0.68      0.57      1933\n",
      "weighted avg       0.72      0.62      0.63      1933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "TIMES = 10\n",
    "experiments = []\n",
    "# 保存所有实验结果的列表\n",
    "all_reports = []\n",
    "\n",
    "data_path = \"/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/matched_results_test_modify_other_update.json\"\n",
    "result_path = \"/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/qwen2.5_14b_prompt.xlsx\"\n",
    "\n",
    "for t in range(TIMES):\n",
    "    origin_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    join_index = []\n",
    "    with open(data_path, encoding=\"utf-8\") as fp:\n",
    "        issue_data = json.load(fp)\n",
    "        taged_data = issue_data\n",
    "        all_messages = []\n",
    "        for idx, data in enumerate(issue_data):\n",
    "            raise_user, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "    \n",
    "            url = data[\"html_url\"]\n",
    "            match = re.search(r\"github\\.com/([^/]+/[^/]+)\", url)\n",
    "            repository = match.group(1)\n",
    "            repo_desc = REPO_DESC[repository][\"description\"]\n",
    "            \n",
    "            join_index.append(idx)\n",
    "            \n",
    "            comment_list = data[\"comments_list\"]\n",
    "            if len(comment_list) > 0:\n",
    "                comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "    \n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": global_issue_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question_comment_prompt.format(\n",
    "                        title, \n",
    "                        body\n",
    "                        )\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            origin_labels.append(label)\n",
    "    \n",
    "            for user, author_association, body in comment_list:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": comment_prompt.format(\n",
    "                            body\n",
    "                            )\n",
    "                    }\n",
    "                )\n",
    "                    \n",
    "            all_messages.append(messages)\n",
    "        \n",
    "        responses = get_qwen_output(model, tokenizer, all_messages, max_tokens=4096)\n",
    "        for idx, response in enumerate(responses):\n",
    "            label = origin_labels[idx]\n",
    "            cls_result = extract_json_from_string(response)\n",
    "            \n",
    "            # print(f\"label: {label}\")\n",
    "            # print(f\"response: {response}\")\n",
    "            # print(f\"cls_result: {cls_result}\")\n",
    "    \n",
    "            if cls_result.get(\"issue_type\") and isinstance(cls_result.get(\"issue_type\"), str):\n",
    "                cls_result = cls_result.get(\"issue_type\")\n",
    "                pred_labels.append(cls_result)\n",
    "            else:\n",
    "                pred_labels.append(\"other\")\n",
    "\n",
    "\n",
    "    origin_labels = [x.lower() for x in origin_labels]\n",
    "    pred_labels = [x.lower() for x in pred_labels]\n",
    "    tmp_pred_labels = []\n",
    "    for x in pred_labels:\n",
    "        if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "            x = 'other'\n",
    "        tmp_pred_labels.append(x)\n",
    "    pred_labels = tmp_pred_labels\n",
    "    \n",
    "    from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "    \n",
    "    labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "    label_map = {\n",
    "        \"error\": 0, \n",
    "        \"performance\": 1,\n",
    "        \"deployment\": 2,\n",
    "        \"question\": 3,\n",
    "        \"other\": 4\n",
    "    }\n",
    "    final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "    final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "    \n",
    "    report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "    mcc = matthews_corrcoef(final_origin_labels_num, final_pred_labels_num)#ZY\n",
    "    print(\"MCC:\", mcc)#ZY\n",
    "\n",
    "    print(report)\n",
    "    report_dict = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels, output_dict=True)\n",
    "    # 将字典转为 DataFrame\n",
    "    df_report = pd.DataFrame(report_dict).transpose()\n",
    "    df_report[\"experiment_id\"] = t + 1  # 添加实验编号\n",
    "    df_report.index.name = \"category\"\n",
    "    df_report.reset_index(inplace=True)\n",
    "    \n",
    "    all_reports.append(df_report)\n",
    "    all_reports.append(pd.DataFrame({\"category\": [\"\"]}))  # 添加空行\n",
    "\n",
    "final_df = pd.concat(all_reports, ignore_index=True)\n",
    "final_df.to_excel(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722d6762-a990-4561-92a7-908b29d970e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(all_reports, ignore_index=True)\n",
    "final_df.to_excel(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dac3052-e110-4a4c-b858-f7997f3b1ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cc6bfa99-761f-4d73-a77a-b5dc047a5d3a/zhaoyu/VSCode/LLM/qwen2.5_14b_prompt.xlsx'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ab46f-dd91-45b1-9e8a-ebe7edb3c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 01:19:52 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 假设你的 vllm 模型对象是 `model`\n",
    "del model  # 删除模型对象\n",
    "torch.cuda.empty_cache()  # 清空 GPU 的缓存\n",
    "gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "import sys\n",
    "# 删除 `vllm` 和相关依赖\n",
    "del sys.modules[\"vllm\"]\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44b804-31b1-4bd6-876e-aceeeb2cc0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2403c9-e567-403e-86c1-4a334ba69292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be666c99-00d1-4be0-9606-f990318fc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'question': 1030, 'error': 322, 'other': 298, 'deployment': 242, 'performance': 37, 'feature request': 2, 'feature_request': 2})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.53      0.68      0.59       251\n",
      " performance       0.43      0.27      0.33        60\n",
      "  deployment       0.42      0.62      0.50       165\n",
      "    question       0.69      0.77      0.73       921\n",
      "       other       0.79      0.44      0.57       536\n",
      "\n",
      "    accuracy                           0.64      1933\n",
      "   macro avg       0.57      0.55      0.54      1933\n",
      "weighted avg       0.66      0.64      0.63      1933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all question类别打标之后的\n",
    "from collections import Counter\n",
    "print(Counter(pred_labels))\n",
    "origin_labels = [x.lower() for x in origin_labels]\n",
    "pred_labels = [x.lower() for x in pred_labels]\n",
    "tmp_pred_labels = []\n",
    "for x in pred_labels:\n",
    "    if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "        x = 'other'\n",
    "    tmp_pred_labels.append(x)\n",
    "pred_labels = tmp_pred_labels\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "label_map = {\n",
    "    \"error\": 0, \n",
    "    \"performance\": 1,\n",
    "    \"deployment\": 2,\n",
    "    \"question\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "\n",
    "report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3b29a-d293-4712-8530-3cc32c24170e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af5545-f547-48e2-b6b2-7831dcb4825a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813fac-96ea-42ea-8f02-d892dd5d4119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e439e41-30f0-4fb2-8edd-629f0c190bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46992be-cc44-4259-8bc9-99a1e088e7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15271f9-4900-4e04-81c5-763e514f6087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c40a9d-78e3-4a2b-b6b1-e966422703ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734c23-c8ae-4808-8cf8-020a5bfd6f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
