number,html_url,title,description,true_label,pred_label
936,https://github.com/streamlit/streamlit/issues/936,docs.streamlit.io doesn't have a favicon,"summary docs.streamlit.io doesn't have a favicon as streamlit.io does. tried using favicon from frontend/public/, and found it to be the older ~~ favicon, so i updated that as well.",other,Error
775,https://github.com/iperov/DeepFaceLab/issues/775,"PC crashed, DeepFake error when using autosave","hi, i ran deepfake for two days and just now my pc froze and crashed. i wanted to continue the process with autosave but it shows an error message. i believe that it can be fixable but i don't have the knowledge to do that. can someone please help?",question,question
325,https://github.com/streamlit/streamlit/issues/325,"Streamlit failed to hash an object of type <class 'function'>.,","there were 6 of these:summary mapping demo this demo shows how to use st.deckchart to display geospatial data. streamlit failed to hash an object of type ., more information: to prevent unexpected behavior, streamlit tries to detect mutations in cached objects so it can alert the user if needed. however, something went wrong while performing this check. please file a bug. to stop this warning from showing in the meantime, try one of the following: preferred: modify your code to avoid using this type of object. or add the argument ignore_cache=true to the st.cache decorator.steps to reproduce what are the steps we should take to reproduce the bug: 1. install streamlit on mac os x 10.11 2. run streamlit hello 3. try the mapping demoexpected behavior: no erroractual behavior: see above this may be relevant: is this a regression? nodebug info - streamlit version: streamlit, version 0.47.4 - python version: python 3.7.3 - using conda? pipenv? pyenv? pex? conda 4.7.12 - os version: mac os x 10.11.6 - browser version: google chrome version 77.0.3865.90 (official build) (64-bit)additional information",Error,other
1035,https://github.com/deepfakes/faceswap/issues/1035,Using TensorFlow backend,"i can't use my gpu. encoding: cp936 gitcommits: 2f15597 requirements.txt - typofix gpucudnn: no global version found. check conda packages for conda cudnn gpu0: geforce rtx 2060 gpuactive: gpudriver: 436.30 gpu0: 6144mb osplatform: windows-10-10.0.18362-sp0 oscommand: c:\users\administrator\faceswap/faceswap.py gui pyversion: conda 4.8.3 pyversion: 3.7.7 pyenv: true sysprocessor: amd64 family 23 model 113 stepping 0, authenticamd sys1594141588948/work cryptography==2.9.2 cycler==0.10.0 cytoolz==0.10.1 dask @ file:///tmp/build/80754af9/dask-core1594357566944/work google-auth-oauthlib==0.4.1 google-pasta==0.2.0 grpcio==1.27.2 h5py==2.10.0 idna @ file:///tmp/build/80754af9/idna1594161405741/work imageio-ffmpeg @ file:///home/conda/feedstockartifacts/imageio-ffmpeg1594236160679/work keras==2.2.4 keras-applications @ file:///tmp/build/80754af9/keras-applications1592846084747/work mkl-fft==1.1.0 mkl-random==1.1.1 mkl-service==2.3.0 networkx @ file:///tmp/build/80754af9/networkx1594298234712/work protobuf==3.12.3 psutil==5.7.0 pyasn1==0.4.8 pyasn1-modules==0.2.7 pycparser @ file:///tmp/build/80754af9/pycparser1594392929924/work pyparsing==2.4.7 pyreadline==2.1 pysocks @ file:///c:/ci/pysocks1592841827918/work requests-oauthlib==1.3.0 rsa==4.0 scikit-image==0.16.2 scikit-learn @ file:///c:/ci/scikit-learn1592916958183/work six==1.15.0 tensorboard==2.2.1 tensorboard-plugin-wit==1.6.0 tensorflow==1.15.0 tensorflow-estimator==1.15.1 termcolor==1.1.0 threadpoolctl @ file:///tmp/tmp9twdgx9k/threadpoolctl-2.1.0-py3-none-any.whl toolz==0.10.0 toposort==1.5 tornado==6.0.4 tqdm @ file:///tmp/build/80754af9/tqdmtflow0 astor 0.8.0 py370 brotlipy 0.7.0 py37he7745221 certifi 2020.6.20 py370 chardet 3.0.4 py370 cloudpickle 1.5.0 py0 cycler 0.10.0 py370 dask-core 2.20.0 py0 fastcluster 1.1.26 py37h9b59f540 conda-forge ffmpy 0.2.3 pypi0 gast 0.2.2 py370 google-auth 1.17.2 py2 google-pasta 0.2.0 py0 h5py 2.10.0 py37h5e291fa0 icc1 icu 58.2 ha925a310 imageio 2.9.0 py0 conda-forge intel-openmp 2020.1 216 joblib 0.16.0 py2 keras 2.2.4 0 keras-applications 1.0.8 py0 keras-preprocessing 1.1.0 py0 libpng 1.6.37 h2a8f88b0 libtiff 4.1.0 h56a325e0 markdown 3.1.1 py370 mkl 2020.1 216 mkl-service 2.3.0 py37hb782905fft 1.1.0 py37h45dec08random 1.1.1 py37h47e9c7a1 numpy 1.18.5 py37h65301190 nvidia-ml-py3 7.352.1 pypi0 olefile 0.46 py370 pypi openssl 1.1.1g he774522einsum 3.1.0 py2 pillow 7.2.0 py37hcc1f9831 protobuf 3.12.3 py37h33f27b40 pyasn1 0.4.8 py0 pycparser 2.20 py0 pyopenssl 19.1.0 py0 pyqt 5.9.2 py37h65383351 pysocks 1.7.1 py374 python-dateutil 2.8.1 pyabi 3.7 10 pywin32 227 py37he7745221 qt 5.9.7 vc14h73c81de0 requests-oauthlib 1.3.0 py0 scikit-image 0.16.2 py37h47e9c7a0 scipy 1.5.0 py37h94399190 sip 4.19.8 py37h65383350 sqlite 3.32.3 h2a8f88b0 tensorboard-plugin-wit 1.6.0 pypy37h9f89a44py37h07d23090 termcolor 1.1.0 py370 tk 8.6.10 he7745220 toposort 1.5 py1 tqdm 4.47.0 py0 vc 14.1 h0510ff6runtime 14.16.27012 hf0eaf9b0 wheel 0.34.2 py37inet0 wincertstore 0.2 py371 xz 5.2.5 h62dcd970 zlib 1.2.11 h62dcd970 ================= configs ================== --------- .faceswap --------- backend: nvidia --------- convert.ini --------- [color.colorpaper: true [color.manual1: 0.0 balance3: 0.0 contrast: 0.0 brightness: 0.0 [color.matchblend] type: gaussian distance: 11.0 radius: 5.0 passes: 1 [mask.masksize: 3 passes: 4 threshold: 4 erosion: 0.0 [scaling.sharpen] method: unsharptransparent: false jpgcompresstransparent: false optimize: false gifquality: 75 pnglevel: 3 tifdeflate --------- extract.ini --------- [global] allowdnn] confidence: 50 [detect.mtcnn] minsize: 20 threshold2: 0.7 thresholddfl] batch-size: 8 [mask.vggobstructed] batch-size: 2 --------- gui.ini --------- [global] fullscreen: false tab: extract optionswidth: 30 consoleheight: 20 iconsize: 9 autosavesession: prompt timeout: 120 automodeltype: none maskkernel: 3 maskmask: false icnrawarepadding: false penalizedloss: true lossrate: 5e-05 [model.dflsae] inputdims: 0 encoderdims: 21 multiscalesize: 256 [model.original] lowmem: false [model.realface] inputsize: 128 denseencoder: 128 complexitysize: 128 lowmem: false clipnorm: true nodes: 1024 complexitydecoderdecoderimages: 14 zoomrange: 10 shiftchance: 50 colorab: 8 colorchance: 50 colormax_size: 4",question,other
485,https://github.com/mozilla/TTS/issues/485,Multi-Speaker TTS with Speaker Encoder and GST,"below are colabs notebooks for demo of two models, the demos allow upload of gst samples, upload of wav file to use as a reference for the speaker (synthesize with your own voice). both models were trained using the vctk dataset. all checkpoints, configs and logs are available on the google drive: the models synthesize well, but when changing the sample of extraction of the embeddings the synthesized voice is not very close to the reference voice. perhaps the problem is the speaker encoder. i think we should train with more data and try to use the angular prototypical that achieved state of the art results in the paper _originally posted by @edresson in",other,other
453,https://github.com/streamlit/streamlit/issues/453,Pandas categories support not implemented,"summary streamlit seems to be unable to render pandas series or dataframes which contain , throwing a `notimplementederror: dtype category not understood.` error.steps to reproduce 1. create a streamlit file with contents of 2. run the file with streamlit and view the served result in the browserexpected behavior: `s` is rendered as a table or datatable.actual behavior: an error along the lines of is this a regression? nodebug info - streamlit version: `streamlit, version 0.47.4` - python version: `python 3.7.4` - using conda? pipenv? pyenv? pex? `pip` with `virtualenv` - os version: `ubuntu 18.04.3 lts` running in wsl. - browser version: firefox, `69.0.2 (64-bit)`additional information seems related to #47.",other,Error
89,https://github.com/streamlit/streamlit/issues/89,Investigate bundling renders after calls to setState,react doesn't bundle renders for new delta msgs consider batching writes to the state,other,other
538,https://github.com/mozilla/TTS/issues/538,Using yaml or toml to manage config?,"hi, thoughts on switching to yaml/toml for config management? that way, we won't have to use a custom json dialect to manage comments in config.",other,other
103,https://github.com/microsoft/recommenders/issues/103,Add tests for new notebooks,- [x] - [x] - [x] evaluation - [x] als deep dive,other,other
1025,https://github.com/microsoft/recommenders/issues/1025,[FEATURE] Sequential model deep dive notebook,description do we need a deep dive notebook for sequential model? @leavingseason @miguelgfierro @anargyri expected behavior with the suggested feature other comments,other,other
1356,https://github.com/streamlit/streamlit/issues/1356,SchemaValidationError:,summary i tried to create an `altair_chart` with the following code in anaconda prompt `streamlit run demo.py`steps to reproduce this is the code in the `demo.py` expected behavior: a chart should be displayed in a web browseractual behavior: the web browser displayed this error message and the anaconda prompt generated this message: debug info - streamlit version: 0.5.73 - python version: 3.7.6 - using conda - os version: windows 7 64-bit - browser version: version 80.0.3987.163 (official build) (64-bit) - altair version: 4.1.0,other,Error
296,https://github.com/mozilla/TTS/issues/296,'AttrDict' object has no attribute 'embedding_size',"hello, while testing the trained model i am getting following error. attributeerror: 'attrdict' object has no attribute 'embedding_size'",question,question
413,https://github.com/deepfakes/faceswap/issues/413,Extracted faces are too small,"expected behavior extract faces from frames actual behavior whenever extracting faces, the process takes too long to start and then the extracted faces are too small. example of an extracted face: this is from an 720 video. additionally getting random issues like this c:\users\administrator\faceswap\lib\alignscalars 0% 9/8456 [22:19 astor 0.6.2 bleach 1.5.0 certifi 2018.4.16 py360 conda-forge face-recognition 1.2.2 face-recognition-models 0.3.0 ffmpeg 4.0 hf48ec3a0 hdf5 1.10.1 vc14rt 2017.0.4 h97af9660 [vc14] conda-forge markdown 2.6.11 matplotlib 2.2.2 mkl 2018.0.2 1 mkl0 mkl0 networkx 2.1 numpy 1.14.3 numpy 1.11.3 py36h4a996260 conda-forge pillow 5.1.0 pip 10.0.1 py360 python-dateutil 2.7.2 pytz 2018.4 pywavelets 0.5.2 pyyaml 3.12 scandir 1.5 py360 conda-forge six 1.11.0 py36h4db23103 vs20150 conda-forge wincertstore 0.2 py36h7fe50ca0 [vc14] conda-forge",other,Performance
1946,https://github.com/streamlit/streamlit/issues/1946,Slack notifications for new package version,"not sure if this is going to be helpful but it would be great if we can get slack notifications somewhere that will notify us of any new package versions. while there still may be test failures, at least it'll be an easy way for us to connect the dots without digging too much. this could also be helpful if we pin packages and inform us when we need to determine if we can upgrade/unpin.",other,other
183,https://github.com/iperov/DeepFaceLab/issues/183,Remove or move text instructions on manual extraction,"during manual extraction, the text instructions often get in the way of visually seeing the face, sometimes for 100s if not 1000s of frames. a few options: - remove the instructions and put them as a stdout print statement on the command line - if detection box is moved to top of extraction window, move text instructions to bottom - add a border on top or bottom of extraction window that includes instructions, but doesnt cover actual video any resolution at all would be nice though.",question,Performance
735,https://github.com/deepfakes/faceswap/issues/735,"ERROR    Caught exception in thread: 'training_0'//original type is ok,but any other model cannot succeed","wanqi@amax:~/desktop/faceswap-master$ python faceswap.py train -a faces/source -b faces/lyf -m models/lyf-128/ -p -t realface 05/22/2019 09:57:53 info log level set to: info using tensorflow backend. 05/22/2019 09:57:57 info model a directory: /home/wanqi/desktop/faceswap-master/faces/source 05/22/2019 09:57:57 info model b directory: /home/wanqi/desktop/faceswap-master/faces/lyf 05/22/2019 09:57:57 info training data directory: /home/wanqi/desktop/faceswap-master/models/lyf-128 05/22/2019 09:57:57 info =============================================== 05/22/2019 09:57:57 info - starting - 05/22/2019 09:57:57 info - using live preview - 05/22/2019 09:57:57 info - press 'enter' to save and quit - 05/22/2019 09:57:57 info - press 's' to save model weights immediately - 05/22/2019 09:57:57 info =============================================== 05/22/2019 09:57:58 info loading data, this may take a while... 05/22/2019 09:57:58 info loading model from realface plugin... 05/22/2019 09:58:04 warning no existing state file found. generating. 05/22/2019 09:58:27 info creating new 'realface' model in folder: '/home/wanqi/desktop/faceswap-master/models/lyf-128' 05/22/2019 09:58:28 info loading trainer from original plugin... 05/22/2019 09:58:29 critical error caught! exiting... 05/22/2019 09:58:29 error caught exception in thread: 'trainingscript file ""/home/wanqi/desktop/faceswap-master/scripts/train.py"", line 98, in process file ""/home/wanqi/desktop/faceswap-master/scripts/train.py"", line 123, in endtrainer file ""/home/wanqi/desktop/faceswap-master/plugins/train/trainer/base.py"", line 96, in processopts file ""/home/wanqi/desktop/faceswap-master/plugins/train/trainer/base.py"", line 617, in getreport.2019.05.22.095829236316.log. please verify you are running the latest version of faceswap before reporting 05/22/2019 09:58:28 mainprocess trainingbase verbose leakylu0 ____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________0 predictors debug compiling predictors 05/22/2019 09:58:28 mainprocess trainingbase get1': 0.5, 'beta0 function verbose using dssim loss 05/22/2019 09:58:28 mainprocess trainingbase loss0 loss0 sessionnames debug adding session lossnames: ['totalloss'] 05/22/2019 09:58:28 mainprocess trainingbase addlossnames. (side: 'b', lossloss', 'loss', 'mask0 predictors debug compiled predictors. losses: ['totalloss'] 05/22/2019 09:58:28 mainprocess trainingbase setdata debug setting training data 05/22/2019 09:58:28 mainprocess trainingbase calculateratio debug requested coverage0 coverageratio: 0.625 05/22/2019 09:58:28 mainprocess trainingbase setdata debug set training data: {'alignments': {'a': '/home/wanqi/desktop/faceswap-master/faces/source/alignments.json', 'b': '/home/wanqi/desktop/faceswap-master/faces/lyf/alignments.json'}, 'previewtoflip': false, 'pingpong': false, 'traininglogs': false, 'maskratio': 0.625, 'preview0 0 realface _0 train load0 train load0 pluginimport info loading trainer from original plugin... 05/22/2019 09:58:28 mainprocess trainingbase _size: 64) 05/22/2019 09:58:28 mainprocess trainingbase addbatchsize debug adding session batchsize: 64 05/22/2019 09:58:28 mainprocess trainingbase processopts debug {'alignments': {'a': '/home/wanqi/desktop/faceswap-master/faces/source/alignments.json', 'b': '/home/wanqi/desktop/faceswap-master/faces/lyf/alignments.json'}, 'previewtoflip': false, 'pingpong': false, 'traininglogs': false, 'maskratio': 0.625, 'preview0 required debug true 05/22/2019 09:58:28 mainprocess trainingbase _opts: '{'alignments': {'a': '/home/wanqi/desktop/faceswap-master/faces/source/alignments.json', 'b': '/home/wanqi/desktop/faceswap-master/faces/lyf/alignments.json'}, 'previewtoflip': false, 'pingpong': false, 'traininglogs': false, 'maskratio': 0.625, 'preview0 alignments _0 alignments get0 alignments get0 alignments get0 alignments get0 alignments get0 alignments get0 alignments load debug loading alignments 05/22/2019 09:58:28 mainprocess training0): error: alignments file not found at /home/wanqi/desktop/faceswap-master/faces/source/alignments.json 05/22/2019 09:58:29 mainprocess mainthread train monitor debug thread error detected 05/22/2019 09:58:29 mainprocess mainthread train monitor debug closed monitor 05/22/2019 09:58:29 mainprocess mainthread train endthread critical error caught! exiting... 05/22/2019 09:58:29 mainprocess mainthread multithreading join debug joining threads: 'training' 05/22/2019 09:58:29 mainprocess mainthread multithreading join debug joining thread: 'training0' traceback (most recent call last): file ""/home/wanqi/desktop/faceswap-master/lib/cli.py"", line 110, in executethread file ""/home/wanqi/desktop/faceswap-master/lib/multithreading.py"", line 460, in join file ""/home/wanqi/desktop/faceswap-master/lib/multithreading.py"", line 390, in run file ""/home/wanqi/desktop/faceswap-master/scripts/train.py"", line 149, in training file ""/home/wanqi/desktop/faceswap-master/scripts/train.py"", line 138, in training file ""/home/wanqi/desktop/faceswap-master/scripts/train.py"", line 197, in loadbase.py"", line 51, in _base.py"", line 96, in processopts file ""/home/wanqi/desktop/faceswap-master/plugins/train/trainer/base.py"", line 617, in getx3.4-1968 gitcommits: not found gpucudnn: not found. check conda packages for conda cudnn gpu0: tesla k80, gpu2: tesla k80, gpu4: tesla k80, gpu6: tesla k80, gpu8: tesla k80, gpudevices0, gpu2, gpu4, gpu6, gpu8, gpudriver: 375.66 gpu0: 11439mb, gpu2: 11439mb, gpu4: 11439mb, gpu6: 11439mb, gpu8: 11439mb, gpumachine: x86platform: linux-3.19.0-80-generic-x86release: 3.19.0-80-generic pycondaimplementation: cpython pyvirtualcores: 40 sys64 systflow0 astor 0.7.1 py360 backports 1.0 py360 blas 1.0 mkl bleach 1.5.0 py365 c-ares 1.15.0 h7b6447c3 certifi 2019.3.9 py361 cloudpickle 1.0.0 py0 cudnn 7.3.1 cuda9.00 cytoolz 0.9.0.1 py36h14c39750 dbus 1.13.2 h714fa370 entrypoints 0.2.3 py360 ffmpeg 4.0 hcdf2ecd0 freeglut 3.0.0 hf484d3e1 gast 0.2.2 py360 gmp 6.1.2 h6c8ec710 grpcio 1.16.1 py36hf8bcb031 gstreamer 1.14.0 hb453b480 harfbuzz 1.8.8 hffaf4a10 html5lib 0.9999999 py361 imageio 2.5.0 py360 ipython 7.1.1 py36h39e3cacgenutils 0.2.0 py360 jasper 2.0.14 h07fcdf60 jinja2 2.10 py360 jpeg 9b h024ee3a0 jupyter 1.0.0 py36client 5.2.3 py36console 6.0.0 py36core 4.4.0 py360 keras-base 2.2.4 py360 kiwisolver 1.1.0 py36he6710b02 libffi 3.2.1 hd88cf552 libgcc-ng 8.2.0 hdf63c600 libglu 9.0.0 hf484d3e3 libopus 1.3 h7b6447c0 libprotobuf 3.7.1 hd4088760 libstdcxx-ng 8.2.0 hdf63c602 libuuid 1.0.3 h1bed4150 libxcb 1.13 h1bed4151 markdown 3.1 py361 matplotlib 2.2.2 py36hb69df0a0 mkl 2019.3 199 mkl0 mkl0 mock 2.0.0 py360 nbformat 4.4.0 py360 ncurses 6.1 hf484d3e0 ninja 1.9.0 py36hfd86e860 numpy 1.16.3 py36h7e9f1db0 olefile 0.46 py361 pandoc 2.2.3.2 0 pandocfilters 1.4.2 py360 pathlib 1.0.1 py360 pcre 8.42 h439df220 pickleshare 0.7.5 py360 pip 18.1 py360 prometheus0 prompt0 protobuf 3.7.1 py36he6710b00 ptyprocess 0.6.0 py360 pygments 2.2.0 py360 pyqt 5.9.2 py36h05f11520 python-dateutil 2.7.5 py360 pytz 2019.1 py1 pyyaml 5.1 py36h7b6447c0 qt 5.9.7 h5867ecd0 readline 7.0 h7b6447c0 scikit-learn 0.21.1 py36hd81dba30 send2trash 1.5.0 py360 sip 4.19.8 py36hf484d3e1 sqlite 3.26.0 h7b6447c0 tensorflow 1.12.0 gpu0 tensorflow-base 1.12.0 gpu0 tensorflow-estimator 1.13.0 py0 tensorflow-gpu-base 1.7.0 py36hcdda91b1 termcolor 1.1.0 py361 testpath 0.4.2 py360 toolz 0.9.0 py360 tqdm 4.31.1 py360 wcwidth 0.1.7 py361 werkzeug 0.15.2 py0 widgetsnbextension 3.4.2 py364 yaml 0.1.7 had098181 zlib 1.2.11 ha838bed0",question,question
984,https://github.com/streamlit/streamlit/issues/984,PyDeck example from PyDeck documentation not working,summary i've upgraded to streamlit 0.53 and wan't to test out the new `st.pydeck_chart` function. at some stage in my work i wan't to add tooltips and i cannot make it work. for example the example from the documentation at does not work. in streamlit the output of the code below does not show the data in html the output of the code below shows the data with tooltips steps to reproduce 1. run the code below 2. inspect the browser outputcode - streamlit version: 0.53 - python version: 3.7.4 - using `python -m venv .venv` - os version: windows 8.1 - browser version: chrome,Error,other
144,https://github.com/microsoft/recommenders/issues/144,"Create a benchmark for SAR, ALS and others with big datasets",this will be executed every night and the metrics and time will be recorded in a database,other,other
962,https://github.com/streamlit/streamlit/issues/962,Update the Uber demo to use PyDeck,problem all our demos should use the awesome new . the doesn't.solution make the uber demo use pydeck!,other,other
481,https://github.com/microsoft/recommenders/issues/481,"[DISCUSSION] General folder structure for reco, cv, and forecasting repos","i'm setting this discussion public in case any of our users or customers want to provide feedback. context we are building repos around computer vision and time series forecasting. we would like to homogenise the structure between them and the recommenders repo. the cv repo is still starting and the forecast repo has been running for some time internally and it is focused on benchmarks. the idea is to have a common structure (and user experience) between the 3 repos. trying to have the best of each: nice examples and utilities from recommenders, nice benchmarks from forecasting repo and support for cv, as well as the other solutions in reco and forecast. question what will be the optimal structure that will help our users and us to build better solutions in recommendations, cv and forecasting? please provide answers in detail ways, example: e1) i would take the recommenders structure (notebooks, recoutils, tests) and add a folder for benchmarks... e3) ...",other,other
1496,https://github.com/microsoft/recommenders/issues/1496,[ASK] Can this library be made compatible with Microsoft Azure DataBricks?,"description i am trying to run this library on azure databricks. this library requires python 3.6 or python 3.7. however, python 3 on databricks is python 3.8.8, with no option to downgrade. i have been unable to find a workaround for this. does anyone have any suggestions? in which platform does it happen? azure databricks how do we replicate the issue? run the following in a databricks notebook: !pip install ms-recommenders expected behavior (i.e. solution) the library should successfully install other comments i am aware that it is a python version compatibility issue. it seems weird that the microsoft platform i am using is completely incompatible with the microsoft library i am trying to use.",question,other
362,https://github.com/streamlit/streamlit/issues/362,cp950 codec decode error on Window machine,summary streamlit hello runs into decode error 'cp950' codec can't decode byte 0xf0 in position 1080: illegal multibyte sequence `'cp950' codec can't decode byte 0xf0 in position 1080: illegal multibyte sequence`steps to reproduce what are the steps we should take to reproduce the bug: window 10 conda create -n streamlit conda install python==3.6.1 pip install streamlit streamlit hello,other,Error
1151,https://github.com/deepfakes/faceswap/issues/1151,gpu is not used,"gpu is not used, but in gui,it say: `setting faceswap backend to nvidia 05/13/2021 10:52:10 info log level set to: info 05/13/2021 10:52:11 info model a directory: 'd:\data\yuhewei\new' (393 images) 05/13/2021 10:52:11 info model b directory: 'd:\data\zhangtianai\new' (481 images) 05/13/2021 10:52:11 info training data directory: d:\data\modle 05/13/2021 10:52:11 info =================================================== 05/13/2021 10:52:11 info starting 05/13/2021 10:52:11 info press 'stop' to save and quit 05/13/2021 10:52:11 info =================================================== 05/13/2021 10:52:12 info loading data, this may take a while... 05/13/2021 10:52:12 info loading model from original plugin... 05/13/2021 10:52:13 info no existing state file found. generating. 05/13/2021 10:52:13 info loading trainer from original plugin... 05/13/2021 10:52:28 info [saved models] - average loss since last save: faceb: 0.46390 ` i say it in form,but you say its normal in windows, the picture of the abnormality has the fnal say, or its a bug?",question,Error
385,https://github.com/iperov/DeepFaceLab/issues/385,Running SAE training without alignments,can we run sae training with images not extracted by dfl? are embedded alignment info in dfl extracted images required for sae training?,other,question
539,https://github.com/iperov/DeepFaceLab/issues/539,PreTraining new SAE model hangs up.,asus rog strix vega 64 8gb,other,other
468,https://github.com/iperov/DeepFaceLab/issues/468,[ (Winx64) Off-Topic Q.:] Possible to use DFL for other Tensorflow (Keras) Projects ?,"hello, i mean to use the prebuild windows app to use it for expl.: i checked the `python_console.bat` and `setenv.bat` for normal it should work..think so ;) any one try something like before ?",question,other
756,https://github.com/iperov/DeepFaceLab/issues/756,Unable to start subprocesses.,"traceback (most recent call last): file ""c:\users\ty\deepfacelabinternal\deepfacelab\main.py"", line 318, in file ""c:\users\ty\deepfacelabinternal\deepfacelab\main.py"", line 42, in processnvidia\nvidia\_internal\deepfacelab\core\joblib\subprocessorbase.py"", line 219, in run exception: unable to start subprocesses.",other,other
1171,https://github.com/microsoft/recommenders/issues/1171,[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package,"description we fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75). the new version of azure-cli is not compatible with the old azureml package and throws an error when creating azureml workspace: linux ubuntu (haven't tested on other platforms) how do we replicate the issue? install reco_pyspark and run operationalization notebook. expected behavior (i.e. solution) fix the version of azure-cli other comments i'm working on #1158 and #900. if fixing the azure-cli-core version is okay, then i will address this issue together.",deployment,deployment
314,https://github.com/deezer/spleeter/issues/314,"[Bug] Nothing happens when ""docker run .."" (trap invalid opcode)","ok, i found this great software today and tried it out right away on my laptop, awesome! but now i was trying on my stationary pc (with same setup) and it doesn't run. this is exactly what worked on my laptop. it downloaded the model an split the audio. i found a difference in the kernel log though.. some system info the only difference here is that on my laptop i'm running docker version 19.03.2 (but i didn't test rolling back, cause downgrading docker is a hassle. betting this isn't the problem). so, any idea why it's working on my laptop but not my stationary pc? pasting this mess last..",question,question
198,https://github.com/streamlit/streamlit/issues/198,Confusing advice in exception when calling `st.sidebar.write`.,"summary the bug you get when you call `st.sidebar.write` confusingly suggests (probably) the wrong rememdy to the user's bug.steps to reproduce run this: actual behavior what you see is: which is perfect.expected behavior i think it should suggest `st.sidebar.markdown` which is more likely what i wanted rather than `st.write`.additional information why doesn't `st.sidebar.write` just work, btw. shouldn't it?",other,Error
3410,https://github.com/streamlit/streamlit/issues/3410,"StreamlitAPIException : set_page_config() can only be called once per app, and must be called as the first Streamlit command in your script.","i am facing this issue when trying to set page config in a multi-page app. however, i have an idea of setting the default layout to wide which will solve my issue but how to do so?",Error,question
2426,https://github.com/streamlit/streamlit/issues/2426,"st.set_page_config(layout=""wide"") needs more padding","add additional padding/margin around reportview when in wide mode. dev notes: - sadly not as simple as `if widemode, left/right padding = 5rem"", for two reasons: - reportview is used by the sidebar as well, and we don't want to increase padding there - on mobile, we presumably don't want to add padding on left and right modes - tried using max-width: calc(100% - 10rem) but that still runs afoul of the mobile and sidebar issues.",other,other
3170,https://github.com/streamlit/streamlit/issues/3170,test_file_watch errors if watchdog is not installed,"streamlit degrades to a 'poolingfilewatcher' and gives a startup message if it is run without watchdog being installed. this is a known inband running mode. in the test code tests/streamlit/watcher/filetests, there is a test: testfile that test the various possible states with regard to watchdog availability and file watching. if that test file is invoked without having watchdog installed, the following error is generated. repro steps: pip uninstall watchdog pythonpath=lib pytest lib/tests/streamlit/watcher/filetest.py currently results in output ending in: filewatchertest.testfile ________________________________original() original(self): target = self.getter() name = self.attribute original = default local = false try: original = target._builtins and isinstance(target, moduletype): self.create = true if not self.create and original is default: raise attributeerror( > ""%s does not have the attribute %r"" % (target, name) ) e attributeerror: does not have the attribute 'eventbasedfilewatcher' ../../opt/anaconda3/lib/python3.7/unittest/mock.py:1293: attributeerror ================================== 1 failed, 2 passed in 0.30s ==================================",deployment,other
5,https://github.com/deezer/spleeter/issues/5,'spleeter' is not recognized as an internal or external command,"i followed the directions, but i get this error when i try using spleeter. did i miss something? do i need to manually add spleeter to my system path? i'm on windows 10.",deployment,question
1029,https://github.com/microsoft/recommenders/issues/1029,[ASK] Colab crash when using bigger dataset,"description when i use dataset with about 400k split in 6k items and 300k users, colab crash during ncfdataset is running. instead when i use a little slice of dataset with about 700 items and 800 users and a total of 1300 rows in the csv file it works. does anyone know how to fix this? do you think this is a google colab related issue? thank you very much for your help. have a nice day. other comments",question,question
3039,https://github.com/streamlit/streamlit/issues/3039,No such file or directory: 'https://rasahq.github.io/rasa-nlu-examples/square-logo.svg',"when i run the command python -m rasalit spelling --port 8501, it could open a port tunnel but shows no such file or directory: '",other,other
3318,https://github.com/streamlit/streamlit/issues/3318,st.image supports zoom in and zoom out,because my picture is so bigan st.image support zoom in and zoom out?,other,question
5235,https://github.com/iperov/DeepFaceLab/issues/5235,Issue during running 6) train SAEHD.bat,"hi guys, i have ran into this issue during running 6) train saehd.bat. please help me. im using deepfacelabrtx2080tiearlier btw.",other,other
3040,https://github.com/streamlit/streamlit/issues/3040,Can't trigger downloads from inside components,"summary i'm using the hiplot component and it offers a export functionality, which would trigger a download. the containing iframe however doesn't allow downloads. could that please be added? either as default or configureable. see for some details. ** the download is blocked.is this a regression? i don't know",other,other
2113,https://github.com/streamlit/streamlit/issues/2113,audio widget not working on Safari,"summary i have successfully managed to deploy a streamlit application on heroku, which you can see . i'm really happy about it; streamlit is really revolutionary. the problem is that the `st.audio` widget doesn't seem to be working on safari. the soundboard i've linked works fine on firefox. i haven't tried it out with chrome.steps to reproduce what are the steps we should take to reproduce the bug: 1. clone and navigate to it 2. `python -m venv env` 3. `source env/bin/activate` 4. `pip install poetry` 5. `poetry install` 6. `streamlit run soundboard/soundboard.py`expected behavior: the audio widget should work out of the box with most browsers.actual behavior:is this a regression? /debug info - streamlit version: 0.67.1 - python version: 3.7.4 - the heroku soundboard is build with a github, which you can see - os version: macos catalina 10.15.5 - browser version: safari 13.1.1 (15609.2.9.1.2); firefox version 80.0additional information keep up the good work!",deployment,other
1174,https://github.com/deepfakes/faceswap/issues/1174,How to change the Number of GPU?,"i have started with two t4 gpus, but if i need to specifically say to use only one gpu instead of both, how i can modify the program? thank you for taking the time to review this question.",question,question
521,https://github.com/deepfakes/faceswap/issues/521,GUI uses a lot of CPU ressources (some fixes included),"expected behavior i expect the gui to not constantly maxing out 1-x cores of my machine. actual behavior each time a training is started via the gui the cpu usage goes way up and stays at that level even when stopping the training. each new trainings session while the gui is running leads to more cpu usage. i tracked the issue down to the training graph updating data every 200msec. the traininggraph starts an animation.funcanimation( each time it is created. this funcanimation is never stopped. stopping the animationfunction each time the traininggraph is removed is probably easy to do and would solve this problem. my tk skills are severly limited, and i already wasted a day to find the issue, otherwise i would have submitted a pr for this. additionally setrate is never called and such the update interval stays at 200ms for ever. possible fix for that is to add `self.setrate(self.calcs.iterations)` after line 258. additionally i suggest changing the initial update interval from 200ms to 500ms. steps to reproduce start and training via gui. other relevant information",deployment,other
2218,https://github.com/streamlit/streamlit/issues/2218,Move boto3 and botocore into dev-packages,"these two are only used for static embedded apps, but the majority of our users never use that feature. we should be able to migrate them into devdependencies and cut down on the ""libraries required when you `pip install streamlit`""",other,other
1395,https://github.com/streamlit/streamlit/issues/1395,pydeck_chart doesn't properly render 3D plots created using PointCloudLayer,"i have created the following example that shows that streamlit is not able to properly render a 3d plot using pointcloudlayer.code import streamlit as st import pandas as pd import numpy as np from sklearn.preprocessing import minmaxscaler import pydeck mapboxkey='' @st.cache def genenerateloaddate() datastate.text(""done! (using st.cache)"") if st.checkbox('show raw data'): st.subheader('raw data') st.write(data) scaler = minmaxscaler(featuretransform(data)) rgb.columns = ['r','g','b'] rgb = rgb.astype(int) data = pd.concat([data, rgb], axis=1) target = list(data.mean()[['x','y','z']].values) viewx=0, rotationstate = pydeck.viewstate(offset=[0, 0], latitude=none, longitude=none, bearing=none, pitch=none, zoom=1) view = pydeck.view(type=""orbitview"", controller=true) pointlayer = pydeck.layer( ""pointcloudlayer"", data=data, getcolor=[""r"", ""g"", ""b""], gethighlight=true, pickable=true, pointobj = pydeck.deck( mapkey=mapboxkey, initialstate=viewcloudchart(pydeckobj) ''' st.deckchart(spec={'height':500, 'width':500, 'viewport':{'x':0, 'y':0, 'width':500, 'height':500}, 'layers':[{ 'data':data, 'type':'pointcloudlayer', 'getcolor' : [""r"", ""g"", ""b""], 'getsize' : 1, 'radiuspixels':1 }]}) ''' i've also tried using `deckchart` function but that didn't work either.output",question,Error
2813,https://github.com/streamlit/streamlit/issues/2813,Testing a method with caching,summary i want to test a method for reading files that has decorator of streamlit cache and i get stackoverflow.steps to reproduce code snippet: test: method: expected behavior: the test will passactual behavior: is this a regression? nodebug info - streamlit version: 0.76.0 - python version: 3.7.9 - conda 4.9.2 - os version: windows 10 - browser version: microsoft edge version 88.0.705.68 (official build) (64-bit),Error,other
363,https://github.com/deezer/spleeter/issues/363,[Bug] Spleeter has no output if filename ends with space,"description it seems that if the filename you are trying to split ends with a space, it won't save any results step to reproduce output exits without error but has no files in the output dir. the folder created by spleeter is there, eg `foo` but without trailing space. environment ----------------- ------------------------------- os windows installation type conda",question,other
111,https://github.com/mozilla/TTS/issues/111,TypeError: bad operand type for unary -: 'NoneType',"i want to train `mozilla tts` on chinese corpus, when everything was setup, then it output an error below:",question,Error
97,https://github.com/deezer/spleeter/issues/97,Ordinal Not Found,"i get this error ""the ordinal 242 could not be located in the dynamic link library libiomp5md.dll",deployment,question
56,https://github.com/microsoft/recommenders/issues/56,SAR PySpark Notebook,- sar pyspark notebook working - include notebook tests as well,other,other
359,https://github.com/mozilla/TTS/issues/359,Resume the last stopped steps,"hi there, i would like to ask, whenever i tried to resume the last checkpoint (due to power failure), why is the epoch **** again but not the last epoch that it stop? does it mean that the model start to train all over again?",question,question
426,https://github.com/microsoft/recommenders/issues/426,Add Vowpal Wabbit based recommender example,is affected by this issue? investigate adding a vw based example for news recommendations or movies recommendations w/ item features in platform does it happen? dsvm / databricks expected behavior (i.e. solution) i have an end to end example of training / deploying / operationalizing a recommender system using vw. other comments,other,other
13,https://github.com/deezer/spleeter/issues/13,Can't install spleeter using Git Bash on Windows 7,the install didn't work for me. `git clone `conda env create -f spleeter/conda/spleeter-cpu.yaml` this results in: `commandnotfounderror: no command 'conda env'.`,question,deployment
145,https://github.com/deepfakes/faceswap/issues/145,/r/deepfakes banned and moved,your repository references the reddit /r/deepfakes subreddit. it is now closed. the community has moved to [link removed],other,other
649,https://github.com/streamlit/streamlit/issues/649,Use DateIndex in line_chart,"problem i have the following csv file: date,idnumber,signature, smoke 20191108133637,837461,true,true 20191108133638,935903,true,false 20191108133639,912378,true,false 20191108133640,439192,true,true 20191108133641,747464,false,false 20191108133642,69270,false,true 20191108133643,11916,true,false 20191108133644,749752,true,false 20191108133645,801349,false,true that i read into a dataframe using ct = pd.readdates=[0]) i then transform the format of the date as follows: ct.date = pd.toindex('date', inplace=true) so the dataframe is now indexed with date, with ct.info() giving: datetimeindex: 9 entries, 2019-11-08 13:36:37 to 2019-11-08 13:36:45 data columns (total 3 columns): idnumber 9 non-null int64 signature 9 non-null bool smoke 9 non-null bool dtypes: bool(2), int64(1) memory usage: 162.0 bytes which is what i need. if i then try to plot the content of the dataframe, streamlit complains with the following error: keyerror: ""the following 'id_vars' are not present in the dataframe: ['index']"" which is certainly the expected behaviour, but it would be very convenient if plots with dataindex were supported.",other,Error
2966,https://github.com/streamlit/streamlit/issues/2966,Application not handling the required links anymore :(,"i just watched your demo on lewagon's youtube channel, and i truly was fascinated by your app and it's concept. it's such a great idea! unfortunately, it's not working anymore, with neither of the two recommended website links. i am really looking forward to trying it out. the world needs apps like this  i am talking about this app: am i at the right place to report this bug?",other,other
1243,https://github.com/streamlit/streamlit/issues/1243,ImportError: Dynamic module does not define module export function(PyInit_bz2),"summary i have setup python3 env and install streamlit . everything is working file even hello program is also running fine . but while i use run command wit my real program found below error . importerror: dynamic module does not define module export function(pyinit_bz2) not able to resolved , need help",other,question
1141,https://github.com/deepfakes/faceswap/issues/1141,DLL load failed while importing qhull,"after one of the latest updates (and may be gpu driver update too) train fails to start. i tried to remove everything including miniconda, cleanup and reinstall. tried to changed default conda enironment name and reinstall again, but this not helps. here is error from cli - win10x64 - python 3.8.8 - conda 4.9.0 - sha-1: f60eaee9557b9f54bd3ec3446563aa820ce421ed - ** the crash report generated in the root of your faceswap folder",other,deployment
61,https://github.com/deezer/spleeter/issues/61,Support files longer than 600 seconds / segmenting?,"description i assume it can only support files <= 600 seconds so i'd like longer support. what if i want to separate a concert? a way to do this, if you're not already doing so, would perhaps be to divide the audio into 1 minute segments and process them with overlap, and that might reduce ram use, too.",other,other
976,https://github.com/microsoft/recommenders/issues/976,NCFDataset not working properly with random split,description when trying to feed the function obtainig train/test sets from the `pythonsplit` function in utilsrandomncfdivencf notebooks.,question,Error
73,https://github.com/microsoft/recommenders/issues/73,review pyspark unit tests,"pyspark unit test cicd pipeline is executing also the spark notebooks, this shouldn't happen",Error,other
715,https://github.com/mozilla/TTS/issues/715,Can it be trained for Bengali language,"can it be used for bengali language, i have around 8hr of transcribed data and want to create a model.",other,question
658,https://github.com/deepfakes/faceswap/issues/658,ValueError: Timeout reached sending Exit Signal,"traceback (most recent call last): file ""c:\users\000\faceswap\lib\gui\wrapper.py"", line 316, in terminate file ""c:\users\000\miniconda3\envs\faceswap\lib\subprocess.py"", line 863, in communicate file ""c:\users\000\miniconda3\envs\faceswap\lib\subprocess.py"", line 1114, in _communicate subprocess.timeoutexpired: command '['c:\\users\\000\\miniconda3\\envs\\faceswap\\python.exe', '-u', 'c:\\users\\000\\faceswap\\faceswap.py', 'train', '-a', 'e:/a/face', '-b', 'e:/b/face', '-m', 'e:/model', '-t', 'iae', '-s', '100', '-bs', '64', '-it', '1000000', '-g', '1', '-ps', '100', '-nl', '-l', 'info', '-gui']' timed out after 60 seconds during handling of the above exception, another exception occurred: traceback (most recent call last): file ""c:\users\000\faceswap\lib\gui\wrapper.py"", line 318, in terminate valueerror: timeout reached sending exit signal",Error,other
95,https://github.com/mozilla/TTS/issues/95,Can't continue to train,"i started training by ""python train.py --configa.json"" comand, then my computer was rebooted. after that, i started ""python train.py --configa.json --restore07+28am-tts-master-d5d8458/checkpoint_45000.pth.tar"", but it reports error: i'm using d5d8458093816be7b3caf4303b239e3c240582e3 version from git.",Error,question
20,https://github.com/deezer/spleeter/issues/20,error trying to run separation,"bug running this command is not successful in creating the output files & gives messages spleeter separate -i audiooutput produces mutiple messages similar to the following: c:\users\bryan\.conda\envs\spleeter-cpu\lib\site-packages\tensorflow\python\framework\dtypes.py:516: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. qint8 = np.dtype([(""qint8"", np.int8, 1)]) your setup windows 10 64 in particular: miniconda",question,Error
1857,https://github.com/streamlit/streamlit/issues/1857,Page Config check is problematic when deployed.,"summary the error that is thrown if page config is run multiple times or is not the first run shows up on rerun. for some reason, this only seems to happen when deployed.steps to reproduce 1. deploy this code sample to 2. switch between radios a variable amount of times to cause reruns.expected behavior: nothingactual behavior:",Error,Error
339,https://github.com/deezer/spleeter/issues/339,[Bug] Found conflicts! when installing v1.5.1 over using Conda for Win 10 x64,"description installation fails for deezer spleeter v1.5.1 conda-forge environment over a clean conda installation on microsoft windows 10 x64. found conflicts in several packages. step to reproduce from anaconda prompt run as administrator: conda config --add channels conda-forge conda install -c conda-forge spleeter output (base) ps c:\deezer\spleeter> conda install -c conda-forge spleeter collecting package metadata (currentrepodata.json, will retry with next repodata source. collecting package metadata (repodata.json): done solving environment: failed with initial frozen solve. retrying with flexible solve. solving environment: found conflicts! looking for incompatible packages. this can take several minutes. press ctrl-c to abort. [several minutes of 'examining'... please see full log in post below] unsatisfiableerror: the following specifications were found to be incompatible with the existing python installation in your environment: specifications: - anaconda-navigator -> python[version='2.7.3.6.'] - anaconda-project -> python[version='3.4.3.5.3.4.3.5.3.6.93.4.3.8.3.5.3.6.93.6.9>=2.7,=3.8,=3.7,=3.6,=3.5, python[version='2.7.3.5.3.6.9 python=2.7 - diff-match-patch -> python[version='2.7.3.5.'] - gevent -> python[version='3.6.93.7.',build='2pypy1pypy0pypy'] - h5py -> python[version=' python[version='3.4.3.5.'] - jinja2 -> python[version='>=3.8, python[version='>=3.8, python[version='>=3.8, python[version='2.7.3.6.3.6.3.5.3.4.3.5.>=3.8, python[version='>=3.8, python=3.4 - openpyxl -> python[version='>=3.8, python[version='>=3.8, python[version='>=3.8, python[version='>=2.7, python[version='>=3.8, python[version='2.7.3.5.'] - prometheus7373731 pyqt -> qt[version='>=5.12.5, icu[version='>=58.1,=64.2,=58.2, qt==5.9.7=vc14h73c81de0 -> autopep8 anaconda==2020.02 -> autopep8==1.4.4=py0 -> terminado[version='>=0.8.1'] jupyterlab -> notebook[version='>=4.3.1'] -> terminado[version='>=0.8.1'] notebook -> terminado[version='>=0.8.1'] jupyterlab0py360'] package m2w64-gcc-libs-core conflicts for: pywinpty -> m2w64-gcc-libs -> m2w64-gcc-libs-core anaconda==2020.02 -> m2w64-gcc-libgfortran==5.3.0=6 -> m2w64-gcc-libs-core blas -> m2w64-gcc-libs -> m2w64-gcc-libs-core m2w64-gcc-libs -> m2w64-gcc-libs-core anaconda==2020.02 -> m2w64-gcc-libs-core==5.3.0=7 m2w64-gcc-libgfortran -> m2w64-gcc-libs-core package pathtools conflicts for: watchdog -> pathtools[version='>=0.1.1'] anaconda==2020.02 -> pathtools==0.1.2=py0 -> pathtools[version='>=0.1.1'] spyder -> watchdog -> pathtools[version='>=0.1.1'] package scikit-learn conflicts for: anaconda==2020.02 -> scikit-learn==0.22.1[build='py36h6288b170py37h6288b170 -> lazy-object-proxy astroid -> lazy-object-proxy=1.4 pylint -> astroid[version='>=2.3.0, lazy-object-proxy=1.4 anaconda==2020.02 -> lazy-object-proxy==1.4.3[build='py37he7745220py36he7745220py360'] statsmodels -> patsy[version='>=0.4.0>=0.5.1'] anaconda==2020.02 -> statsmodels==0.11.0=py36he774522which conflicts for: pywinpty -> backports.shutilwhich package alabaster conflicts for: sphinx -> alabaster[version='>=0.7, sphinx==2.4.0=py0py360'] numpydoc -> sphinx -> alabaster[version='>=0.7, sphinx[version='>=0.6.6'] -> alabaster[version='>=0.7, jupyter==1.0.0=py36console jupyter -> jupyterconsole==6.1.0=py0 -> libxslt[version='>=1.1.33, libxslt==1.1.33=h579f6680py360'] jupyter -> notebook jupyterlab -> jupyterlabserver -> notebook[version='>=4.2.0'] anaconda==2020.02 -> jupyter==1.0.0=py360py38he7745220'] seaborn -> statsmodels[version='>=0.5.0>=0.8.0'] anaconda==2020.02 -> seaborn==0.10.0=py0py360'] cffi -> pycparser anaconda==2020.02 -> cffi==1.14.0=py36h7a1dbc10 -> winpty package jdcal conflicts for: anaconda==2020.02 -> jdcal==1.4.1=py0 -> jdcal openpyxl -> jdcal package intel-openmp conflicts for: numpy-base -> mkl[version='>=2019.4, intel-openmp numexpr -> mkl[version='>=2019.4, intel-openmp scikit-learn -> mkl[version='>=2019.4, intel-openmp scipy -> mkl[version='>=2019.4, intel-openmp mklfft -> mkl[version='>=2019.4, intel-openmp mkl -> intel-openmp numpy -> mkl[version='>=2019.4, intel-openmp anaconda==2020.02 -> mkl==2020.0=166 -> intel-openmp package snappy conflicts for: blosc -> snappy[version='>=1.1.7, blosc[version='>=1.16.3, snappy[version='>=1.1.7, blosc==1.16.3=h7bd577a3 package prompt-toolkit conflicts for: ipython -> prompt-toolkit[version='!=3.0.0,!=3.0.1,=2.0.0'] prompttoolkit[version='>=2.0.0, prompt-toolkit[version='>=3.0.4,=3.0.5, libarchive[version='>=3.3.3'] -> bzip2[version='>=1.0.6,=1.0.8, bzip2==1.0.8=he7745225 -> bzip2[version='>=1.0.6,=1.0.8, bzip2[version='1.0.2.3.1.2.3.1.2.3.1.2.3.1.>=7.55.1, nbconvert -> bleach anaconda==2020.02 -> nbconvert==5.6.1=py360py360'] package sphinxcontrib-devhelp conflicts for: sphinx -> sphinxcontrib-devhelp spyder -> sphinx[version='>=0.6.6'] -> sphinxcontrib-devhelp numpydoc -> sphinx -> sphinxcontrib-devhelp anaconda==2020.02 -> sphinxcontrib-devhelp==1.0.1=py0 -> sphinxcontrib-devhelp package heapdict conflicts for: distributed -> zict[version='>=0.1.3'] -> heapdict anaconda==2020.02 -> heapdict==1.0.1=py0 -> heapdict package numpydoc conflicts for: spyder -> numpydoc[version='>=0.6.0'] python-language-server -> jedi[version='>=0.12'] -> numpydoc anaconda==2020.02 -> numpydoc==0.9.2=py0 -> numpydoc ipython -> jedi[version='>=0.10'] -> numpydoc jedi -> numpydoc package pympler conflicts for: spyder -> pympler hypothesis -> attrs[version='>=16.0.0'] -> pympler attrs -> pympler pytest -> attrs[version='>=17.2.0'] -> pympler package winpton conflicts for: anaconda==2020.02 -> winpton==1.1.0[build='py380py370 -> winpton urllib3 -> pysocks[version='>=1.5.6, winpton pysocks -> winpton package werkzeug conflicts for: anaconda==2020.02 -> werkzeug==1.0.0=py0 -> werkzeug[version='>=0.14'] package etxmlfile anaconda==2020.02 -> openpyxl==3.0.3=pyxmlfile anaconda==2020.02 -> et0py36h3d2d7360'] package qdarkstyle conflicts for: spyder -> qdarkstyle[version='>=2.7>=2.8'] anaconda==2020.02 -> spyder==4.0.1=py360 package msgpack-python conflicts for: dask -> distributed[version='>=2.14.0'] -> msgpack-python[version='=0.6.0'] anaconda==2020.02 -> msgpack-python==0.6.1[build='py37h74a97931py38h74a97930 -> msgpack-python[version='>=0.6.0'] distributed -> msgpack-python[version='=0.6.0'] package python-libarchive-c conflicts for: conda-package-handling -> python-libarchive-c conda[version='>=4.8.3'] -> conda-package-handling[version='>=1.3.0'] -> python-libarchive-c anaconda==2020.02 -> python-libarchive-c==2.8[build='py3813py376h29e5d5d1hcfe7411openblas -> libopenblas[version='0.3.60.3.60.3.60.3.60.3.70.3.70.3.80.3.90.3.70.3.70.3.70.3.70.3.70.3.7',build='h29e5d5d4hcfe74116h29e5d5d0h29e5d5d0hcfe74113h29e5d5d5h29e5d5d6h29e5d5d1hcfe74110 anaconda==2020.02 -> spyder==4.0.1=py36client -> twisted prometheuslauncher conflicts for: jupyterlab -> jupyterlablauncher[version='=0.10.0,=0.11.2,=0.13.1,=0.6.0,=0.5.4,=0.5.4>=0.4.0>=0.3.0>=0.2.3>=0.11.0, pycosat==0.6.3[build='py37he7745220py36he7745220py38he7745220'] notebook -> nbconvert -> mistune[version='>0.6>=0.7.4>=0.8.1'] anaconda==2020.02 -> nbconvert==5.6.1=py360 -> pywinpty terminado -> pywinpty anaconda==2020.02 -> pywinpty==0.5.7[build='py380py37metadata -> pathlib2 -> scandir package networkx conflicts for: anaconda==2020.02 -> scikit-image==0.16.2=py36h47e9c7a0 scikit-image -> networkx[version='>=1.8,=1.8>=2.0'] package llvmlite conflicts for: anaconda==2020.02 -> numba==0.48.0=py36h47e9c7a0py38ha925a310'] numba -> llvmlite[version='0.20.0.22.0.24.*>=0.25.0,=0.26.0,=0.27.0,=0.28.0,=0.29.0,=0.29.0>=0.30.0,=0.30.0>=0.31.0,=0.32.0>=0.32.0,=0.31.0'] package mock conflicts for: anaconda==2020.02 -> pytables==3.6.1=py36h1da09760 pytables -> mock package liblief conflicts for: py-lief -> liblief==0.9.0[build='ha925a312ha925a312 conda-build -> py-lief -> liblief==0.9.0[build='ha925a312ha925a31lowlowlowlow0 package jupyterlab conflicts for: jlabext0 package pkginfo conflicts for: conda-build -> pkginfo anaconda==2020.02 -> pkginfo==1.5.0.1[build='py380py37_cuda==10.2=0 your installed cuda driver is: 10.2 environment os: microsoft windows 10 v1909 x64 installation type: conda v4.8.3 (anaconda v3-2020.02) / python v3.7.6 ram available: 64 gb hardware specifications: nvidia geforce gtx 970 4gb / intel core i7-8700k @ 3.70 ghz / asrock z370 taichi additional context this issue is similar to #295 (might be, in fact, the same). i've been installing deezer spleeter since v1.4.4 without problems, but i had to use the 'python -m' workaround since v1.4.9. antivirus was always deactivated, as per issue #252.",question,deployment
659,https://github.com/mozilla/TTS/issues/659,cuDNN error: CUDNN_STATUS_BAD_PARAM,"hi. i tried training to tacotron (windows 10 and one gpu geforce rtx2080 ti) and i got the following error message: thanks a lot for your help python tts/bin/trainpath tts/tts/configs/config.json 2021-02-17 17:09:11.997602: i tensorflow/streamloader.cc:48] successfully opened dynamic library cudart6405+09pm-e9e0784 > setting up audio processor... > samplemels:80 > mindb:-100 > framems:none > framems:none > refdb:20 > fftlimnorm:true > symmetricfmin:50.0 > melgain:1.0 > stftmode:reflect > maxnorm:true > dosilence:true > trimsoundpath:scalelength:256 > winexecutor/platform/default/dso101.dll > using cuda: true > number of gpus: 1 2021-02-17 17:09:21.598745: i tensorflow/streamloader.cc:48] successfully opened dynamic library cudart64executor/platform/default/dso101.dll > using cuda: true > number of gpus: 1 2021-02-17 17:09:27.454550: i tensorflow/streamloader.cc:48] successfully opened dynamic library cudart6405+09pm-e9e0784 traceback (most recent call last): file ""tts/bin/traintacotron.py"", line 619, in main trainlossstep = train(traintacotron.py"", line 165, in train decoderoutput, alignments, stopbackwardbackward = model( file ""c:\users\voice-trainner\anaconda3\envs\tf2\lib\site-packages\torch\nn\modules\module.py"", line 722, in impl result = self.forward(input, *input, **",question,question
1432,https://github.com/streamlit/streamlit/issues/1432,grid layout for the main panel,"problem is your feature request related to a problem? please describe the problem here. ex. i'm always frustrated when [...]solution ** if you don't like the mvp above, tell us why, and what you'd like done instead.additional context add any other context or screenshots about the feature request here. for exmaple, did this fr come from or another site? link the original source here!",other,other
867,https://github.com/deepfakes/faceswap/issues/867,extracting not working,** it keep stoping at some point of extracting the frames from videos or even photos. it will stop at 19%. im also using all the cpu modes because it cant extract using my gpu. im using a mac pro python 3.6,question,Performance
300,https://github.com/iperov/DeepFaceLab/issues/300,Issue when running data_dst extract faces MT best GPU,when running datasrc.,other,other
669,https://github.com/deezer/spleeter/issues/669,train 2stems in musdb18,"i train 2stems in musdb18，json set ""instrumentdoruncallsessionrun tensorflow.python.framework.errorsestimator/python/estimator/estimator.py"", line 1514, in withspec file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1284, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1370, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1201, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 968, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1191, in dodoimpl.outofrangeerror: end of sequence errors may have originated from an input operation. input source operations connected to node iteratorgetnext: iteratorv2 (defined at usr/local/lib/python3.6/dist-packages/tensorflowrunasrunseparation/new/spleeter/spleeter/_separation/new/spleeter/spleeter/_separation/new/spleeter/spleeter/_estimator/python/estimator/training.py"", line 505, in trainevaluate file ""usr/local/lib/python3.6/dist-packages/tensorflowestimator/python/estimator/training.py"", line 747, in runestimator/python/estimator/estimator.py"", line 349, in train file ""usr/local/lib/python3.6/dist-packages/tensorflowtrainestimator/python/estimator/estimator.py"", line 1201, in modelestimator/python/estimator/estimator.py"", line 1037, in featureslabelsinputestimator/python/estimator/util.py"", line 61, in parsefnops.py"", line 420, in getdatasetgetdefapplyhelper file ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3565, in opdoruncallsessionrun tensorflow.python.framework.errorsrunasrunseparation/new/spleeter/spleeter/_separation/new/spleeter/spleeter/_separation/new/spleeter/spleeter/_estimator/python/estimator/training.py"", line 505, in trainevaluate file ""/usr/local/lib/python3.6/dist-packages/tensorflowestimator/python/estimator/training.py"", line 747, in runestimator/python/estimator/estimator.py"", line 349, in train file ""/usr/local/lib/python3.6/dist-packages/tensorflowtrainestimator/python/estimator/estimator.py"", line 1208, in modelestimator/python/estimator/estimator.py"", line 1515, in withspec file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 919, in internal file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basicrunestimator/python/estimator/training.py"", line 567, in end file ""/usr/local/lib/python3.6/dist-packages/tensorflowevaluate file ""/usr/local/lib/python3.6/dist-packages/tensorflowandestimator/python/estimator/estimator.py"", line 467, in evaluate file ""/usr/local/lib/python3.6/dist-packages/tensorflowactualestimator/python/estimator/estimator.py"", line 499, in estimator/python/estimator/estimator.py"", line 1647, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py"", line 272, in once file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1284, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1370, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitoredsession.py"", line 1201, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 968, in run file ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1191, in dodoimpl.invalidargumenterror: type mismatch: actual float vs. expect uint8 "" i wonder what the reason is,thanks u",other,question
648,https://github.com/streamlit/streamlit/issues/648,vscode debugging failure,"summary following the tutorial on does not work. streamlit apps can not be started with ptvsd. loading the browser on a streamlit app that has ptvsd enabled causes the following exception this is the script that can reproduce the issue: debug info - streamlit version: streamlit, version 0.49.0 - python version: python 3.6.8 - virtualenv - os version: ubuntu 18.04.3 lts - browser version: chrome version 78.0.3904.70 (official build) (64-bit)",other,Error
1841,https://github.com/streamlit/streamlit/issues/1841,Make the search bar sticky,"we have some pretty long pages and right now if you want to search, you have to scroll up to the top in order to search. it's a bit annoying on our api page especially given the length. two possible improvements: 1. use css to make the header (including search) sticky 2. add a floating button to return to the top of the page.",other,other
572,https://github.com/iperov/DeepFaceLab/issues/572,Failed to set cuDNN stream,hi iperov! couldn't wait to test 2.0 on a mining rig to see how cool is the multi-gpu training. i use aligned src and dst from 1.0. here the result: =============== model summary =============== == == == model name: wolfpreviewiter: 0 == == randomtype: f == == modelsondims: 256 == == edims: 64 == == ddims: 22 == == learndropout: false == == randomfacestylestylemode: none == == clipgrad: false == == pretrain: false == == batchexecutor/cuda/cudastatuserror 2020-01-23 15:59:07.067791: e tensorflow/streamdnn.cc:373] could not create cudnn handle: cudnninternalexecutor/cuda/cudastatuserror 2020-01-23 15:59:07.073161: e tensorflow/streamdnn.cc:373] could not create cudnn handle: cudnninternalexecutor/cuda/cudastatuserror 2020-01-23 15:59:07.079058: e tensorflow/streamdnn.cc:373] could not create cudnn handle: cudnninternalexecutor/cuda/cudastatus_success (7 vs. 0)failed to set cudnn stream. 2appuyez sur une touche pour continuer...,deployment,other
198,https://github.com/deepfakes/faceswap/issues/198,GAN plugin stalls losses at 17 percent regarless of GPU setup.,"gan plugin stalls losses at 17 percent regarless of gpu setup. would really like to use gan as i hear it gives better results, and my results from training have been spotty with the default trainer. tested on a m2200 (640 cores, 4gb ram) and then a p100 (3584 cores, 16gb ram) am using -t gan in addition to normal training command, first two numbers fluctuate wildly, but the latter two seem to steadily decrease until 17.xyz. any help would be appreciated.",Performance,question
24,https://github.com/deepfakes/faceswap/issues/24,"Training is not stopping when press ""q""","had been training the model from past 36 hours on mac os high sierra. now the issue i am facing is when i press ""q"" to stop the training process, nothing happens, it's continuing to train the model. please help as i don't want to terminate the program using ctrl+c to stop it as the trained model will also be lost and all my time/eletricity/etc will get wasted!",question,question
3072,https://github.com/streamlit/streamlit/issues/3072,Cache issues with file_uploader,"summary optional selectbox generated after a file upload are still there when filecolumns) to select the value to filter. not displayed here for lighter code. 1. load file 2. select a number of criteria for filtering 3. for each line, select a variable and do stuff 4. remove uploaded file 5. reload a new file ** ninput`, but number of lines displayed is the previous nuploader`debug info - streamlit version: 0.74.1 - python version: 3.8.5 - conda: 4.8.3 - os version: windows 10 - browser version: tested on latest firefox and chrome",Error,other
652,https://github.com/deepfakes/faceswap/issues/652,cuda10 +cudnn7.5 support,"** my os :ubuntu 16.04x64, and installed cuda10 + cudnn7.5, but it's not work now",other,other
3232,https://github.com/streamlit/streamlit/issues/3232,Multiple streamlit apps share same theme,"problem when we have multiple streamlit apps in one project the .streamlit config file applies the same theme to all of the files. and any change in the config file changes all the streamlit apps in the projectsolution ** if you don't like the mvp above, tell us why, and what you'd like done instead.additional context add any other context or screenshots about the feature request here. for example, did this fr come from or another site? link the original source here!",other,other
1538,https://github.com/streamlit/streamlit/issues/1538,st.cache is not work in jupyter notebook,"summary when i run st.cache in jupyter notebook, it raise an error below i know the reason, and it's ok when run in py file, but could it be possible to let us use cache in notebook? it's a rather cool function",other,Error
125,https://github.com/microsoft/recommenders/issues/125,Review docstrings in reco_utils,some files don't have the correct docstrings. ex:,Error,other
105,https://github.com/mozilla/TTS/issues/105,Where to pass text argument(Question),i installed current master branch of `mozilla tts` and `ljspeech-1.1 `. everything looks perfect except one step. from terminal what i need to excute to pass my text argument? i dont see usage documents,question,question
3575,https://github.com/streamlit/streamlit/issues/3575,"Previously fixed bug is back: selectbox not working as expected when ""navigating between pages""",the exact same issue as #389 is present in v0.84.2. i copied the minimal reproducible example and once again ran into the issue posted there. my preferred solution would be to return an empty list.,Error,Error
271,https://github.com/iperov/DeepFaceLab/issues/271,error with FAN-x conversion on OpenCL,"expected behavior trying to convert sae using fan-x on newest opencl build. actual behavior attribute error steps to reproduce running converter. loading model... using plaidml.keras.backend backend. info:plaidml:opening device ""opencltonga.0"" ===== model summary ===== == model name: sae == == current iteration: 360 == == model options: == == batchbyflip : true == == resolution : 128 == == facemask : true == == optimizerdims : 512 == == edims : 42 == == ddims : 21 == == multiscaleweights : true == == pixelstylestylerandominternal\deepfacelab\joblib\subprocessorbase.py "", line 59, in run file ""c:\deepfacelabopenclsse\initialize file ""c:\deepfacelabopenclsse\cliinternal\deepfacelab\facelib\fansegmentator.py"" , line 25, in _internal\deepfacelab\nnlib\nnlib.py"", line 754, in importinternal\deepfacelab\nnlib\nnlib.py"", line 188, in importsession'",deployment,question
39,https://github.com/iperov/DeepFaceLab/issues/39,Extracting issue in Openfaceswap,"i'm just trying to extract faces from the collect frames. issue is that doesn't work. text says ""the specified module is untraceable"" (i use it in french so i'm not sure of teh translation). i use this guide",other,question
3885,https://github.com/streamlit/streamlit/issues/3885,"Once a session takes GPU memory, it holds the memory even when the browser is closed","summary we are trying to provide a deep learning demo on streamlit using gpus. however, once a session takes gpu memory, it does not release the memory even the browser of the session is closed. thus, whenever we start and run a new demo session, each one takes its memory and finally our server has memory full and the overall sessions are shut down. we would like to end a session and release gpu memory whenever we close a browser running a session. steps to reproduce during each demo, we fine-tune deep neural models for user requests. therefore, each user gets different models, and the take up gpu memory. after we close a browser, the models still remain on the memory. (sorry that we can't provide code snippet.) ** once a language model is trained and takes gpu memory, it is not removed from the memory until the overall streamlit code is shut down. is this a regression? nodebug info - streamlit version: 0.87.0 - python version: python 3.7 - using conda? pipenv? pyenv? pex? we run it on a docker - os version: ubuntu - browser version: chrom 94.0.4606.61 (i don't think our problem depends on the version)",Performance,other
1106,https://github.com/deepfakes/faceswap/issues/1106,Which file to edit Tkinter tooltips?,didn't really fit into either category. noticed a few typos (i'm one of those people) and couldn't figure out where the gui text and tooltips are located.,Error,question
72,https://github.com/iperov/DeepFaceLab/issues/72,Erode as a relative value?,"could erode changed to be a relative value depending on the mask size? because on closeups you need higher values and on total shots lower ones. entering a percentage, for example?",question,question
5345,https://github.com/iperov/DeepFaceLab/issues/5345,Missing .bat files?,"i recently download dfl using torrent (deepfacelabuprtx2080ti062021.exe). i can't run it bcz of this error. operable program or batch file. '""""' is not recognized as an internal or external command, operable program or batch file.",other,question
66,https://github.com/streamlit/streamlit/issues/66,"On a Mac, check if xcode installed before requiring watchdog","streamlit watches files in the current folder for changes by using one of two methods: 1. the `watchdog` python module, which uses os-level event-based apis 2. a custom module that polls the files we care about every so often. if the `watchdog` module is available on the user's system, we currently use method 1. if not, we use method 2. the problem is that the `watchdog` module it compiled locally by pip when the user pip-installs it -- and on a mac this compilation step requires the free xcode cli tools with: ** 1. modify `setup.py` so we check whether xcode tools are available before making `watchdog` a dependency (only if the current system is a mac, of course). 2. in `localsourceswatcher.py`, when we fall back to the polling solution, print a message telling the user to install the watchdog module (see below). 3. add config option to turn that message off: `global.disablewatchdogwarning` message: (leave a blank line at the end, so the message does not merge with whatever we print after it.)",other,other
405,https://github.com/streamlit/streamlit/issues/405,format_func reference in API,"would you please specify where can the functions reference be found? to add option to modify the labels, in selectbox for instance. format_func (function) thanks,",other,other
634,https://github.com/mozilla/TTS/issues/634,dead links on the wiki,"hey, thanks for this amazing works. i just wanted to let you know that the links in this wiki page ( are dead (especially the link redirecting to checkspectrograms, analyzedataset and dataset/preprocess.py.).",Error,other
931,https://github.com/deepfakes/faceswap/issues/931,Black screen and hard crash when sorting,"trying to do tools > sort with default options always crashes the whole pc, no matter what shortly after picking the input and output dir and starting the sort the screen turns black and turning off the pc by long-pressing the button is impossible, it only turns off by pulling the cord no log included as there is no time to write any sortsetup_x64.exe) - crashes 100% of the time",question,question
377,https://github.com/microsoft/recommenders/issues/377,"python evaluation ""KeyError: 'rating'"" when a result DataFrame includes a col_rating column","is affected by this bug? python evaluation module do we replicate the issue? if `colprediction` and the result dataframe has both `colprediction` columns, `ratingpred` module's merge operation will change `colrating`""rating`""prediction` column and a ground-truth dataframe that has `colrating` in addition to the `col_prediction`, the functions should ignore that and work as expected.",Error,Error
335,https://github.com/microsoft/recommenders/issues/335,Create SAR deep dive notebook,is affected by this bug? more information should be provided in a deep dive notebook to help users understand sar. on the platform does it happen? platform independent. do we replicate the issue? expected behavior (i.e. solution) other comments,other,other
65,https://github.com/mozilla/TTS/issues/65,param issue in Synthesizer.load_model() function when calling AudioProcessor(...),"when setup the test server using ""python server/server.py ..."" to start flask server to test, there is an error reported that the nfft is correct set in config.json. tracing back, found there is an minor issue in synthesizer.loadrate = config.samplemels = config.numlevellevelshiftshiftlengthlengthlevellevelfreq = config.numlim_iters=60)",Error,Error
1418,https://github.com/streamlit/streamlit/issues/1418,Graphistry integration,"problem integrate into streamlit. the following snippet works but is not ideal : solution test if custom components or iframe support works with graphistry. if not possible, then try to embed the html directlyadditional context",other,other
3920,https://github.com/streamlit/streamlit/issues/3920,Error when running streamlit,"after upgrading streamlit with version 1.0.0, i cannot run streamlit, even 'streamlit hello' i mean, when i run ""streamlit hello"", there is output below. however, i can connect with and i think it is a bug - streamlit version: 1.0.0 - python version: 3.8.12 - using conda? pipenv? pyenv? pex? - os version: linux (in ssh) - browser version: chrome",other,question
977,https://github.com/deepfakes/faceswap/issues/977,Dockerfiles outdated,"crash report: `root@19e13ca3b3aa:/srv/faceswap# python faceswap.py -h setting faceswap backend to cpu traceback (most recent call last): file ""faceswap.py"", line 11, in exception: this program requires at least python3.6 ` bug the current dockerfiles start from images that have python 3.5 installed. these should be updated.",deployment,Error
817,https://github.com/deepfakes/faceswap/issues/817,cpu and gpu usage is nearly 0%,"it stucks at the first step (extracting). both cpu/gpu usage is below 1%. how to config/tune it? logs: > loading... > 07/29/2019 10:42:35 info log level set to: trace > 07/29/2019 10:42:38 info output directory: e:\video\mlx\fake\clips > 07/29/2019 10:42:38 info input video: e:\video\mlx\fake\clips\ym.mp4 > 07/29/2019 10:42:38 verbose using 'json' serializer for alignments > 07/29/2019 10:42:38 verbose alignments filepath: 'e:\video\mlx\fake\clips\ym_alignments.json' > 07/29/2019 10:42:38 info loading detect from mtcnn plugin... > 07/29/2019 10:42:38 verbose loading config: 'd:\workspaces\ai\faceswap\config\extract.ini' > 07/29/2019 10:42:38 info loading align from fan plugin... > 07/29/2019 10:42:38 verbose ** > 07/29/2019 10:42:38 info starting, this may take a while...",Performance,question
203,https://github.com/mozilla/TTS/issues/203,Transformer TTS,have you considered trying to add transformer tts repo? the result in the article is great and the convergence speed is 4-5 times faster than tacotron. the url is as follow:,other,other
993,https://github.com/streamlit/streamlit/issues/993,Feature request: st.hvplot_chart for hvplot plots,"problem i would like to be able to display a holoviews plot in a streamlit app that was generated from hvplot.pandas.solution i'd like to propose the following top-level api call: to do this, i would like to propose the following simple wrapper that would accomplish this. additional context this issue originated from .",other,other
1498,https://github.com/microsoft/recommenders/issues/1498,Add gpu build step to Github Action CI,description this is one of the sequence of changes to migrate ci infrastructures onto github actions: adding self-hosted agent to run the gpu tests. expected behavior with the suggested feature other comments,other,other
2347,https://github.com/streamlit/streamlit/issues/2347,Widgets overlap in Safari 14 when using columns,"summary hi, i have an app with (streamlit 0.71) that exhibits overlap when using columns.steps to reproduce expected behavior: dropdown widget on the left and not overlap.actual behavior: the dropdown widgets extends over the entire width, but can only be activated on it's true position (to the left)is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info streamlit, version 0.71.0 python 3.8.5 installed using pyenv/ pip os: macos 10.15.7 browser: safari 14.0",other,Error
3229,https://github.com/streamlit/streamlit/issues/3229,Different outcomes between one and two nested IF st.button('label'),"summary if st.button is under one statement `if st.button(.)`, the outcome is working perfectly. but if `st.button` under one more statements `if st.button(.)`, the web will be refresh and nothing to show.steps to reproduce code snippet: expected: show success message ""first layer ok"", ""second layer"" and ""third layer"" sequentially by press buttons ""first button"", ""second button"", and ""third button"" actually: only show the successful message ""first button ok"" and ""second button"" by press ""first button"". web will be refresh to start status by press ""second button""debug info - streamlit version: 0.81.1 - python version: 3.6.13 - conda 4.10.0 and pip 21.0.1 - os version: macos big sur 11.3.1 - browser version: safari 14.1 thanks!",other,Error
1048,https://github.com/microsoft/recommenders/issues/1048,Does the team have any advice on online evaluation?[ASK],description which metrics to use during an a/b test of a recommendation system? examples in e commerce would be awesome. i found this windows presentation that has awesome information. do you have more examples like the bing one? thanks!!,question,other
1686,https://github.com/streamlit/streamlit/issues/1686,File Uploader never resets (not None for every session),"summary once uploaded a file, no matter whether the server was restarted or the app just rerun or the caches cleared - the file_ only helps for the first run after server restart. in fact, short of creating (renaming) the uploader and starting again, i see no way to clear this uploader. since my server has to run all the time without restarting, it would be very nice, if you fix this issue!is this a regression? that is, did this use to work the way you expected in the past? yes debug info - streamlit version: 0.62.1 - python version: 3.8.3 - using conda? pipenv? pyenv? pex? - no - os version: catalina 10.15.5 - browser version: safari 13.1.1 additional is there is any way to manually set the uploader to none?",Error,other
1130,https://github.com/deepfakes/faceswap/issues/1130,Crash after trained for 2 days and can not run Train process and Tools/Preview process,"crash after trained for 2 days and can not run train process and tools/preview process again. ** loading... setting faceswap backend to nvidia please backup your data and/or test the tool you want to use with a smaller data set to make sure you understand how it works. 03/08/2021 09:47:41 info log level set to: info 03/08/2021 09:47:43 info input video: c:\users\lpc02\videos\forrestgumpc1.mp4 03/08/2021 09:47:44 info reading alignments from: 'c:\users\lpc02\videos\forrestgumpc1script process = script(arguments) file ""d:\app\faceswap\tools\preview\preview.py"", line 62, in _samples = samples(arguments, 5, self.lock, triggerpredictor = predict(queuequeue(""previewin""), file ""d:\app\faceswap\scripts\convert.py"", line 661, in _model = self.model() file ""d:\app\faceswap\scripts\convert.py"", line 746, in model model.build() file ""d:\app\faceswap\plugins\train\model\io.base.py"", line 520, in model(self.model return hdf5modelhdf5(filepath, customformat.py"", line 181, in loadfromweightshdf5weights'], model.layers) file ""c:\users\lpc02\miniconda3\envs\faceswap\lib\site-packages\tensorflow\python\keras\saving\hdf5weightshdf5objects.pyx"", line 54, in h5py.phil.wrapper file ""h5py\objects.withhl\group.py"", line 264, in _e(name), lapl=self.objects.pyx"", line 54, in h5py.phil.wrapper file ""h5py\objects.withinreport.2021.03.08.094746579604.log'. you must provide this file if seeking assistance. please verify you are running the latest version of faceswap before reporting process exited.",other,other
973,https://github.com/deepfakes/faceswap/issues/973,Running on the NVIDIA Jetson Nano ,"i am attempting to run faceswap on an board. a few of the required faceswap dependencies didn't have pre-built binaries for this chip architecture so i manually compiled them, including: - opencv-python==4.1.2.30 - numpy==1.17.4 the faceswap program runs ok and i can begin to train but it always eventually runs out of memory with `you do not have enough gpu memory available to train the selected model at the selected settings`. i've tried the most memory-saving techniques i found with no luck: the first issue i noticed was that faceswap doesn't detect an nvidia gpu at all: i know the (nvlm) which explains why faceswap isn't able to detect the gpu, because faceswap uses the `pynvml` library to . i guess my question is, is it possible and feasible with the 4gb of shared memory to run faceswap or are there other hardware limitations? i've included the crash log: 02/15/2020 15:06:50 mainprocess 0 memorygradients debugvalues([, , , , , , , , , , , ]): [] 02/15/2020 15:06:50 mainprocess 0 memorygradients debugvalues([, , , ]) in place of dictvalues([, , , , , , , , , , , ]): [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug for %s: [['training/adam/loss/mul:0']] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug processing list []: [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugat [, , ]: [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtotrainingconnectinputs debug connecting control inputs of op: decoder32leakyrelu/leakyrelu 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale01 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder16pixelshuffler/transpose 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder32pixelshuffler/reshape 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder32conv2d/biasadd 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingsavingprint debug copied [, , , , , , , , ] to dicttrainingsavingprint debug rewired %s in place of %s restricted to %s: [['training/adam/decoder16conv2d/biasadd1/reshape4pixelshuffler/transposea/upscale01/reshape:0', 'encoder/upscale0a/upscale0a/upscale0a/upscale01', 'training/adam/decoder16pixelshuffler/transpose', 'training/adam/decoder32conv2d/biasadd', 'training/adam/decoder32conv2d/convolution', 'training/adam/decoder32leakyrelu/leakyrelu', 'training/adam/decoder32pixelshuffler/reshape', 'training/adam/decoder32pixelshuffler/transpose']] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug for []: [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug with boundary backprop substitutions []: [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug found 9 ops to copy within [, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ], seed [], stoptrainingsavingprint debug opscopy = [, , , , , , , , ]: [] 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder8leakyrelu/leakyrelu 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale01 02/15/2020 15:06:51 mainprocess 0 transform control4pixelshuffler/reshapetrainingconnectinputs debug connecting control inputs of op: decoder8pixelshuffler/reshape 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder16conv2d/convolution 02/15/2020 15:06:51 mainprocess 0 transform controla/upscale0trainingconnectinputs debug connecting control inputs of op: decoder8conv2d/convolution 02/15/2020 15:06:51 mainprocess 0 memorygradients debugvalues([, , , , , , , , ]): [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debuga/upscale0sg:0', 'training/adam/encoder/flattensg:0', 'training/adam/encoder/upscale0sg:0'], ['decoder32pixelshuffler/transpose:0', 'encoder/flatten4pixelshuffler/transpose:0'], ['training/adam/decoder16conv2d/biasadd', 'training/adam/decoder16conv2d/convolution', 'training/adam/decoder8conv2d/biasadd', 'training/adam/decoder8conv2d/convolution', 'training/adam/decoder8leakyrelu/leakyrelu', 'training/adam/decoder8pixelshuffler/reshape', 'training/adam/decoder8pixelshuffler/reshapea/upscale04pixelshuffler/reshapetrainingsavingprint debug got gradients [none, none, , none, none, none, none, none, none, none, none, none, none, none, none, , , , , none, none, none, none]: [] 02/15/2020 15:06:51 mainprocess 0 memorygradients debugtrainingsavingprint debug with respect to [, , , , , , , , , , , <tf.variable 'dense_2/kern",deployment,question
653,https://github.com/streamlit/streamlit/issues/653,Add the ability to programatically re-run the streamlit script,"problem i am building a small labeling too where i read in an example, the user will select labels from a list of check boxes and click submit in order to save the labels, when they submit a field in the data for it being labeled is added so in the next read it skips that example. the problem is that the submit button will cause a rerun where the labels are collected and saved but it doesn't cause another re-run that will cause the data to be reread (this will move the app on to the next question)solution i think a solution would be a `st.rerun()` function that you can call to reexecute the script from the top. in my cause this would be called after my save and trigger a re-read. basically it would do the same thing i am currently using a second `next` button for but remove the need for user interaction.",other,other
1033,https://github.com/deepfakes/faceswap/issues/1033,How can I cancel the interactive mode?,"i'd like to train the model in a super computer center with lsf system, i got an error message saying `termios.error: (25, 'inappropriate ioctl for device')` this is how i submit the job and here is the crash report, thanks for help",question,question
5384,https://github.com/iperov/DeepFaceLab/issues/5384,export model to .dfm format,"hi everyone! i've made a model for a face. now i'm trying to use it into the deepfacelive app, but it needs a dfm format for importing models. i've already chage the main.py file as it's indicated, but not working. is there any way to export the model from deepfacelab to that format? kind regards!",question,other
664,https://github.com/mozilla/TTS/issues/664,[Feature] Sentiment analysis,"what about not just sharing sentence and wave file to each other, but also sentence sentiment and wave file. there are many sentiment analyzers. a flow could be like this: sentence -> sentiment analysis -> sentiment learning sentence -> wave -> tts learning. tts learning+ sentiment learning -> sentiment aware learning. therefore in inference, based on the sentiment of the sentence to be spoken, the emotion can be transferred. issues i see: - short sentences, like ""yes it is"". they don't transmit sentiment normally. in this case, the whole paragraph has to be taken into account to guess sentiment. benefits: - the emotion of the speaker is being used. therefore spoken text is easier to comprehend and sounds more natural.",other,question
363,https://github.com/iperov/DeepFaceLab/issues/363,Wide face - part of dst person's eyes still visible.,"expected behavior i am trying to deepfake a wide face (especially wide eyes - yoda) and i would expect to be able to cover the dst face in full with the new src face actual behavior i can't find the way to cover dst eyes in full with the src face. i have already increased face by 15% and tried multiple other parameter combinations but it's not enough. increasing more causes mouth to be too large - go beyond face. ideally i would need to just increase the size of the top part while not causing at the same time the bottom to be increased. or somehow remove/blur the yoda's (dst) remaining eyes. the only way i can think of right now, is going manually and try to blur manually each photo which is not probably the way to go... steps to reproduce i have attached the photo that shows visually the problem. one can reproduce issue by taking any yoda face and trying to deepfake it. other relevant information i'm looking for a suggestion from experienced person whether it's possible to deal with this problem in any way (parameter, procedure etc.)",other,other
831,https://github.com/microsoft/recommenders/issues/831,[FEATURE] LibffmConverter input type,"description when i use libffmconverter to do the conversion on movielens dataframe, userid and itemid are not converted correctly after using movielens.loaddf . these two columns are loaded as int. i changed these two columns into string to make the converter work properly. we may need to mention that types of columns need to be ""string"" if they are categorical variables, in the readme file or description of libffmconverter part.",other,Error
375,https://github.com/microsoft/recommenders/issues/375,User-friendly guidance on data preparation,it takes time in real-world problems that user feel frustrated about preparing data for recommender algorithms. it would be good to have notebooks to detail that.,other,other
1152,https://github.com/microsoft/recommenders/issues/1152,[FEATURE] Change newsrec dataset to MIND,description change newsrec dataset to mind expected behavior with the suggested feature change the dataset used in npa naml nrms lstur to mind dataset,other,other
3276,https://github.com/streamlit/streamlit/issues/3276,streamlit to  executable file,"hi, is there a way to convert streamlit gui into executable file? so far no informationn regarding this is found? please help",question,question
984,https://github.com/microsoft/recommenders/issues/984,[BUG] Memory error in map_at_k (using Cornac BPR),"description i am running cornac bpr on a problem with 13k users and 25k items on an standard nc12 system with 118gb of ram. i used the cornac bpr example and adapted it to do hyperparameter optimization in azure. the model is fit and allatmap = mapk(test, allprediction=""prediction"", k=toputils/evaluation/pythonatutils/evaluation/pythoncolumnwrapper *args, **kwargs) file recodfobject.columns) memoryerror i am able to run a different, smaller problem with under 550 users and 24k items with no problems. what is the best way to run the evaluation functions for large datasets? is there a way to batch feed the predictions to the evaluation functions mapk, ndcgk etc.? in which platform does it happen? how do we replicate the issue? expected behavior (i.e. solution) other comments",other,question
2564,https://github.com/streamlit/streamlit/issues/2564,URL markup does not get generated as a link,summary urls used to generate an anchor tag automatically in markup. now it does notsteps to reproduce code snippet: expected behavior: actual behavior: is this a regression? yes as of 0.74,Error,Error
2985,https://github.com/streamlit/streamlit/issues/2985,Add human-useful str/repr methods to classes,"printing the string representation of an object is useful, particularly when debugging. the python default string method is awful for this, telling us only the class and a memory address. we should define `__`, which is supposed to be a string representation of python code that could be copied to construct the object again, when possible. automatically defining these methods in a sensible way is done by modules and packages like attrs and dataclasses, but we don't use the former and can't use the latter until we drop 3.6 support, so i think it is worth the small effort to do it ourselves.",other,other
9,https://github.com/deezer/spleeter/issues/9,OSError: FFMPEG binary (ffmpeg) not found,tried running spleeter on on my machine and kept getting this error `oserror: ffmpeg binary (ffmpeg) not found`,question,question
214,https://github.com/deezer/spleeter/issues/214,[Discussion] Example on how to change bitrate,"i'm new to python and don't know much. i know that in the recent release of spleeter bitrate options were introduced. i used pip uninstall, deleted other info from it, and then reinstalled it so i could have it (please tell me if i did it wrong). i tried using the command as so spleeter separate -i 'searchlight.mp3' -p spleeter:5stems -b 320k -o splits however nothing is changing as the bitrate is still 128. is there an example on how this should work?",question,question
1009,https://github.com/streamlit/streamlit/issues/1009,Altair 4 interactive legend does not work,"summary the new interactive legend feature of altair 4.x does not work when displaying charts with both `altair_chart` and `write`. the feature does work with identical code in a jupyterlab notebook.steps to reproduce test app pulled from altair documentation: expected behavior: the legend should be clickable, with data series being filtered to the selected legend entries.actual behavior: nothing happens when the legend is clicked. there are no js errors in the console output.is this a regression? nodebug info - streamlit version: 0.53.0 - python version: 3.7.6 - altair version: 4.0.1 - using conda? pipenv? pyenv? pex? conda, pip - os version: linux 5.0.0-37-generic #40~18.04.1-ubuntu smp - browser version: chrome 79.0.3945.130 / firefox 72.0.1 (64-bit)additional information",Error,Error
5240,https://github.com/iperov/DeepFaceLab/issues/5240,Faceset creation guide link broken.,"this is not tech support for newbie fakers post only issues related to bugs or code expected behavior i click in the link for the ""faceset creation guide"", and magic! a link with that content appears on my browser. actual behavior a page on the forum saying that that post does not exists appears. steps to reproduce click on the link ""faceset creation guide"" other relevant information none",other,Error
423,https://github.com/mozilla/TTS/issues/423,Latest model configuration is not compatible with master,i tried with master and got the following error: here is the full backtrace: how can i test things on master? i don't have the resources to train my own models. ** the latest commit that works for me is around 53b24625a7b898447b0cda2929503b96752d9eae,deployment,Error
262,https://github.com/deezer/spleeter/issues/262,Tensorflow-gpu - Not having luck with conda,"hi all, i am not having any luck with tensorflow and spleeter-gpu. training is not workking. i have completely removed spleeter and installed tensorflow-gpu version 1.14 using pip. i have installed cuda 10.0 i have installed cudnn 7.4.2 this is the combination to use as indicated by the tensor flow website. i have gone through some tensorflow tutorials with the above set-up and proved that the gpu is detected by tf and is used. now some questions, probably more for the deezer guys if they can add any detail. in the installation section of the wiki it says that spleeter-gpu is not available using pip. i was under the impression that if tensorflow detected a gpu it would use it. i have installed spleeter using pip and it indeed ignores the gpu. what needs to be changed in spleeter to use the gpu features of tensorflow? i have never built packages for conda or pip so is this something done within the package manager? when trying to debug what was happening with conda, i simply added ""print"" statements to see where in the code it was having issues. it was within a tensorflow function estimator.trainevaluate which is where i suspected the issue was and why i needed to confirm that tensorflow could see the gpu. so i know i can change the source code installed by pip / conda. where in the code do i need to focus on getting spleeter to use the gpu?",question,question
1103,https://github.com/deepfakes/faceswap/issues/1103,need help in installing,"setting up faceswap environment... this may take a while warning: retrying (retry(total=4, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/opencv-python/ warning: retrying (retry(total=2, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/opencv-python/ warning: retrying (retry(total=0, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/imageio-ffmpeg/ warning: retrying (retry(total=3, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/imageio-ffmpeg/ warning: retrying (retry(total=1, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/imageio-ffmpeg/ error: could not find a version that satisfies the requirement imageio-ffmpeg>=0.4.2 error: no matching distribution found for imageio-ffmpeg>=0.4.2 warning: retrying (retry(total=4, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/ffmpy/ warning: retrying (retry(total=2, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('ssl.c:1106: the handshake operation timed out'))': /simple/ffmpy/ warning: retrying (retry(total=0, connect=none, read=none, redirect=none, status=none)) after connection broken by 'proxyerror('cannot connect to proxy.', timeout('_ssl.c:1106: the handshake operation timed out'))': /simple/ffmpy/ error: could not find a version that satisfies the requirement ffmpy==0.2.3 error: no matching distribution found for ffmpy==0.2.3 > what can i do to install this software successfully",question,other
3706,https://github.com/streamlit/streamlit/issues/3706,Iframe embeds are not present for certain elements,"after 0.85 was released, iframe embeds for `areachart`, and `line_chart` are missing. latest version (no chart iframes): v0.84 (shows chart iframes):",Error,Error
780,https://github.com/iperov/DeepFaceLab/issues/780,Error while ectractin F from WF collection,i`ve got an error while ectractin f from wf collection ` if extractdflimg and len(rects) != 1`: and extraction script stops working,other,question
136,https://github.com/deezer/spleeter/issues/136,Can't find spleeter,git clone the install looked good. what am i missing.,question,question
342,https://github.com/deezer/spleeter/issues/342,[Bug] name your bug,"description result: spleeter works fine on win7, but produces this, on win10: step to reproduce installed: python-3.8.2.exe miniconda3-latest-windows-x86click(object sender, eventargs e) bei system.windows.forms.control.onclick(eventargs e) bei system.windows.forms.button.onclick(eventargs e) bei system.windows.forms.button.onmouseup(mouseeventargs mevent) bei system.windows.forms.control.wmmouseup(message& m, mousebuttons button, int32 clicks) bei system.windows.forms.control.wndproc(message& m) bei system.windows.forms.buttonbase.wndproc(message& m) bei system.windows.forms.button.wndproc(message& m) bei system.windows.forms.control.controlnativewindow.onmessage(message& m) bei system.windows.forms.control.controlnativewindow.wndproc(message& m) bei system.windows.forms.nativewindow.callback(intptr hwnd, int32 msg, intptr wparam, intptr lparam) die zone der assembly, bei der ein fehler aufgetreten ist: mycomputer mscorlib spleetgui system.windows.forms system system.drawing accessibility mscorlib.resources system.windows.forms.resources um das jit-debuggen (just-in-time) zu aktivieren, muss in der konfigurationsdatei der anwendung oder des computers (machine.config) der jitdebugging-wert im abschnitt system.windows.forms festgelegt werden. die anwendung muss mit aktiviertem debuggen kompiliert werden. zum beispiel: ausnahmen an den jit-debugger gesendet, der auf dem computer registriert ist, und nicht in diesem dialogfeld behandelt. environment firewall: disabled. host file: untouched from stock windows 10 ----------------- ------------------------------- os windows 10 installation type conda / pip ram available 4go hardware spec fujitsu q702, gpu: intel hd graphics 4000, intel(r) i3-3217u1.80ghz additional context",other,question
2720,https://github.com/streamlit/streamlit/issues/2720,Some instances of my component are load as empty when I change their order,"summary sometimes it happened to me, that my component did not fully load. in this case, only component's `public/index.html` was loaded without anything in `` tag. i have many instances of my component in my streamlit app. this issue happened when i change order of my components.steps to reproduce 1. in some browser (i tried firefox and chromium), go to 2. click on `submit` button 3. scroll down to `passages:` and you will see passages in instances of my component 4. expand `demo options` on the sidebar 5. under `passages` (or under `spans` it happens too) switch `descending order` checkbox twice with some pause between 6. now you may see, that some instances of my component are empty. if not, try step 5 again. 7. close the sidebar or change the window size and then empty instances will fill up. but their height of iframe is static.expected behavior: when i changed order of my component's instances, all displayed well.actual behavior: when i changed order of my component's instances, some of them displayed empty and with static height of iframe. screenshot before changed order: screenshot after changed passages order twice: is this a regression? nodebug info - streamlit version: 0.75.0 - python version: 3.8.5 - using conda 4.9.2 - os: linux ** streamlit-component-lib@1.1.3",Error,Error
116,https://github.com/deezer/spleeter/issues/116,[Discussion] FileNotFoundError When Trying to Train,** 1. installed using anaconda 2. run as: spleeter train -p sirenstrain.csv 3. filenotfounderror: [errno 2] file b'configs/sirenstrain.csv' sirens _train.csv definitely exists so i'm not sure what the issue is. any help would be great thanks!,other,question
266,https://github.com/streamlit/streamlit/issues/266,"Wrong Quote in Code Example in ""Main Concepts"" page","summary due to quote ` instead of a single quote in the code example, it throws an error. steps to reproduce what are the steps we should take to reproduce the bug: 1. go to page 2. scroll down to section. 3. copy the code for the square the number example and run using streamlit.expected behavior: the code should have worked correctly in the interpreter.actual behavior: python throws syntax error due to wrong quote.is this a regression? nodebug info - streamlit version: 0.47.2 - python version: 3.7 - using conda - os version: linux mint 18.3 - browser version: chrome",Error,Error
560,https://github.com/deepfakes/faceswap/issues/560,EFFmpeg doesn't work,------------------------------- os: windows 10 pro 64bit cpu: i5 8600 ram: 16gb ddr4 2666 gpu: gtx 1070 -------------------------------,other,deployment
169,https://github.com/deezer/spleeter/issues/169,[Bug] Failed to load the native TensorFlow runtime.,"description unable to separate. step to reproduce 1. installed using `pip` 2. run as `python -m spleeter separate -i out.mp3 -o out` 3. got ""failed to load the native tensorflow runtime."" error output environment ----------------- ------------------------------- os windows installation type pip ram available 8g hardware spec cpu additional context i'm not sure what can effect this issue.",question,question
3810,https://github.com/streamlit/streamlit/issues/3810,Streamlit not working after upgrading to Debian 11 in WSL2,"summary streamlit doesn't work well after upgrading debian to 11 and python to 3.9. and seems to be stuck on certain functions just as always.steps to reproduce i'm currently working on two wsl2 machines and a server machine. all of three are now debian 11. on the server side, i did the same upgrading as i did on wsl2, and streamlit works on it without any difficulties. on both wsl2 machines, streamlit used to work fine, but not so since upgrading the system, no matter what command i feed to it as below, it just stuck and took over 10 minutes to give proper results (takes forever!). i also have tried to reinstall streamlit by first removing it from both python3.7 (from debian 10) and python3.9 (default python3 version), but it didn't change a thing. by forcing the process to stop by `ctrl+c`, i could see there can be two different kinds of tracebacks (listed below, every command gives the same result). is this a regression? yesdebug info - streamlit version: 0.88.0 - python version: 3.9.2 - using conda? pipenv? pyenv? pex? none of above - os version: debian gnu/linux 11 (bullseye) on windows 10 x86_64 - linux kernel version: 5.10.43.3-microsoft-standard-wsl2",question,deployment
3292,https://github.com/streamlit/streamlit/issues/3292,"How to share the apps settings, preferable via a custom URL?","i am looking for a way to share the state of the app. in the most simple form, this is what i am after: a user sets some sliders, ticks some boxes. he then can create an url or something else that he can share. when somebody receives this link, once he opens the app he gets the same settings as the user who shared his settings. as a url solution i am thinking about generating a random id (caching it when needed), and all settings are connected to this id and can be shared after clicking on a button that says `save and generate share link`. is there some solution implemented to handle this case? where can find further help in case such a feature is yet to come? thanks for your replies!",question,question
128,https://github.com/iperov/DeepFaceLab/issues/128,4) data_src extract faces <any>.bat error,"hi! expected behavior trying to extract faces after successful extracting png from the given sample datasrc extract faces` have an issue: steps to reproduce extract png from databuild01_2019.zip` windows 10 x64 integrated video + 2 x gtx 1070 cuda 8 installed both graphic cards work, tested by mining",other,question
328,https://github.com/deepfakes/faceswap/issues/328,Filter image doesn't work?,tried using a filter image to remove other actors from the scene but it doesn't seem to work. would be great if it did since my current workflow involves manually masking out other actors in after effects which is time consuming.,Performance,question
893,https://github.com/streamlit/streamlit/issues/893,Likert Scale: Changing slider end point labels to categoricals,"i am wondering if there is a way to use the slider to return a numerical, but label it with categoricals. for example, if the slider is from 0 to 10, i would like 0 to be labelled ""low performance"", 10 be replaced with ""high performance"", and the slider dot to not show the numerical value. i know this could be accomplished with radio buttons or a dropdown, but since it is just relabelling, seems like it would be easy to implement. thanks!",other,other
548,https://github.com/deezer/spleeter/issues/548,spleeter.separator not found when installing with pip,description pip installing seems to be missing the separator module. step to reproduce 1. pip installed ffmpeg and spleeter 2. ran this code environment ----------------- ------------------------------- os windows installation type pip ram available 20gb hardware spec gtx 1060 / i5 6600k python version 3.8.0,question,deployment
254,https://github.com/mozilla/TTS/issues/254,Train with partial load model,"i'm trying to use tacotron2-iter-260k (824c091) from with master (b1657d70b160c097be7ac24290514a9aed010452) `time cudadevices=0 python train.py --configmodel/ljspeech-260k/configpath ../pretrained260000.pth.tar --datatrain.log` > model has 28180578 parameters but get an error: ``` > epoch 0/1000 /pytorch/aten/src/thc/thctensorindex.cu:362: void indexselectlargeindex(tensorinfo, tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [32,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [33,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [34,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [35,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [36,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [37,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [38,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [39,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [40,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [41,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [42,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [43,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [44,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [45,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [46,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [47,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [48,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [49,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [50,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [51,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [52,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [53,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [54,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [55,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [56,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [57,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [58,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [59,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [60,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [61,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [62,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [63,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [96,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [97,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [98,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [99,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [100,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [101,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [102,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [103,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [104,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [105,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [106,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [107,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [108,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [109,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [110,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [111,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [112,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [113,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [114,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [115,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [116,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [117,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [118,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [119,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [120,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [121,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [122,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [123,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [124,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [125,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [126,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [127,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [64,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [65,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [66,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [67,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [68,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [69,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [70,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [71,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [72,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [73,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [74,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [75,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [76,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [77,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [78,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [79,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [80,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [81,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [82,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsigned int, dstdim = 2, srcdim = 2, idxdim = -2, indexismajor = true]: block: [224,0,0], thread: [83,0,0] assertion `srcindex , tensorinfo, tensorinfo, int, int, indextype, indextype, long) [with t = float, indextype = unsign",question,question
3992,https://github.com/streamlit/streamlit/issues/3992,Streamlit returns error if get_elevation added to ColumnLayer,"summary i am trying to display a `pydeck` layer with columns. i want the column height to be the `rain (mm)` column i am providing in a dataframe, but whenever i use `getelevation='rain (mm)'`, the map appears but the tooltip only returns `rain (mm)`is this a regression? nodebug info - streamlit version: 1.1.0 - python version: 3.6.10 - using conda - os version: windows 10 - browser version: microsoft edge 94.0.992.50 64-bitadditional information the data i am using have the following structure: because all `pydeck` examples with columnlayer work, returning an html object where the map is rendered and there is no issue with the tooltip or with `get_elevation`, i think the issue is created by `st.write(r)` if `r` is an html object. this tells me streamlit has some issues with rendering html objects.",other,Error
2382,https://github.com/streamlit/streamlit/issues/2382,Add simple cache with no fancy features that's not buggy,"i'm using `streamlit` to preprocess a lot of data and to analyze the results of the preprocessing.problem the `st.cache` function continues to cause me issues, and it looks like i'm not the only person: `st.cache` is integral to building a complex `streamlit` app, and yet it's complex and buggy. the cache gets cleared, for no apparent reason, constantly. the internal implementation is complex, and it's hard to debug. it's hard to answer the question: ""why was my cache cleared? how can i prevent that from happening in the future?"".solution can a simple version of cache be included in `streamlit`? a cache that works like `functools.lrucache`. without such a function, using `streamlit` is getting progressively less productive as my app gets more complex. and i might just switch back to notebooks... 憥 it's easier to reason about the caching mechanisms in notebooks.other solutions - can i implement something similar myself? can you provide a mechanism for persistent memory storage across reruns? - can you provide tools to debug `st.cache`? i'd like to be able to know... ""why was my cache cleared? how can i prevent that from happening in the future?"" - support `multiprocessing.pool.pool`.",Error,question
466,https://github.com/mozilla/TTS/issues/466,broken links in wiki,there are broken links in wiki/training-and-testing#testing-model,Error,other
246,https://github.com/streamlit/streamlit/issues/246,Support Streamlit on Heroku?,"@kantuni had issues getting heroku to bind the websocket port, but it seems like this should be possible?",question,other
502,https://github.com/deezer/spleeter/issues/502,[Feature] Installing and running Spleeter on Windows,description installing and running spleeter on windows (python 3.7) ` edit: `cd ..` clone spleeter repo in the environment: `git clone cd spleeter `pip install . && pip install pytest pytest-xdist ` also install `pip install musdb ` `pip install museval ` then: `python setup.py install` it worked ex: `spleeter separate -i song.mp3 -o audio_output -p spleeter:5stems `,other,deployment
471,https://github.com/streamlit/streamlit/issues/471,"Add links to discuss, docs and Github when in discuss discussion","problem when viewing a discussion at discuss.streamlit.io there is no direct links back to the discuss front page. you can off course click ""back"" but sometimes you did not arrive in the discussion from the front page. this makes it cumbersome to navigate.solution please add link to ** as well because often you need to study the docs before you can join in on the discussion. and sometimes you would like to create an issue or feature request based on the discussion.additional context",other,other
3766,https://github.com/streamlit/streamlit/issues/3766,how to display html,"problem is your feature request related to a problem? please describe the problem here. ex. i'm always frustrated when [...]solution ** if you don't like the mvp above, tell us why, and what you'd like done instead.additional context add any other context or screenshots about the feature request here. for example, did this fr come from or another site? link the original source here!",question,other
2253,https://github.com/streamlit/streamlit/issues/2253,Select All Feature in st.selectbox,"i am attempting to plot a scatter plot using altair on streamlit. i am having trouble with the selectbox which contains each us state. before touching the selectbox all the states are displayed on the chart, however when i choose one specific state to focus on using selectbox i can't go back to select all unless i remove the dropdown the page. is there a way i can select all values using a dropdown with having to do it manually using multiselect?",other,question
91,https://github.com/deezer/spleeter/issues/91,only Stem2 working,"successfully installed on a 1-cpu 1g ram vm, running ubuntu 19.04. ran the example with sample audio file, worked ok. uploaded my own audio file (alison by elvis costello, can supply if necessary) 2stem worked ok, 4 and 5 stem failed. example output shown: spleeter separate -i audioexample.mp3' from 0.0 to 600.0 info:spleeter:audio data loaded successfully info:spleeter:file output/audioid=ubuntu distribcodename=eoan distrib_description=""ubuntu 19.10"" i would like to be able to extract the bass !",question,question
1733,https://github.com/streamlit/streamlit/issues/1733,Undocumented breaking change in multiselect default parameter,"summary in the most recent streamlit version the default parameter in multiselect now expects a list instead of a string. i couldn't find this breaking change documented within the changelog happy to give more info if needed, just let me know.",other,Error
2430,https://github.com/streamlit/streamlit/issues/2430,Ensure file uploader works with st.image and st.video,"when developing, noticed that an exception had to be made with uploadedfile in media proto but during pr review testing, this didnt seem to throw an issue. however, testing it now throws an error. to be safe, make sure anywhere we're taking a bytesio also takes in the returned object.",Error,other
452,https://github.com/iperov/DeepFaceLab/issues/452,Cannot extract faces with any extraction method,expected behavior run datadst videos i know have had faces extracted before in older versions. steps to reproduce run any of the face extraction methods other relevant information windows 10 deepfacelab opencl sse build from 10-19-2019,Error,other
8,https://github.com/mozilla/TTS/issues/8,Inference time metrics?,"hello, i am very interested in this project, i am looking for a pytorch implementation of tacotron/tacotron2/wavenet and may wish to contribute. do you have any metrics on forward-pass time for inference on new text? i am looking to export a pytorch model into caffe2 and run it on a mobile platform.",other,question
956,https://github.com/deepfakes/faceswap/issues/956,Failed to convert image: ... Reason: '>=' not supported between instances of 'NoneType' and 'float',"** ============ system information ============ encoding: cp1252 gitcommits: c721c22 merge branch 'staging'. 9ebc0ab tools.sort - optimize sort by face. e1d832f clarification of phases in extract (#949). 2e3e602 bugfix: lib.tools.mask (#953). c1e6080 training: cleaner loss printing gpucudnn: no global version found. check conda packages for conda cudnn gpu0: geforce gtx 1060 3gb gpuactive: gpudriver: 441.41 gpu0: 3072mb osplatform: windows-10-10.0.18362-sp0 oscommand: c:\users\jpcho\faceswap/faceswap.py gui pyversion: conda 4.7.10 pyversion: 3.6.8 pyenv: true sysprocessor: intel64 family 6 model 42 stepping 7, genuineintel systflow0 astor 0.7.1 py360 cloudpickle 1.1.1 py0 cycler 0.10.0 py36h009560c1 dask-core 1.2.2 py1 fastcluster 1.1.25 pypi0 pypi freetype 2.9.1 ha9979f80 grpcio 1.16.1 py36h351948d0 hdf5 1.10.4 h7ebc959rt 2019.0.0 h0cc432a1 imageio 2.6.1 py360 pypi intel-openmp 2019.4 245 joblib 0.13.2 py361001 conda-forge keras 2.2.4 0 keras-applications 1.0.8 py0 keras-preprocessing 1.1.0 py0 libblas 3.8.0 8mkl conda-forge liblapack 3.8.0 8mkl conda-forge libmklml 2019.0.3 0 libpng 1.6.37 h2a8f88b0 libtiff 4.0.10 hb8987942 conda-forge markdown 3.1.1 py360 mkl 2019.4 245 mkl-service 2.0.2 py36he774522fft 1.0.12 py36h14836ferandom 1.0.2 py36h343c1720 networkx 2.3 py0 numpy-base 1.17.4 py36hc3f50950 pypi olefile 0.46 py365 conda-forge opencv-python 4.1.2.30 pypi3 pathlib 1.0.1 py360 pip 19.1.1 py360 psutil 5.6.2 py36he7745220 pyqt 5.9.2 py36h65383351 python 3.6.8 h9f7ef890 pytz 2019.1 py1 pywin32 223 py36hfa6e2cd0 qt 5.9.7 vc14h73c81de0 scikit-learn 0.21.2 py36h6288b170 setuptools 41.0.1 py360 six 1.12.0 py360 tensorboard 1.13.1 py36h33f27b4py36h9006a92py36h871c8ca0 tensorflow-gpu 1.13.1 h0d30ee61 tk 8.6.8 hfa6e2cd0 toposort 1.5 pypi0 tqdm 4.32.1 py4 vs20151 werkzeug 0.15.4 py0 wincertstore 0.2 py36h7fe50ca4 yaml 0.1.7 hc54c5093 zstd 1.3.7 h508b16etransfer] clip: true preservebalance] colorspace: hsv balance2: 0.0 balancehist] threshold: 99.0 [mask.boxblend] type: normalized radius: 3.0 passes: 4 erosion: 0.0 [scaling.sharpen] method: unsharptransparent: false jpgcompresstransparent: false optimize: false gifquality: 75 pnglevel: 3 tifdeflate --------- extract.ini --------- [global] allowdnn] confidence: 50 [detect.mtcnn] minsize: 20 threshold2: 0.7 thresholddfl] batch-size: 8 [mask.vggobstructed] batch-size: 2 --------- gui.ini --------- [global] fullscreen: false tab: extract optionswidth: 30 consoleheight: 20 iconsize: 9 autosavesession: prompt timeout: 120 automodeltype: none maskkernel: 3 maskmask: false icnrawareupscaling: false reflectmaskfunction: mae learningh128] lowmem: true [model.dflsize: 128 clipnorm: true architecture: df autoencoderdims: 42 decoderdecoder: false [model.dlight] features: lowmem details: good outputsize: 64 outputnodes: 1536 complexitydecoder: 512 [model.unbalanced] inputencoder: 128 complexitya: 384 complexityb: 512 [model.villain] lowmem: true [trainer.original] previewamount: 5 rotationrange: 5 fliplightness: 30 colorclaheclahesize: 4",Error,other
656,https://github.com/streamlit/streamlit/issues/656,Hide browser buttons(inc/decr) in the numeric input,"summary in some browsers, default numeric input buttons still appearsreported behavior: solution: behavior:",Error,Error
1345,https://github.com/microsoft/recommenders/issues/1345,Getting com.microsoft.aad.msal4j.AcquireTokenSilentSupplier failed error on getting access token from clientSecret,"hi team, i am trying to get access token from clientsecret. please find below code snippet. `iclientcredential cred = clientcredentialfactory.createfromsecret(clientsecret); confidentialclientapplication app; try { // build the msal application object for a client credential flow app = confidentialclientapplication.builder(applicationid, cred ).authority(authority).build(); } catch (malformedurlexception e) { system.out.println(""error creating confidential client: "" + e.getmessage()); return null; } iauthenticationresult result; try{ silentparameters silentparameters = silentparameters.builder(scopeset).build(); result= app.acquiretokensilently(silentparameters).join(); } catch (exception ex ){ if (ex.getcause() instanceof msalexception) { clientcredentialparameters parameters = clientcredentialparameters .builder(scopeset) .build(); // try to acquire a token. if successful, you should see // the token information printed out to console result = app.acquiretoken(parameters).join(); } else { // handle other exceptions accordingly system.out.println(""unable to authenticate = "" + ex.getmessage()); return null; } }` but while running i am getting below error, can you please suggest me when changes i have to make. i am using msal4j-1.9.1.jar. [forkjoinpool.commonpool-worker-1] error com.microsoft.aad.msal4j.confidentialclientapplication - [correlation id: 7c7ce67e-*********] execution of class com.microsoft.aad.msal4j.acquiretokensilentsupplier failed. com.microsoft.aad.msal4j.msalclientexception: java.net.sockettimeoutexception: connect timed out",question,question
1865,https://github.com/streamlit/streamlit/issues/1865,Create and Replace Alert Component,"we know of one location for the alert component. * instead, we will use baseweb's goal is to mimic the same look and feel as before and announce if there are any changes that are unattainable. please display before and after images in pull requests.",other,other
369,https://github.com/deepfakes/faceswap/issues/369,faceswap gui deletes save file if it's in the same directory than the model files,"i saved training parameters for later use with the gui, so it created a .fsw file. everything was cool. i looked in the file (it's a txt file) and it looked ok. at some point, python crashed for a reason or another (or the pc rebooted) and when i relaunched the gui and wanted to load the settings, the .fsw file was nowhere to be found. it happened twice, so i think there's something fishy. on the other hand, when i quit the gui ""properly"", the file stays there.",Error,question
331,https://github.com/microsoft/recommenders/issues/331,papermill: need to specify kernel when running notebook from another notebook,"as the title says, when executing notebook from another notebook, need to specify the kernelsize in datanotebook( 99 notebooks[algorithm], 100 outputk=k, movielenssize=dataname = ""reconame = ' + '""' + kernel.name + '""') to set the kernel - look at stackoverflow for explanation.",other,other
3570,https://github.com/streamlit/streamlit/issues/3570,Running two functions simultaneously,"hello everyone, i have a query. there is a function which have 3 steps, step a, step b, step c the above function is initiated when we click a ubmit i need to update the progress bar as steps gets completed in the backend (running on different port) of the code. currently, i am appending the status of the three steps on the .txt file and the streamlit app is reading it and updating the progress bar. i want to do perform both the action simultaneously. can you please help me how to proceed ? below is the sample code i am trying, however the progress bar is updated after the completion of the function, is it possible to run both the functions parallelly ? ` import os import time import streamlit as st dir = r""d:\\workspace\backend"" progresstext = st.empty() status1.txt"", ""w"") def processfile.write(""step a completed 200\n"") time.sleep(10) statusfile.write(""step c completed 200\n"") statusdata() while true: time.sleep(5) with open(dir + os.path.sep + ""statusdata = f.readlines() print(""2"", statusdata) == 0: progresstext.text('starting') elif len(statusbar.progress(33) statusdata) == 2: progresstext.text('completed step b') elif len(statusbar.progress(100) status_text.text('completed step c') break ` is there any better way to do it ? appreciate your help and time. thank you.",question,question
693,https://github.com/deepfakes/faceswap/issues/693,bug,"```04/04/2019 17:54:34 mainprocess mainthread _paramsfile debug loaded rc file /home/lw/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc 04/04/2019 17:54:34 mainprocess mainthread _frozenimp', 'thread', 'frozenexternal', 'codecs', 'encodings.aliases', 'encodings.utfsignal', '_1', 'io', 'abc', 'stat', 'posixpath', 'genericpath', 'os.path', 'abc', 'sysconfigdatalinux64-linux-gnu', 'locale', 'types', 'functools', 'operator', 'keyword', 'heapq', 'collections', 'weakref', 'collections.abc', 'importlib', 'importlib.bootstraptoolkits', 'google', 'sphinxcontrib', 'lib', 'lib.cli', 'argparse', 'copy', 'copyreg', 're', 'enum', 'sresre', 'sreconstants', 'textwrap', 'gettext', 'locale', 'struct', 'string', 'threading', 'atexit', 'platform', 'subprocess', 'signal', 'socket', 'pickle', 'pickle', 'datetime', 'lib.queuemain_compression', 'lzma', 'pwd', 'grp', 'random', 'hashlib', 'blake2', 'bisect', 'multiprocessing', 'multiprocessing.util', 'multiprocessing.pool', 'multiprocessing.popencommon', 'psutil.pslinux', 'base64', 'binascii', 'glob', 'psutil.psutilpsutilstats', 'pynvml', 'ctypes', 'endian', 'ctypes.util', 'lib.utils', 'pathlib', 'ntpath', 'urllib', 'urllib.parse', 'cv2', 'numpy', 'numpy.importnewdocs', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.typeinternal', 'numpy.compat', 'numpy.compat.methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.functionbase', 'numpy.core.einsumfunc', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'difflib', 'pprint', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing.private.utils', 'gc', 'numpy.lib.utils', 'numpy.testing.private.nosetester', 'numpy.testing.tricks', 'numpy.lib.functionbase', 'numpy.lib.histograms', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'ast', 'lite', 'numpy.linalg.linalg', 'numpy.lib.stridebase', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib.iotools', 'numpy.lib.financial', 'decimal', 'version', 'numpy.core.tests', 'numpy.init', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpackpolybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermiteruntime', 'mtrand', 'numpy.random.mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'cv2.error', 'cv2.detail', 'cv2.dnn', 'cv2.fisheye', 'cv2.flann', 'cv2.instr', 'cv2.ipp', 'cv2.ml', 'cv2.ocl', 'cv2.ogl', 'cv2.videostab', 'cv2.cv2', 'dlib.cuda', 'dlib.imagemetadata', 'dlib', 'lib.faceseyes', 'plugins', 'plugins.plugintensorflow', 'tensorflow.python.platform', 'tensorflow.python.platform.selfinfo', 'tensorflow.python.pywrapinternal', 'imp', 'swigdata4', 'tensorflowlogging', 'six', 'tensorflow.python.util.tfdecorator', 'tensorflow.python.util.decoratorinmode', 'tensorflow.python.util.tfinspect', 'inspect', 'dis', 'opcode', 'apiestimator', 'tensorflowapi', 'tensorflowapi.v1', 'tensorflowapi.v1.estimator', 'tensorflowapi.v1.estimator.experimental', 'tensorflowestimator.python.estimator', 'tensorflowlib', 'tensorflowestimator.python.estimator.canned.baseline', 'tensorflow.python.featurecolumn.featurepb2', 'google.protobuf', 'pkgpolicybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email.resources.extern', 'pkgvendor', 'pkgresources.resources.extern.six.moves', 'pkgvendor.six.moves', 'pkgresources.extern.appdirs', 'pkgvendor.packaging._resources.extern.packaging', 'pkgresources.extern.packaging.resources.extern.packaging.specifiers', 'pkgcompat', 'pkgresources.extern.pyparsing', 'pkgresources.extern.packaging.markers', 'google.protobuf.descriptor', 'google.protobuf.internal', 'google.protobuf.internal.apiapitypemessage', 'google.protobuf.reflection', 'google.protobuf.messagepool', 'google.protobuf.descriptorencoding', 'google.protobuf.pyext.cppdatabase', 'tensorflow.core.framework', 'tensorflow.core.framework.costpb2', 'tensorflow.core.framework.tensorpb2', 'google.protobuf.internal.welltypes', 'tensorflow.core.framework.typespb2', 'tensorflow.core.framework.nodepb2', 'tensorflow.core.framework.attrpb2', 'tensorflow.core.framework.tensorhandlepb2', 'tensorflow.core.framework.oppb2', 'tensorflow.core.framework.versionsstatsdescriptiondescriptionpb2', 'tensorflow.core.protobuf.clusterconfigapidefimpl', 'tensorflow.core.lib', 'tensorflow.core.lib.core', 'tensorflow.core.lib.core.errorpb2', 'tensorflow.python.eager.tape', 'tensorflow.python.framework.cppinferenceinterpolation', 'tensorflow.python.util.tfdefshape', 'tensorflow.python.framework.traceableflowargumentcsv', 'absl.flags.defines', 'absl.flags.flag', 'absl.flags.validators', 'tensorflow.python.util.functionutil', 'tensorflow.python.util.memory', 'tensorflow.python.framework.sparseutil', 'tensorflow.python.framework.fastutil', 'tensorflow.python.keras', 'tensorflow.python.keras.activations', 'tensorflow.python.keras.backend', 'json', 'json.decoder', 'json.scanner', 'ops', 'tensorflow.python.ops.arrayshapes', 'tensorflow.python.framework.constantformat', 'google.protobuf.internal.typeformat', 'tensorflow.python.ops.genops', 'tensorflow.python.framework.oplibrary', 'tensorflow.python.ops.genops', 'tensorflow.python.util.nest', 'tensorflow.python.ops.genflowops', 'tensorflow.python.ops.genops', 'tensorflow.python.ops.mathutil', 'tensorflow.python.framework.graphimpl', 'tensorflow.python.ops.genops', 'tensorflow.python.ops.genops', 'tensorflow.python.ops.numerics', 'tensorflow.python.ops.controlops', 'tensorflow.core.protobuf.controlpb2', 'tensorflow.python.ops.condimpl', 'tensorflow.python.ops.genops', 'tensorflow.python.ops.genflowloggingresourceops', 'tensorflow.python.ops.tensorops', 'tensorflow.python.util.tfuse', 'tensorflow.python.ops.ctcctcgrad', 'tensorflow.python.ops.gradientstodef', 'tensorflow.python.ops.resourceops', 'tensorflow.core.framework.variablestateops', 'tensorflow.python.training', 'tensorflow.python.training.checkpointable', 'tensorflow.python.training.checkpointable.base', 'tensorflow.python.ops.genops', 'tensorflow.python.training.saveablescope', 'tensorflow.python.ops.initlinalgopsops', 'tensorflow.python.framework.randomrandomgrad', 'tensorflow.python.ops.sparseops', 'tensorflow.python.ops.controlgrad', 'tensorflow.python.ops.functionalgrad', 'tensorflow.python.ops.genops', 'tensorflow.python.ops.linalgops', 'tensorflow.python.ops.linalg', 'tensorflow.python.ops.linalg.linalgmathops', 'tensorflow.python.ops.stringstringgrad', 'tensorflow.python.ops.manipmanipgrad', 'tensorflow.python.ops.randomgrad', 'tensorflow.python.ops.spectralops', 'tensorflow.python.ops.gradients', 'tensorflow.python.eager.function', 'tensorflow.python.eager.graphops', 'tensorflow.python.framework.tensorstrategyloader', 'tensorflow.python.eager.backprop', 'tensorflow.python.eager.imperativegradient', 'tensorflow.python.ops.imageopsops', 'tensorflow.python.ops.datagrad', 'tensorflow.python.ops.dataops', 'tensorflow.python.lib', 'tensorflow.python.lib.io', 'tensorflow.python.lib.io.pythonrecord', 'tensorflow.python.ops.nnsamplingcandidateops', 'tensorflow.python.ops.tensorgrad', 'tensorflow.python.keras.utils', 'tensorflow.python.keras.utils.dataencodedssl', 'tensorflow.python.keras.utils.genericutils', 'h5py', 'h5py.conv', 'h5py.h5r', '01', 'h5py.proxy', 'h5py.h5d', 'h5py.h5ds', 'h5py.h5f', 'h5py.h5g', 'h5py.h5i', 'h5py.h5fd', 'h5py.hl.filters', 'h5py.hl.compat', 'h5py.hl.group', 'h5py.h5o', 'h5py.h5l', 'h5py.hl.selections', 'h5py.hl.datatype', 'h5py.version', 'h5py.attrs', 'h5py.highlevel', 'h5py.tests.old.testdata', 'h5py.tests.old.testdataset', 'h5py.tests.old.testdimensionfile', 'h5py.tests.old.testimage', 'h5py.tests.old.testh5', 'h5py.tests.old.testh5p', 'h5py.tests.old.testobjects', 'h5py.tests.old.testslicing', 'h5py.tests.hl', 'h5py.tests.hl.testgetitem', 'h5py.tests.hl.testswmr', 'h5py.tests.hl.testdimensionproxy', 'h5py.tests.hl.testattributethreads', 'h5py.tests.hl.testutils', 'tensorflow.python.keras.utils.convgpulayer', 'tensorflow.python.keras.constraints', 'tensorflow.python.keras.initializers', 'tensorflow.python.keras.regularizers', 'tensorflow.python.keras.utils.tfcond', 'tensorflow.tools', 'tensorflow.tools.docs', 'tensorflow.tools.docs.doclayer', 'tensorflow.python.keras.engine.training', 'tensorflow.python.data', 'tensorflow.python.data.experimental', 'tensorflow.python.data.experimental.ops', 'tensorflow.python.data.experimental.ops.batching', 'tensorflow.python.data.experimental.ops.getelement', 'tensorflow.python.data.ops', 'tensorflow.python.data.ops.datasetops', 'tensorflow.python.data.ops.optionaldatasetobjectpb2', 'tensorflow.core.protobuf.metapb2', 'google.protobuf.anypb2', 'tensorflow.python.framework.metaio', 'tensorflow.python.lib.io.fileops', 'tensorflow.python.platform.gfile', 'tensorflow.python.training.checkpointutil', 'tensorflow.python.training.checkpointpb2', 'tensorflow.python.data.util.randomops', 'tensorflow.python.ops.genops', 'tensorflow.python.data.experimental.ops.grouping', 'tensorflow.python.data.util.convert', 'tensorflow.python.data.experimental.ops.counter', 'tensorflow.python.data.experimental.ops.scanops', 'tensorflow.python.data.experimental.ops.errorexperimentalops', 'tensorflow.python.data.experimental.ops.interleaveops', 'tensorflow.python.data.ops.readers', 'tensorflow.python.ops.genrandomops', 'tensorflow.python.training.basicrunpb2', 'tensorflow.core.util', 'tensorflow.core.util.eventrunio', 'tensorflow.python.summary', 'tensorflow.python.summary.summaryasset', 'tensorflow.python.summary.writer.eventwriter', 'tensorflow.python.summary.writer.eventwriteropssummaryopcache', 'tensorflow.python.data.experimental.ops.optimization', 'tensorflow.python.data.experimental.ops.parsingops', 'tensorflow.python.ops.genops', 'tensorflow.python.data.experimental.ops.prefetchingops', 'tensorflow.python.data.experimental.ops.resampling', 'tensorflow.python.data.experimental.ops.statsutil', 'tensorflow.python.keras.losses', 'tensorflow.python.keras.metrics', 'tensorflow.python.ops.confusionbroadcastimpl', 'tensorflow.python.ops.genops', 'tensorflow.python.keras.optimizers', 'tensorflow.python.training.optimizer', 'tensorflow.python.training.distribute', 'tensorflow.python.ops.losses', 'tensorflow.python.ops.losses.lossesutil', 'tensorflow.python.training.slottrainingutils', 'tensorflow.python.summary.summary', 'google.protobuf.jsonops', 'tensorflow.python.summary.textmatchclient', 'urllib3.connection', 'urllib3.util', 'urllib3.util.connection', 'urllib3.util.wait', 'urllib3.util.selectors', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.sslcollections', 'urllib3.request', 'urllib3.filepost', 'urllib3.fields', 'mimetypes', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.response', 'urllib3.poolmanager', 'chardet', 'chardet.compat', 'chardet.universaldetector', 'chardet.charsetgroupprober', 'chardet.enums', 'chardet.charsetprober', 'chardet.escprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.latin1prober', 'chardet.mbcsgroupprober', 'chardet.utf8prober', 'chardet.mbcssm', 'chardet.sjisprober', 'chardet.mbcharsetprober', 'chardet.chardistribution', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.jpcntx', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.sbcsgroupprober', 'chardet.sbcharsetprober', 'chardet.langcyrillicmodel', 'chardet.langgreekmodel', 'chardet.langbulgarianmodel', 'chardet.langthaimodel', 'chardet.langhebrewmodel', 'chardet.hebrewprober', 'chardet.langturkishmodel', 'chardet.version', 'requests.exceptions', 'urllib3.contrib', 'urllib3.contrib.pyopenssl', 'openssl', 'openssl.rand', 'openssl.cffiopenssl.lib', 'openssl', 'cryptography.hazmat.bindings.openssl.transparency', 'cryptography.x509.base', 'cryptography.hazmat.primitives', 'cryptography.hazmat.primitives.asymmetric', 'cryptography.hazmat.primitives.asymmetric.dsa', 'cryptography.hazmat.primitives.asymmetric.ec', 'cryptography.hazmat.primitives.asymmetric.rsa', 'cryptography.hazmat.backends', 'cryptography.hazmat.backends.interfaces', 'cryptography.x509.extensions', 'asn1crypto', 'asn1crypto.version', 'asn1crypto.keys', 'asn1crypto.curve', 'asn1crypto.errors', 'asn1crypto.types', 'asn1crypto.ffi', 'asn1crypto.perf.numteletextime', 'time.lib', 'time', 'cryptography.hazmat.bindings.time', 'cryptography.hazmat.primitives.serialization', 'cryptography.x509.generaldata', 'idna.core', 'idna.idnadata', 'idna.intranges', 'cryptography.x509.name', 'cryptography.x509.oid', 'cryptography.hazmat.primitives.hashes', 'openssl.ssl', 'openssl.version', 'cryptography.hazmat.backends.openssl', 'cryptography.hazmat.backends.openssl.backend', 'cryptography.hazmat.backends.openssl.aead', 'cryptography.hazmat.backends.openssl.ciphers', 'cryptography.hazmat.primitives.ciphers', 'cryptography.hazmat.primitives.ciphers.base', 'cryptography.hazmat.primitives.ciphers.modes', 'cryptography.hazmat.backends.openssl.cmac', 'cryptography.hazmat.primitives.mac', 'cryptography.hazmat.backends.openssl.dh', 'cryptography.hazmat.primitives.asymmetric.dh', 'cryptography.hazmat.backends.openssl.dsa', 'cryptography.hazmat.backends.openssl.utils', 'cryptography.hazmat.primitives.asymmetric.utils', 'cryptography.hazmat.backends.openssl.ec', 'cryptography.hazmat.backends.openssl.encodeasn1', 'cryptography.hazmat.backends.openssl.hashes', 'cryptography.hazmat.backends.openssl.hmac', 'cryptography.hazmat.backends.openssl.rsa', 'cryptography.hazmat.primitives.asymmetric.padding', 'cryptography.hazmat.backends.openssl.x25519', 'cryptography.hazmat.primitives.asymmetric.x25519', 'cryptography.hazmat.backends.openssl.x509', 'cryptography.hazmat.primitives.ciphers.algorithms', 'cryptography.hazmat.primitives.kdf', 'cryptography.hazmat.primitives.kdf.scrypt', 'urllib3.packages.backports', 'urllib3.packages.backports.makefile', 'requests._internalmatchclient', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.selectors', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.sslcollections', 'requests.packages.urllib3.request', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.response', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3.contrib', 'requests.packages.urllib3.contrib.pyopenssl', 'requests.packages.urllib3.packages.backports', 'requests.packages.urllib3.packages.backports.makefile', 'requests.packages.idna', 'requests.packages.idna.packagecodes', 'requests.api', 'requests.sessions', 'requests.adapters', 'urllib3.contrib.socks', 'socks', 'tensorflow.python.keras.engine.traininglib', 'scipy.version', 'scipy.lib.lib.c', 'scipy.sparse', 'scipy.sparse.base', 'scipy.sparse.sputils', 'scipy.sparse.csr', 'scipy.sparse.lib.csparsetools', 'scipy.sparse.dok', 'scipy.sparse.coo', 'scipy.sparse.bsr', 'scipy.sparse.construct', 'scipy.sparse.extract', 'scipy.sparse.io', 'scipy.sparse.csgraph', 'scipy.sparse.csgraph.laplacian', 'scipy.sparse.csgraph.path', 'scipy.sparse.csgraph.tools', 'scipy.sparse.csgraph.mintree', 'scipy.sparse.csgraph.distributed', 'tensorflow.python.keras.engine.traininggenerator', 'tensorflow.python.keras.engine.network', 'tensorflow.python.keras.engine.saving', 'yaml', 'yaml.error', 'yaml.tokens', 'yaml.events', 'yaml.nodes', 'yaml.loader', 'yaml.reader', 'yaml.scanner', 'yaml.parser', 'yaml.composer', 'yaml.constructor', 'yaml.resolver', 'yaml.dumper', 'yaml.emitter', 'yaml.serializer', 'yaml.representer', 'yaml.cyaml', 'structures', 'tensorflow.python.training.checkpointable.layerutils', 'tensorflow.python.keras.utils.visapplications', 'kerasapplications.imagenetapplications.vgg19', 'kerasapplications.inceptionapplications.inceptionv2', 'kerasapplications.mobilenet', 'kerasv2', 'kerasapplications.nasnet', 'kerasapplications.resnetapplications.resnetapplications.resnext', 'tensorflow.python.keras.layers', 'tensorflow.python.keras.layers.advanced_activations', 'tensorflow.python.keras.layers.convolutional', 'tensorflow.python.keras.layers.pooling', 'tensorflow.python.keras.layers.co",other,other
2145,https://github.com/streamlit/streamlit/issues/2145,Components keep refreshing/remounting when in a column,"summary with or without a `key`, a component in a column will eventually refresh/remount, sometimes lock itself into a refresh loop. not sure but seems to depend on the browser (chrome/edge stop refreshing at some point but if you interact with the component it goes off again. firefox stays in loop forever).steps to reproduce ** component will stop at state 1. expected behavior: if there's a key, no remount. if there's no key, as usual.actual behavior: is this a regression? using the drawable canvas example, i went back to the @akrolsmir :heavymark: works on streamlit-0.65.24-py2.py3-none-any.whl :red_square: doesn't work on streamlit-0.66.22-py2.py3-none-any.whl",Error,Error
2990,https://github.com/streamlit/streamlit/issues/2990,Incorrect warning printed when setting server config options via CLI flag,"this bug is actually caused by #2989, but i'm filing it separately as fixing #2989 may end up being a few days worth of effort while a small workaround to fix this apparently incorrect behavior (that's just a bandaid until the real issue is fixed) would likely only take a couple hours. what happens here is that a file calls `config.getconfig_options` later gets called to set config options set via cli flag, a change to the `[server]` section is detected and thus a warning about this is incorrectly printed.",Error,other
657,https://github.com/microsoft/recommenders/issues/657,[BUG] on VW integration tests,"description two days ago it worked, now: in which platform does it happen?",Error,deployment
1498,https://github.com/streamlit/streamlit/issues/1498,CircleCI fails periodically,investigate the following circleci failures: - - - possible issue: -,other,other
98,https://github.com/deezer/spleeter/issues/98,[Discussion] Demo Colab notebook with 2 min song snippet (Aadum Neram by Ilaiyaraja 1988),"spleeter team, i took spleeter 2stem and 5stem models for a test drive on a composer i simply love. i have created a notebook with step by step conversion of karaoke and then 5 stem, converting to high quality mp3 and added my observations on each of the 5 tracks. as i have noted, this output is truly more coherent and confident than commercial option i tried! it is one click test run and will produce tracks in sub folders as shown. i will also upload the snippets onto my git for people who dont want to take trouble running the code, but want to see results. i am truly very impressed by the technology and outputs and the generosity of your sharing. this technology of source separation will become a boon to musicians, composers, arrangers and producers and entertain people in new ways. note: if you think this discussion is out of scope for git discussion, please feel free to close or i can delete. thanks ravi",other,other
742,https://github.com/iperov/DeepFaceLab/issues/742,XSeg training fails to start,"this is pretty much a copy paste of pylon69's post since i get the exact same problem. i tried his solution by moving dfl to the os drive but i keep getting the same error. problem still exists on the newest version (build06_2020) downloaded from mega ** windows 10 amd 3900x rtx 2070 super latest driver 32gb 3600mhz ram edit: @iperov mentioned paging file is too small in issue #735 which is the same as my issue, i thought i should say that my paging file is 4.9gb.",question,question
311,https://github.com/microsoft/recommenders/issues/311,Refactor / clean up readme & setup,is affected by this bug? 1. 2. 3. ... on the platform does it happen? 1. 2. 3. do we replicate the issue? () 1. 2. 3. ... expected behavior (i.e. solution) 1. the tests for sar pyspark should pass successfully. other comments,other,other
784,https://github.com/deepfakes/faceswap/issues/784,Training from A - B,i can observe that the tool is training data for a to b and b to a both. but we would like to use it only for a to b. so it can train fast.,other,other
25,https://github.com/streamlit/streamlit/issues/25,Create docker environment to run Cypress tests in,goal is to have an environment that we can run cypress tests in that matches the circleci environment,other,other
3764,https://github.com/streamlit/streamlit/issues/3764,streamlit 0.88.0 upload button,"summary hi, when i use a code which was working with version 0.85 to upload some file, i get the error: attributeerror: 'google.protobuf.pyext.fileuploader.py"", line 243. here is the full error message: traceback: file ""/opt/software/miniconda/lib/python3.6/site-packages/streamlit/scriptrunsessioncallbacks() file ""/opt/software/miniconda/lib/python3.6/site-packages/streamlit/state/sessioncallbacks wid for wid in self.widgetwidgetstate.py"", line 399, in wid for wid in self.widgetwidgetstate.py"", line 405, in changed newnewstate.get(widgetcollectionsstate.py"", line 120, in _uploader.py"", line 166, in deserializeuploader filegetrecs(widgetvalue) file ""/opt/software/miniconda/lib/python3.6/site-packages/streamlit/elements/filegetrecs uploadedinfo = widgetfilefile = st.file_uploader(""upload your data"", type= (""xlsx"",""csv""))",question,Error
3741,https://github.com/streamlit/streamlit/issues/3741,RaspberryPi4 deployed app loading stuck with Nginx Proxy Manager & Docker,"hi, i'm trying to deploy my streamlit app via docker on my raspberry pi 4 with my personal router. everything is containerized with docker and are working well. i use `nginx proxy manager` as a secure reverse proxy to share the app on internet at , but i can't make the app loading. the app is running well locally on `:8501` but not loading on the remote link. - streamlit dockerfile: - streamlit config.toml here is the screenshot from the stuck app: here is the screenshot of the network tab in dev console showing `stream` pending requests: i tried all things advised in the streamlit docs without success finally here is the nginx config file, maybe the problem is coming from this file, but i'm not getting it: `192.168.1.40` is the internal raspberry pi ip adress and `8501` the streamlit app port. if someone has the solution, that would be awesome! :) p.s.: obviously ports 80 and 443 are forwarded on my router.",deployment,question
454,https://github.com/microsoft/recommenders/issues/454,Nightly build failed,"is affected by this bug? nightly build on staging branch in platform does it happen? the devops machine. do we replicate the issue? expected behavior (i.e. solution) successful pass. other comments checking the failure details, it looks like some issue with fast ai notebook.",deployment,deployment
2674,https://github.com/streamlit/streamlit/issues/2674,st.beta_columns doesn't document use_column_width parameter,"in the example, the parameter `usewidth` is presented, but it's not part of the docstring. add the parameter to the docstring and make it clear its usage.",other,Error
2448,https://github.com/streamlit/streamlit/issues/2448,No API reference of experimental functions,"** in the documentation, there is no list of the various `st.experimental_function`. the docs mention that there are experimental functions that can be used for additional functionality, but i cannot find what options there are. a good place to add them would be under the",other,other
129,https://github.com/streamlit/streamlit/issues/129,Blacklist common virtualenv folders by default,"we should blacklist the folders below from being watched by streamlit. this would fix the issue where some people hit the inotify watch limit when running streamlit from a weird working directory. /.virtualenv /.venv /anaconda3 /anaconda2 /miniconda3 /miniconda2 ` see also the config option `server.folderwatchblacklist`. for this fix, you can probably use the same mechanism this config option uses.",other,other
1118,https://github.com/microsoft/recommenders/issues/1118,[HELP] als_deep_dive problem with Spark,"description i am using the alsdive notebook and fiddling with the different size of the spark instance, and the iterations of the als algorithm. everything works until i increase the iterations from 15 to 20, or i change the memory from 16 to 32, in this line `spark = startget_spark(""als deep dive"", memory=""32g"")` as an example, with max iterations = 20 and memory=32gb, i get the following error after running the evaluation cell (sparkratingevaluation). the error is super long, so i am just pasting the errors that don't repeat. this is my environment: can you help me figuring out how i can increase the iterations with which i can train the als algorithm? thanks",Error,question
279,https://github.com/deezer/spleeter/issues/279,Pandas Error,"hi, i'm trying to follow the examples shown on the readme file with: ` spleeter separate -i myfile.wav -p spleeter:2stems -o output but keep getting and error msg: ` from pandas.core.indexers import checkindexer importerror: cannot import name 'checkindexer' from 'pandas.core.indexers' (/home/eli/anaconda3/lib/python3.7/site-packages/pandas/core/indexers.py) ` ` any idea why this is happening?",question,question
165,https://github.com/mozilla/TTS/issues/165,Amount of data to train own voice,"hey, i am trying to build a model with your version of tacotron and my own data in us english. as i am collecting data and formatting it, i am wondering what is the amount of data necessary to start getting some good results for the target voice? does anyone know any empirical experiments so that i can set a target. i have around 3/4h right now. thanks for the hard work on the repo",question,question
657,https://github.com/streamlit/streamlit/issues/657,number_input for floats switches to int after the value in the widget is changed.,"summary after the value is changed in the numberinput was not in the prev version...)debug info - streamlit version: 0.49.0 - python version: 3.6.8 - python -m venv venv - os version: ubuntu 18.04 inside wsl under windows 10 - browser version: firefox 70.0.1 (64-bit), chrome 78.0.3904.70 (official build) (64-bit) (cohort: stable)",Error,Error
703,https://github.com/mozilla/TTS/issues/703,UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 3230: character maps to <undefined>,"when i tried to run the below command i'm getting this error tts --text ""text for tts hello how are you."" --modelmodels/en/ljspeech/glow-tts --vocodermodels/universal/libri-tts/wavegrad --outentryscripts', 'tts')()) file ""c:\python\python37\scripts\tts-script.py"", line 25, in importlibentrymetadata\_module(match.group('module')) file ""c:\python\python37\lib\importlib\_module return gcdutils import setupvocoderutils.py"", line 5, in from matplotlib import pyplot as plt file ""c:\python\python37\lib\site-packages\matplotlib\pyplot.py"", line 71, in from matplotlib.backends import pylabstack() matplotlib.use('agg') 2021-05-17 17:21:57.209037: w tensorflow/streamloader.cc:59] could not load dynamic library 'cudart64101.dll not found 2021-05-17 17:21:57.225132: i tensorflow/streamstub.cc:29] ignore above cudart dlerror if you do not have a gpu set up on your machine. > ttsmodels/universal/libri-tts/wavegrad is already downloaded. traceback (most recent call last): file ""c:\python\python37\scripts\tts-script.py"", line 33, in sys.exit(loadpoint('tts', 'consolepath, configpath, vocoderpath, args.usecuda) file ""c:\xampp\htdocs\tts\tts\utils\synthesizer.py"", line 80, in loadconfig = loadconfig) file ""c:\xampp\htdocs\tts\tts\utils\io.py"", line 46, in loadjsoncomments(configjsoncomments inputdecode(input,self.errors,decoding_table)[0] unicodedecodeerror: 'charmap' codec can't decode byte 0x81 in position 3230: character maps to below is my terminal screenshot",question,question
2954,https://github.com/streamlit/streamlit/issues/2954,fggfhfghfh,"problem is your feature request related to a problem? please describe the problem here. ex. i'm always frustrated when [...]solution ** if you don't like the mvp above, tell us why, and what you'd like done instead.additional context add any other context or screenshots about the feature request here. for example, did this fr come from or another site? link the original source here!",other,other
29,https://github.com/iperov/DeepFaceLab/issues/29,No training data provided,"expected behavior a: 745 png b: 552 png python main.py train --training-data-src-dir a\ --training-data-dst-dir b\ --model-dir m\ --model liaef128 actual behavior 00747.png - no embedded faceswap info found required for training loading: 100%██████████████████████████████████████████████████████████████████████████████████████████████████████ 745/745 [00:01<00:00, 509.29it/s] loading: 100%██████████████████████████████████████████████████████████████████████████████████████████████████████ 552/552 [00:00<00:00, 569.36it/s] traceback (most recent call last): file ""c:\python36\lib\multiprocessing\process.py"", line 258, in utils.py"", line 39, in processfunc valueerror: no training data provided. steps to reproduce 1.python main.py extract --input-dir input --output-dir output --detector mt copy output/.png 2.python main.py sort --input-dir sort --by hist-blur del some blur face copy sort/.png 3.python main.py train --training-data-src-dir a\ --training-data-dst-dir b\ --model-dir m\ --model liaef128 other relevant information - ** 3.6",other,other
711,https://github.com/deepfakes/faceswap/issues/711,dfaker and mask size,"i was just wondering if dfaker is still the model with the best potential for largest mask size and if the others can be increased to cover a similar mask. i think at the moment the jaw line and facial construction is the biggest weakness in this, besides the colors in villain.",Performance,question
104,https://github.com/deezer/spleeter/issues/104,[Bug] Spleeter seems to be in an endless loop while separating audio,"i'm trying to separate an audio into 2 stems, using `spleeter.separator.separatefile`. installed using pip. run via `py`. got this error (i get this error repeatedly, until i terminate the script): `audio.mp3` is the . ----------------- ------------------------------- os windows installation type pip ram available 32 gb, about 20 gb available hardware spec gpu: amd radeon r9 200 series, cpu: intel i5-4460",Performance,question
942,https://github.com/deepfakes/faceswap/issues/942,preview stoped,"** no creash report found in the faceswap folder, but some call stack print to the console: exception in tkinter callback traceback (most recent call last): file ""/home/bd/miniconda3/envs/faceswap/lib/python3.6/tkinter/_page.py"", line 248, in file ""/home/bd/faceswap/lib/gui/displaypage file ""/home/bd/faceswap/lib/gui/displayitemlatestloadto_cache file ""/home/bd/miniconda3/envs/faceswap/lib/python3.6/site-packages/pil/image.py"", line 1890, in resize file ""/home/bd/miniconda3/envs/faceswap/lib/python3.6/site-packages/pil/imagefile.py"", line 249, in load oserror: image file is truncated (53 bytes not processed)",question,Error
700,https://github.com/mozilla/TTS/issues/700,about hinge loss,has anyone tried this loss? how about its performance?,other,question
3586,https://github.com/streamlit/streamlit/issues/3586,Allow initialization of range slider with session state,"summary i want to be able to create a slider and a checkbox where if the checkbox is unclicked, the slider functions as normal with some default value `val1`, but if the checkbox is clicked, the slider is locked in to a default value `val2` and resets to this value with every rerun, independent of user action. this functionality is possible with a standard slider using session state and key to initialize the slider value depending on the value of the checkbox, however, streamlit throws an error if you attempt to use session state to initialize a tuple of initial values for the slider (i.e. to create a rangeval2` is checked, and 70-90 and fixed if `use_val2` is not checked. ** raises `typeerror: (30, 50) has type tuple, but expected one of: int, long, float`is this a regression? nodebug info - streamlit version: 0.84.0 - python version: 3.8.11 - using: pip - os version: windows 10 - browser version: chrome",Error,other
1091,https://github.com/streamlit/streamlit/issues/1091,Streamlit embedded app for Altair has tooltip but not the code,"from we found out a discreapancy between the streamlit embedded app for altair chart and the relevant altair code. the code reads : so there is no tooltip, but in the embedded app there is a tooltip appearing. so i guess the correct code should read : * ** for altair : for vega-lite : also so i could find source file for a streamlit vega-lite but there's no tooltip so i guess that's not the one which generated the embedded app. could not find `charts.altair_chart.py` nor `text.write4.py` so i'm not sure which source file generated the app.",other,other
1022,https://github.com/deepfakes/faceswap/issues/1022,Windows installer broken,"** faceswap is not installing , so they have no directory for storing crash report",question,Error
285,https://github.com/deezer/spleeter/issues/285,FileNotFoundError: [WinError 2] The system cannot find the file specified,"c:\users\amy\desktop\vaan>python3 -m spleeter separate -i c:\users\amy\desktop\vaan\vaan.mp3 -p spleeter:4stems -o ""e:\output"" traceback (most recent call last): file ""c:\python3\lib\runpy.py"", line 193, in modulemain file ""c:\python3\lib\runpy.py"", line 85, in code file ""c:\python3\lib\site-packages\spleeter\_toprobe.py"", line 20, in probe file ""c:\python3\lib\subprocess.py"", line 729, in _execute_child filenotfounderror: [winerror 2] the system cannot find the file specified",question,other
126,https://github.com/iperov/DeepFaceLab/issues/126,"Something is wrong with SAE converter, strange artifacts...","this is not tech support for newbie fakers post only issues related to bugs or code expected behavior just merging as usual. actual behavior *= imgbordermaskinternal\bin\deepfacelab\models\convertermasked.py:243: runtimewarning: invalid value encountered in multiply imgblurryprdrecta converting: 71%################################################3 116/163 [00:07<00:02, 21.36it/s]c:\users\plague\documents\deepfacelabtorrent\maskaaa *.",Error,other
778,https://github.com/deepfakes/faceswap/issues/778,pynvml.NVMLError_LibraryNotFound: NVML Shared Library Not Found,"traceback (most recent call last): file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 644, in handle = name, mode) oserror: libnvidia-ml.so.1: cannot open shared object file: no such file or directory during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/data/github/faceswap-master/lib/cli.py"", line 123, in executemethod=normalization) file ""/data/github/faceswap-master/plugins/extract/pipeline.py"", line 36, in _parallel = self.setprocessing(multiprocess) file ""/data/github/faceswap-master/plugins/extract/pipeline.py"", line 109, in setprocessing gpustats.py"", line 45, in _stats.py"", line 92, in initialize raise err file ""/data/github/faceswap-master/lib/gpuloadnvmllibrary() file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 646, in nvmlcheckreturn(nvmllibraryfound) file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 310, in librarynotfound: nvml shared library not found during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 644, in handle = name, mode) oserror: libnvidia-ml.so.1: cannot open shared object file: no such file or directory during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/data/github/faceswap-master/faceswap.py"", line 36, in arguments.func(arguments) file ""/data/github/faceswap-master/lib/cli.py"", line 130, in executefile = crashlog from lib.sysinfo import sysinfo file ""/data/github/faceswap-master/lib/sysinfo.py"", line 355, in sysinfo = sysinfo() # pylint: disable=invalid-name file ""/data/github/faceswap-master/lib/sysinfo.py"", line 21, in _stats = gpustats(log=false) file ""/data/github/faceswap-master/lib/gpustats.py"", line 92, in initialize raise err file ""/data/github/faceswap-master/lib/gpuloadnvmllibrary() file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 646, in nvmlcheckreturn(nvmllibraryfound) file ""/home/ytc/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pynvml.py"", line 310, in librarynotfound: nvml shared library not found",question,question
230,https://github.com/iperov/DeepFaceLab/issues/230,Feature Request: Image size stretching separately on x/y axis,"hi, the face resize on output feature is great and i use it often (only in the +/- 5 range). it would be nice to resize x and y axis separately and/or adding an offset value for the y axis to shift the face up or down.",other,other
351,https://github.com/iperov/DeepFaceLab/issues/351,Error in DeepFaceLabCUDA10.1AVX_build_08_16_2019,"hi there with the new version i got an error: `traceback (most recent call last): file ""m:\internal\deepfacelab\main.py"", line 252, in file ""m:\internal\deepfacelab\main.py"", line 223, in processvideosequence file ""m:\internal\deepfacelab\mainscripts\videoed.py"", line 138, in videosequence file ""m:\internal\deepfacelab\utils\pathfirstby_stem typeerror: '<' not supported between instances of 'nt.direntry' and 'nt.direntry'` any idea? get this by extracting and make the mp4 file. train and convert work.",Error,Error
427,https://github.com/microsoft/recommenders/issues/427,Unable to create an appropriately versioned cluster per instructions in reference architecture and als_movie_o16n,"is affected by this bug? creating an appropriate cluster unit tests.which do we replicate the issue? 1. 2. 3. in the databricks runtime version, there is no longer an option for db 4.1, spark 2.3.0. it was deprecated on 2019-01-17. see deprecation schedule . expected behavior (i.e. solution) workarounds: - it is still possible to create a cluster by cloning a cluster of the recommended version. - it is still possible to create a cluster through the api. happy to do pr with appropriate json for creating with databricks cli or directly through the rest api. other comments have we tested whether the cosmosdb connector jar works with more current versions of adb and spark?",other,other
94,https://github.com/deezer/spleeter/issues/94,[Feature] offline app or a web based service,"description it will be a nice feature to have a simple web interface for the project. making it easier for non-it users to benefit from. additional information i noticed a docker folder in the project, this will make the deployment easier already.",other,other
1117,https://github.com/deepfakes/faceswap/issues/1117,No GPU detected. Switching to CPU mode,"an unhandled exception occured loading pynvml. original error: uninitialized no gpu detected. switching to cpu mode 1. double click faceswap icon 2. the python console displays the error gpu should be detected - os: windows 10 1909 18363.1256 - python version: 3.7.9 - conda: version 4.9.2 - commit id: unsure how to find this, installed with the current windows installer - nvidia geforce gtx 1050 ti - driver version 27.21.14.6089 - nvidia cuda development & runtime 10.2 ** change ` fn = v2"")` to ` fn = _nvmlgetfunctionpointer(""nvmlinit"")` not crashing, no report.",other,question
1383,https://github.com/streamlit/streamlit/issues/1383,Deploying Streamlit on port 80,hi! i'm trying trying hard to redirect traffic from port 80 of my server to port 8501 where streamlit runs by default. i tried both the methods described linked in the official faq () aka the one with apache2 and the one with nginx but after successfully using the reverse proxy () the page gets stuck and never loads (as shown in the picture below) i successfully managed to use the following command on ubuntu to reroute traffic from port 80 to 8051 `sudo iptables -t nat -a prerouting -i eth0 -p tcp --dport 80 -j redirect --to-port 8501` however i'm really looking forward to get it working with either apache2 or nginx in order to use https. i'm currently using streamlit 0.58,deployment,question
495,https://github.com/microsoft/recommenders/issues/495,User affinity formula description is incorrect in sar_single_node_deep_dive,is affected by this bug? whichfor example: all do we replicate the issue? na expected behavior (i.e. solution) other comments main issue is the log factor should be $-log(2) x$ ** $-log_2(x)$,other,Error
1189,https://github.com/microsoft/recommenders/issues/1189,[BUG] DeepRec test failing,description there are some tests in deeprec failing: in which platform does it happen? how do we replicate the issue? expected behavior (i.e. solution) other comments,Error,question
1027,https://github.com/deepfakes/faceswap/issues/1027,error: (-215:Assertion failed) !ssize.empty() in function 'resize' on extraction,**,deployment,Error
366,https://github.com/mozilla/TTS/issues/366,Crash on certain text in pytorch with scratchpad_ being null,"this must be the funniest bug report i ever made. reproduction: linux 64bit virtualenv `pip install -u pip setuptools wheel` `python -m tts.server.server` enter: ""standup: landing top_paths not 1 pr create metadata file with model export evaluating impact of lms writing some guidance for contributors in issue 2773 * quartznet explorations"" actual result: in the browser: ""error: networkerror when attempting to fetch resource."" on the console: and the process stopped. i.e. crashed. this is reproducible.",question,question
222,https://github.com/microsoft/recommenders/issues/222,O16N with ALS and databricks,connect with john and the happy paths,other,other
12,https://github.com/deezer/spleeter/issues/12,Feature request: Add demos to the README,"some (like myself) are very curious about the methods and results, but don’t want or need to run the project ourselves. ideally a couple examples with starting file and resulting stems. maybe even at least one “ideal” example, and a least one example where the project still struggles, creating non-ideal output.",other,other
1043,https://github.com/microsoft/recommenders/issues/1043,instantâneoCreat - Azure,"description hello friends, i would like to create a routine to create and maintain the last 15 blobs snapshots other comments",other,other
769,https://github.com/microsoft/recommenders/issues/769,[BUG] Windows reco_base - tests\unit\test_timer.py:40: AssertionError,"description windows test on recotimer.py:40: assertionerror. here are the error lines: assert t.interval == pytest.approx(1, abs=tol) e assert 0.9884977 == 1 � 1.0e-02 e + where 0.9884977 = .interval e + and 1 � 1.0e-02 = (1, abs=0.01) e + where = pytest.approx in which platform does it happen? windows dsvm test machine, running tests using the recobase python environment, and run tests\unit\test_timer expected behavior (i.e. solution) should work without error. i've seen this before, but it only happens sporadically. it happened again, so i logged it. maybe we need to adjust the acceptable threshold? other comments",Error,question
697,https://github.com/mozilla/TTS/issues/697,Reduce processing time,"if anyone, have any ideas on to reduce processing time. 20 secondes for me, between running command and saving wav output. with a one line sentence. i used default tts command like this: `tts --text ""blabla wha. . ..""`",question,question
510,https://github.com/streamlit/streamlit/issues/510,Support running streamlit in a Jupyter cell,"the streamlit ui library is a very useful tool for data researchers like data analysis, data scientists to explore the data. but, lots of data researchers do their work by jupyter notebook. it is no doubt that it would be more popular if integrate streamlit ui into jupyter notebook among users.",other,other
3114,https://github.com/streamlit/streamlit/issues/3114,http://persona-landmarkchat.com:8888/,"summary type here a clear and concise description of the bug. aim for 2-3 sentences.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,other
2520,https://github.com/streamlit/streamlit/issues/2520,There's an error like  Streamlit encountered an object of type `builtins.function`,"while caching the body of `load_data()`, streamlit encountered an object of type `builtins.function`, which it does not know how to hash",other,Error
741,https://github.com/iperov/DeepFaceLab/issues/741,SAEHD crashes at 40% Initializing Models ,"expected behavior trying to run saehd, have done so multiple times in the past, but it no longer works properly. actual behavior i put in the usual settings, saehd crashes after initializing models hits 40%. have tried putting in lower settings, it still crashes at the same time (40%). steps to reproduce occurs whenever trying to do saehd. have tried with multiple different graphic drivers, and on separate hard drives. tried older version of dfl, and issue is replicated across all recent versions. other relevant information - windows 10 version 10.0.18363 - python version 3.6.8",question,deployment
7,https://github.com/iperov/DeepFaceLab/issues/7,Upside down side faces,"sometimes, for some reason, dlib extractor puts the face upside down. it does this for some side faces. i guess it depends on the exact angle. i get things likes this: question: should i keep them for conversion? i understand deleting them from the training set, but at convert time, will it be converted correctly?",question,question
870,https://github.com/streamlit/streamlit/issues/870,How to clear the cache with each run,"first off, streamlit is awesome. i am making an app where every time you push a button the state changes, but i would like to clear the cache the first time the script is run. so every time the page is refreshed, the cache is cleared. thanks for your help!",other,question
663,https://github.com/streamlit/streamlit/issues/663,quotes and space in the  DATA_URL stops it from working,"** `data_url = ( "" )`",other,other
830,https://github.com/streamlit/streamlit/issues/830,Pytest depends on actual config.toml!,"for example, try setting s3 sharing options in `~/.streamlit/config.toml` and running `make pytest`. here's what you'll see:",Error,other
548,https://github.com/iperov/DeepFaceLab/issues/548,Train Quick96 does not work.,"hello. running on rx 470. when i run ""train quick96"", it refuses to work and display this error:",other,question
2731,https://github.com/streamlit/streamlit/issues/2731,Datepicker does not select date if it's 1970/01/01,expected on left,Error,other
485,https://github.com/streamlit/streamlit/issues/485,Passing a date in index to st.line_chart defaults to UTC time zone,"summary this may not be a streamlit bug - it may be a python issue where i need to declare a datetime. when i pass a date through as the index to st.linechart(df.setdatetime() on a dataset. actual behavior: shifts it back in time by 7 hours.is this a regression? nodebug info streamlit, version 0.47.4 python 3.7.2",Error,Error
367,https://github.com/mozilla/TTS/issues/367,Unable to use pretrained model,"hello, i'm trying to follow instructions on this page: specifically: at some point pip asks for my github login and password. after i provide these credentials i'm getting the following error from pip: can you please advise?",other,question
5297,https://github.com/iperov/DeepFaceLab/issues/5297,SAHED Deepface lab custom configuration picking wrong facesets,"i am using deepface lab with sahed training algorithm , after entering the custom configuration the process started but the training is not loading the faces from the workspace it is loading from celeba. i don't know if this is how it supposed to work??",question,question
437,https://github.com/deezer/spleeter/issues/437,[Bug] Spleeter adding small padding to output audio files,"description during an effort to reduce memory footprint by splitting input files in chunks of 30 seconds, discussed on we noticed that spleeter is adding a tiny padding after each output stem file, what makes a small gap when stitching back the 30's chunks in one single stem. sometimes this gap can be unnoticeable, but when processing a song and mixing it back, it is easy to spot the hiccup in the song. also, after analyzing the waveform, it's clear that a gap is added by spleeter: in order to make sure it is related to spleeter, i've tried separating and stitching other files not processed via spleeter and the stitching was flawless. during the entire experiment, i've used only lossless(wav) files to avoid issues with padding that some lossy files would cause. is the file that generated the waveform above, you can notice a hiccup (gap) every 30 seconds when listening carefully. step to reproduce 1 - use an example wav file that has more than 30 seconds and split it into 30s chunks using ffmpeg or sox. you can rename your file to myfile.wav to reuse the code below: ffmpeg: `ffmpeg -i myfile.wav -f segment -segment_time 30 -c copy myfile-%03d.wav` sox: `sox myfile.wav myfile-.wav trim 0 30 : newfile : restart` 2 - process all the chunks using spleeter: `spleeter separate -i myfile-.wav; do echo ""file '$pwd/$f'""; done) -c copy output.wav` sox: `sox accompaniment.wav accompaniment2.wav output.wav` 5 - listen to output.wav and notice the hiccup during the transition at ~30s. you can also use this by @amo13 environment ----------------- ------------------------------- os linux using docker installation type conda ram available 6gb hardware spec docker using 8 cpus additional context",Performance,Performance
618,https://github.com/iperov/DeepFaceLab/issues/618,Suggestion: Bring back RankSRGAN upscaling method.,while ranksrgan was fairly slow compared to faceenhancer it did provide alternative look that worked much better on low resolution models (especially when used with sharpening). faceenhancer produces much more grainy/noisy output whereas ranksrgan was better at only refining edges thus giving appearance of higher resolution while keeping skin/texture fairly soft. therefore i want to suggest bringing back ranksrgan as a toggable option as an additional upscaling method that could be used together with faceenhancer.,other,other
833,https://github.com/deepfakes/faceswap/issues/833,CUDA driver version is insufficient for CUDA runtime version,"loading... 08/12/2019 03:14:02 info log level set to: debug 08/12/2019 03:14:04 info output directory: c:\users\administrator\desktop\out 08/12/2019 03:14:04 info input video: c:\users\administrator\desktop\1.mp4 08/12/2019 03:14:04 verbose using 'json' serializer for alignments 08/12/2019 03:14:04 verbose alignments filepath: 'c:\users\administrator\desktop\1featureruntime/gpu/gpuruntime/gpu/gpubase.py"", line 112, in run self.align(args, **kwargs) file ""c:\users\administrator\faceswap\plugins\extract\align\fan.py"", line 47, in initialize raise err file ""c:\users\administrator\faceswap\plugins\extract\align\fan.py"", line 41, in initialize self.model = fan(self.modelratio) file ""c:\users\administrator\faceswap\plugins\extract\align\fan.py"", line 199, in _session(ratio) file ""c:\users\administrator\faceswap\plugins\extract\align\fan.py"", line 221, in setsession = tfnewsessionref(self.cimpl.internalerror: cudagetdevice() failed. status: cuda driver version is insufficient for cuda runtime version 08/12/2019 03:14:34 error got exception on main handler: traceback (most recent call last): file ""c:\users\administrator\faceswap\lib\cli.py"", line 125, in executeextraction() file ""c:\users\administrator\faceswap\scripts\extract.py"", line 183, in runaligner() file ""c:\users\administrator\faceswap\plugins\extract\pipeline.py"", line 206, in launchreport.2019.08.12.031434375303.log'. please verify you are running the latest version of faceswap before reporting process exited.",question,question
5353,https://github.com/iperov/DeepFaceLab/issues/5353,The Specified module could not be found,"it is showing that the ""import error : dll load failed the specified module could not be found"" how to resolve it ?",question,question
330,https://github.com/microsoft/recommenders/issues/330,Add link to benchmark notebook and full results in README,"is affected by this bug? main readme reduced the size of benchmarks table, we need to add a link and some wording to the new benchmark notebook once it is merged to show the full results. other comments this is blocked by #325 and #326",other,other
543,https://github.com/mozilla/TTS/issues/543,Cannot compile due to cython compiling error,"hi, i tried to install, followed the readme. however it fails: i stepped back and found the last working commit, which is: f9001a4bdd8a169da8b4eefd481461f370def8f9 later commits (10258724d135656da526d5f6b5aaaeee787e4f04) include new cython files indeed tested on docker and it fails too suggestions?",deployment,question
282,https://github.com/deepfakes/faceswap/issues/282,"Got 6GPU's, faceswap only works with one","expected behavior actual behavior `o callmultimodelwithgpus=2, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. however this machine only has: ['/cpu:0', '/gpu:0']. try reducinggpus.` *just run with -g 6* gpu `from tensorflow.python.client import devicelib.listdevices())` i can see there are 6 gpu's detected: `80 ti, pci bus id: 0000:06:00.0, compute capability: 6.1) [name: ""/device:cpu:0"" devicelimit: 268435456 locality { } incarnation: 17778470088000163189 , name: ""/device:gpu:0"" devicelimit: 10968950375 locality { busdevicetype: ""gpu"" memoryid: 1 } incarnation: 13404147881441786455 physicaldesc: ""device: 1, name: geforce gtx 1080 ti, pci bus id: 0000:02:00.0, compute capability: 6.1"" , name: ""/device:gpu:2"" devicelimit: 10968950375 locality { busdevicetype: ""gpu"" memoryid: 1 } incarnation: 12057677249504026410 physicaldesc: ""device: 3, name: geforce gtx 1080 ti, pci bus id: 0000:04:00.0, compute capability: 6.1"" , name: ""/device:gpu:4"" devicelimit: 10968950375 locality { busdevicetype: ""gpu"" memoryid: 1 }`",other,deployment
71,https://github.com/iperov/DeepFaceLab/issues/71,Upload Prebuilt windows app to Google Drive plase.,it's more easy to bypass the limit than mega. thx!,other,other
512,https://github.com/iperov/DeepFaceLab/issues/512,OOM but i got enough memory,"running trainer. loading model... model first run. enable autobackup? (y/n ?:help skip:n) : y write preview history? (y/n ?:help skip:n) : n target iteration (skip:unlimited/default) : n 0 batchbyflip: true == == lighterloss: false == == batch0tensoruponcudasse\cudasse\onecudasse\h64\model.py"", line 89, in ontrainoneiter file ""c:\users\*****************\downloads\deepfacelab\deepfacelab9.2internal\python-3.6.8\lib\site-packages\tensorflow\python\framework\errorsimpl.resourceexhaustederror: oom when allocating tensor with shape[2048,1024,3,3] and type float on /job:localhost/replica:0/task:0/device:gpu:0 by allocator gpubfc hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. done. press any key to continue . . .",other,question
3887,https://github.com/streamlit/streamlit/issues/3887,Nothing display on app when using st.graphviz_chart(),"streamlit version 1.0.0 graphviz version 0.17 steps to reproduce the bug: import streamlit as st from graphviz import digraph graph = digraph() graph.edge('run', 'intr') graph.edge('intr', 'runbl') graph.edge('runbl', 'run') graph.view() st.graphvizchart(''' digraph { run -> intr intr -> runbl runbl -> run } ''')",question,other
616,https://github.com/microsoft/recommenders/issues/616,[BUG] Error running test notebook surprise_svd_deep_dive on Windows DSVM,"description ran this test: pytest tests/unit/testpython.py -k testdeepruns see this error output: e papermill.exceptions.papermillexecutionerror: e --------------------------------------------------------------------------- e exception encountered at ""in [10]"": e --------------------------------------------------------------------------- e typeerror traceback (most recent call last) e in e ----> 1 evalmae = mae(test, predictions) e 3 evalexpvar(test, predictions) e 5 e e ~\notebooks\recommenders\recoevaluation.py in checkdtypestrue, ratinguser, colrating, coluser e 60 ) e 61 ) e e typeerror: data types of column userid are different in true and prediction in which platform does it happen? tested on a windows cpu dsvm how do we replicate the issue? i created a windows dsvm (standard ds3 v2 - 4 vcpus, 14 gb memory). don't know if this issue is specific to windows (probably not). windows setup steps were (for now): 1. login to dsvm 2. cd notebooks 3. git clone 4. git checkout staging 5. conda update conda 6. conda update --all 7. cd recommenders 8. python scripts/generatefile.py 9. conda env create -f recobase"" is there) 11. conda activate reconotebookssurprisedive_runs expected behavior (i.e. solution) the test should run successfully other comments",Error,question
484,https://github.com/streamlit/streamlit/issues/484,Streamlit 404 error in Google Cloud Run,"summary streamlit app deployed in google cloud run fails with a 404 error ""requested url healthz not found"" steps to reproduce see file healthz404.txt in attached example zip.expected behavior: should show a small dataframe (example from getting started page in docs)actual behavior: the browser reports connecting and ""please wait"" from streamlit so it is seeing the cloud run instance, see screenshot in attached. after a minute or so it throws the 404 error, full text in the healthz404.txt fileis this a regression? that is, did this use to work the way you expected in the past? nodebug info - streamlit version: 48.1 - python version: 3.7-slim, docker image - using pip - os version: n/a - browser version: chrome 77.0.xadditional information if needed, add any other context about the problem here.",deployment,question
3177,https://github.com/streamlit/streamlit/issues/3177,"Runs perfect on localhost, but loads forever when deployed","summary my streamlit app runs perfectly on localhost, but when i deploy it using heroku, it runs forever and the loading never ends. what should i do?steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,question
3709,https://github.com/streamlit/streamlit/issues/3709,df.dtypes fails with Arrow,summary using `df.dtypes` to show the datatypes for a dataframe fails when you're using the arrow codepath.steps to reproduce code snippet: ** the app shows this exception: exception text is this a regression? yes (arrow vs legacy)debug info - streamlit version: 0.86.0,Error,Error
965,https://github.com/deepfakes/faceswap/issues/965,the newest  windows release lack yaml?,"i install the newest windows release, but it can not run successfully. when i extract the faces, the debug info: loading... setting faceswap backend to nvidia 01/20/2020 18:10:24 info log level set to: debug 2020-01-20 18:10:24.943963: i tensorflow/streamloader.cc:44] successfully opened dynamic library cudart64_100.dll process exited. i try change tensorflow version but failed. when i install the pyyaml, it can work well. and i find requirement.txt has not yaml dependence. so in some case, when lack of yaml dependence, it can not work well but the debug info has no tips about yaml.",other,question
255,https://github.com/deezer/spleeter/issues/255,OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.,"hi all, i am currently running a training session on spleeter on the musdb18 dataset. my thinking being get this working first before i attempt my own training set. installed using conda i am running on a mac pro 5,1 (mid 2010) - 128gb memory - running high sierra 10.13.6 i also have a nvidia geforce gtx 980 i have the cuda drivers installed i know that tensorflow doesn't support gpu on mac, so have installed the normal version. i am getting the following message throughout the processing. omp: warning #190: forking a process while a parallel region is active is potentially unsafe. omp: warning #190: forking a process while a parallel region is active is potentially unsafe. omp: warning #190: forking a process while a parallel region is active is potentially unsafe. is this message coming from tensorflow or spleeter? also what does it actually mean? it is still running at the moment, so hopefully will get an idea of how long it will take to train without the use of a gpu. of course, if someone knows how to get the gpu working in the above it would be great to know how to do this as well. any info would be great. thanks, alec",question,question
321,https://github.com/iperov/DeepFaceLab/issues/321,The models turned red when they were working.,"when src.dst's work was in 0.0151, it suddenly changed to 3.1111 and the face of the work turned red why did this happen?",question,question
5207,https://github.com/iperov/DeepFaceLab/issues/5207,Windows (magnet link) last release torrent link broken?,i don't think there is a problem on my end but i'm trying to d/l the latest torrent version and the link seems to be broken?,question,question
3476,https://github.com/streamlit/streamlit/issues/3476,disable `external url`,sometimes i want to disable `external url` and open only `network url` did u can this or add to doc this feature,other,other
1966,https://github.com/streamlit/streamlit/issues/1966,"Update pydeck dependency to `pydeck = ""*""`",no real reason to require a dev version. see slack discussion:,other,other
1255,https://github.com/streamlit/streamlit/issues/1255,Bokeh not working on python 3.5,- bokehjs 2.0 needs pybokeh 2.0 to work - we have bokehjs 2.0 pinned in our package.json - but pybokeh 2.0 requires python>=3.6 - bokeh is failing on python 3.5 charts do not appear,other,deployment
674,https://github.com/deezer/spleeter/issues/674,[Discussion] raise ValueError,"file ""/home/josema/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1291, in restore valueerror: can't load save_path when it is none. sorry for my english. the system works fine with 2 streams but not for 4 streams. thankyou for yor attention",question,Error
731,https://github.com/microsoft/recommenders/issues/731,[BUG] Sporadic AssertionError running non-notebook/spark/gpu unit tests on Windows DSVM,"description i'm hitting a sporadic assertion error when running: conda activate recobase python environment, and run pytest tests/unit -m ""not notebooks and not spark and not gpu"". expected behavior (i.e. solution) should not produce an error. perhaps there's a way to make things more deterministic. other comments",Performance,question
747,https://github.com/microsoft/recommenders/issues/747,[BUG] No matching distribution found for nni==0.5.2.1 on Windows DSVM,"description seeing an error when installing nni on windows. this is the error that i see when setting up the environment: debug menuinstver} ${platform}', prefix: 'c:\anaconda\envs\nightlybase', envrecomode: 'system' debug menuinstrecorecorecowork\1\s\condaenv.ieyjoeec.requirements.txt (line 11)) (from versions: 0.3.2, 0.3.3, 0.3.4) no matching distribution found for nni==0.5.2.1 (from -r c:\users\recocat\agent\base python environment will cause the error. expected behavior (i.e. solution) no error. not sure about solution - we'll probably have to conditionally disable this somehow on windows. other comments",deployment,deployment
344,https://github.com/streamlit/streamlit/issues/344,st.sidebar.selectbox dropdown is not working,"summary when using the selectbox in the sidebar, the dropdown is not workingsteps to reproduce run in `streamlit` open the ui and click on the selectbox in the sidebarexpected behavior: i can see the dropdown menuactual behavior: selectbox is not reactiveis this a regression? yesdebug info - streamlit version: dev 961c97b93bc2b00ac18cb0826bfdd02 - python version: 3.7.4 - using conda? pipenv - os version: macos - browser version: chrome",other,Error
1861,https://github.com/streamlit/streamlit/issues/1861,Create and Replace Tooltip Component,"we know of one locations for the tooltip component. * instead, we will use baseweb's goal is to mimic the same look and feel as before and announce if there are any changes that are unattainable. please display before and after images in pull requests.",other,other
309,https://github.com/iperov/DeepFaceLab/issues/309,Not training with RTX 2080 Ti (Using Windows Pre-build App),"windows 10 pro i have tried all the extraction methods, and training types, and any combinations in-between. i seem to be getting nan issues, and the training window will be noise or just black. it seems to run fine with my gtx 1070, but as soon as i try to use the other card, it bugs out on any of the training bat. if i attempt to refresh the preview, it crashes instantly. sometimes only, `loss_history` seems not to `nan` out, and it allows me to update the preview using the key `p` without crashing, but the update will just be a black preview and will not train period.",other,deployment
1051,https://github.com/microsoft/recommenders/issues/1051,[BUG] Problem with ADO scheduled triggers,"description after 18jan the scheduled master pipeline test stopped working. before that, every day the nightly tests on master and staging were being executed, after that date there is an erratic behavior see pipelines here: in which platform does it happen? how do we replicate the issue? expected behavior (i.e. solution) other comments",deployment,Error
879,https://github.com/streamlit/streamlit/issues/879,v0.52.0: Bad message format - Tried to use SessionInfo before it was initialized,"summary on first page load, i get a message dialog: ""bad message format - tried to use sessioninfo before it was initialized"". i can then click on the ok button and i can proceed as normal.steps to reproduce on windows 10 - installed clean separate python virtual environment. then installed using then launched hello demo app: streamlit hello then i immediately get the dialog message.is this a regression? yesdebug info - streamlit version: 0.52.0 - python version: 3.7 - using conda - os version: windows 10 - browser version: chrome 79 pip freeze output:",Error,other
324,https://github.com/mozilla/TTS/issues/324,how to deploy  tacotron1 on aws lambda ?,i want to decrease the size of the packages and model,question,question
1441,https://github.com/streamlit/streamlit/issues/1441,Multiselect example in the documentation does not work if copied,"the example, if copied and pasted, doesn't run. there's a few small issues with it (missing comma at the end of a line, order of parameters wrong and the defaults should be a list rather than a tuple.",Error,Error
627,https://github.com/mozilla/TTS/issues/627,problem with the installation on python 3.8 and 3.6,gives the following error,question,deployment
1212,https://github.com/streamlit/streamlit/issues/1212,running as systmd service - EXCEPTION! 'NoneType' object has no attribute 'strip',"summary same bug like mentioned in #554 but i do not use docker, instead i want to run it as a service on ubuntu. steps to reproduce create a bash `initreport_service` script like this: create a systemd service like this: try to start the service with `systemctl start chatbot-bug-report`. note: if you run the bash file on its own streamlit starts without problems, if you want to run it as a service it won麓t. expected behavior: running streamlit as a serviceactual behavior: debug info - streamlit version: 0.56.0 - python version: 3.7.5 - conda version: 3.7.12 - os version: ubuntu 18.04.3 lts - local browser version: chrome version 78.0.3904.108",Error,Error
2296,https://github.com/streamlit/streamlit/issues/2296,Terminal is hung during API processing,"summary hung terminal after running indexclass=tldrstory.api.api uvicorn ""txtai.api:app""steps to reproduce this was run on macos. 1. install wget via homebrew `brew install wget` 2. run the commands from readme.md from behavior: as shown in behavior: everything from 1. download the ports news is this a regression? nodebug info - streamlit version: streamlit, version 0.69.2 - python version: python 3.7.4 - using conda: conda - os version: macos catalina version 10.15.6 (19g2021) - browser version: version 86.0.4240.80 (official build) (x86_64)additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,Error
659,https://github.com/iperov/DeepFaceLab/issues/659,Limiting Used Cores,"hi there i got a problem: new merger uses all cores. that means in my case 64 cores. in the last version there was the option to limit the cores, but now, this option is away. how can i limit the used cores?",question,question
471,https://github.com/iperov/DeepFaceLab/issues/471,SAE and SAEHD models don't run on rtx 2060,sae and saehd training models don't run on rtx 2060 and i have seen people run these models on gtx 1050 ti or 1060 3gb,question,question
3026,https://github.com/streamlit/streamlit/issues/3026,Deleting config.toml does not trigger a config refresh,"this was originally discovered by @jrieke during a streamlit company hackathon. the original description of the bug is copied below. note: even though this bug is described as the theming bug, the root cause of the issue is that we're not reloading config options when a `config.toml` file is deleted.steps to reproduce 1. create a streamlit app with a custom theme in `./.streamlit/config.toml` 2. `streamlit run ...` 3. delete the entire file `./.streamlit/config.toml`current behavior custom theme of the deleted file is still applied to the appexpected behavior custom theme should go away and default light theme should be applied. comments tested with streamlit 0.79.0. this doesn't seem to happen if i just delete the file contents of `.streamlit/config.toml` but the file is still there.",other,Error
390,https://github.com/iperov/DeepFaceLab/issues/390,About “Warning: multiple faces detected.” and No response for a long time.,"why is there a ""warning: multiple faces detected."" error? i am pretty sure that only the target person's avatar is in `workspace\data_dst\aligned`. and the model only uses the avatar of the target character and the avatar of the material character in the training. i‘m waiting for two hours, there is still no conversion progress.",other,question
3880,https://github.com/streamlit/streamlit/issues/3880,ReportSession is not released correctly and it causes a memory leak,"summary in order to watch a for file changes and to react to them we register a function which is updating the report session. the registered function was different from the deregistered one and therefore the reference of the object providing the function was never release. this leads to a memory leak.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",Performance,other
561,https://github.com/microsoft/recommenders/issues/561,[BUG] SVD notebook presentation improvements,"description misc presentation suggestions/issues on svd deep dive notebooks. 1. ""the svd model algorithm is very similar to the als algorithm presented in the als deep dive notebook"": add als notebook link. 2. ""also called baselines in the litterature"": typo, also need to add references. 3, ""the model is svd is then as follows"": revise. 4. "" 5. move introduction of svd and references up front. in which platform does it happen? how do we replicate the issue? expected behavior (i.e. solution) other comments",other,Error
981,https://github.com/streamlit/streamlit/issues/981,Make Streamlit methods work nicely with IDEs (docstrings and autocomplete),"one nice aspect of using ides like pycharm and vs code is how they show docstrings and signatures for functions when you hover on them, how they autocomplete arguments, and so on. however, since streamlit methods make heavy use of decorators and higher-order functions our methods confuse those ides. we should refactor our code to remove to make ides happy. in the process, the code may become a little more repetitive, but it's a small price to may!",other,other
339,https://github.com/mozilla/TTS/issues/339,why Tacotron2 mel are sometimes negative?,"hello everyone! i mentioned that the mel spectrogram generated by the tacotron model (obtained with where input_ is, as i suppose, mel-spectrogram) sometimes (always) has negative values (very small ones, like -0.037 or so). if we compute the ground-true spectrogram with we have always positive values, though. so if we compare ground-truth mels with generated by tacotron2 ones, the ground-truth's are always a bit bigger, like 0.550 comparing to 0.450 and so on. so, here's my question: why is that? may i get this around? i want to train melgan and generate the mels not by tacotron2, but with audioprocessor. it is the right way to go? i am worried about those negative values, as they are not going to be presented in the training set for melgan. thank you!",question,question
290,https://github.com/streamlit/streamlit/issues/290,st.map() scrolling throws TypeError,summary get the following error when running st.map() with default settings (i.e. scatterplot) and zooming in: typeerror: e.set is not a functionsteps to reproduce i used: - virtualenv - python 3.7 - streamlit 0.47.2,Error,Error
360,https://github.com/streamlit/streamlit/issues/360,Add 2 cells on the same line,"hi, i was working on a text classification algorithm that uses hand-classified data. what i'd like to be able to do is to have on the same line half the cell (or say 3/4) that displays the text to classify, and a smaller cell in which the user can add the label manually. i have attached an example of 2 cells side by side: it is already feasible in this version? if not, do you think it would be? thanks!",other,other
3943,https://github.com/streamlit/streamlit/issues/3943,Error displaying DataFrame,"summary type here a clear and concise description of the bug. aim for 2-3 sentences.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** the following error appears alueerror: dataframe constructor not properly called! traceback: file ""/users/aquiba/opt/anaconda3/envs/dev/lib/python3.7/site-packages/streamlit/scriptrunstreamlit/unsolved/advanceddf = pd.dataframe(stockchain_ raise valueerror(""dataframe constructor not properly called!"")is this a regression? that is, did this use to work the way you expected in the past? not suredebug info - streamlit version: (get it with `$ streamlit version`) 1.0 - python version: (get it with `$ python --version`) 3.7.1 - using conda? pipenv? pyenv? pex? conda - os version: macos big sur 11.6 - browser version: safari v 15.0additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,other
3003,https://github.com/streamlit/streamlit/issues/3003,Can't login in app by using DNS domain name address,"summary when i use dns domain name to login my streamlit app running in a docker by chrome, i see a blue box in the center of the page with the text 淧lease wait︹€steps to reproduce code snippet: 1. i runned this code by ""streamlit run test.py --server.port 18000"", this 18000 is the docker port mapped to port 54000 2. i can login in by address ""ip:54000"" and see ""test"" in the page; 3. i get a dns domain name address ""tools.xx.com"" routed to ""ip:54000"" , but when i use this address to login in the app, i see a blue box in the center of the page with the text 淧lease wait︹€ * --server.enablecors=false * --server.enablewebsocketcompression=false but it's not work. debug info - streamlit version: 0.79.0 - python version: 3.7.10 - using conda: miniconda - os version: ubuntu16.04 - browser version: chrome 87.0.4280.141",other,other
2923,https://github.com/streamlit/streamlit/issues/2923,Segmentation fault (core dumped),summary running a very simple dashboard i keep randomly getting the error message `segmentation fault (core dumped)` and streamlit crashing. i saw the other closed issue and set the debugging level higher but that hasn't reviled much (see below)steps to reproduce code snippet: full code - a very simple dashboard and then i'm just running `streamlit run main.py`. the error is only happening sometimes (to the extent that if i just restart it a load of times it sometimes works but other times gives `segmentation fault (core dumped)` or just `killed`. when it does work after a while (have no idea how long because i left it alone) it just does it again. **,other,Error
595,https://github.com/deepfakes/faceswap/issues/595,Adjust convert does not work for GAN and GAN128 model,adjust convert plugin fails in case of gan or gan128 model expected behavior adjust convert completes w/o errors actual behavior gan and gan128 fails with errors (please see pic attached) steps to reproduce 1) extract faces from images and train gan or gan128 model 2) perform python faceswap convert with adjust plugin other relevant information - ** gpu,other,Performance
5246,https://github.com/iperov/DeepFaceLab/issues/5246,data_dst faceset extract not working on GTX1650,"have been attempting to extract datamain file ""multiprocessing\spawn.py"", line 118, in bootstrap file ""multiprocessing\process.py"", line 93, in run file ""c:\users\carly\onedrive\desktop\deepfacelab\deepfacelabinternal\deepfacelab\core\joblib\subprocessorbase.py"", line 62, in run file ""c:\users\carly\onedrive\desktop\deepfacelab\deepfacelabinternal\deepfacelab\mainscripts\extractor.py"", line 73, in onnvidia\nvidia\fornvidia\nvidia\nvidia\nvidia\nvidia\nvidia\ops.py"", line 3422, in pad file ""c:\users\carly\onedrive\desktop\deepfacelab\deepfacelabinternal\python-3.6.8\lib\site-packages\tensorflow\python\ops\genops.py"", line 6484, in pad file ""c:\users\carly\onedrive\desktop\deepfacelab\deepfacelabinternal\python-3.6.8\lib\site-packages\tensorflow\python\framework\oplibrary.py"", line 750, in opnvidia\createinternal file ""c:\users\carly\onedrive\desktop\deepfacelab\deepfacelabinternal\python-3.6.8\lib\site-packages\tensorflow\python\framework\ops.py"", line 1990, in __ ------------------------- images found: 2129 faces detected: 1 ------------------------- done. press any key to continue . . .",question,question
1109,https://github.com/streamlit/streamlit/issues/1109,Display local PDF in Streamlit as text,"i know there isn a simple way to do this in the current versions, but is there a reasonable workaround to insert local pdf in streamlit? here what ie tried: ** tried pdf2image to convert the pdf to an image first and then render it using st.image but all i got is 1x1 blank image. this doesn seem to be streamlit issue cause i tried it from the terminal using the underlying tools that pdf2image uses and still got the same result. the pdf is only one page but has quite complex content which might be an issue. is there any way i can insert local pdf into my streamlit app? it is a central part of the app and the main reason why i needed a visualization app in the first place. thank you in advance!",other,other
2254,https://github.com/streamlit/streamlit/issues/2254,Disable streamlit widgets,i would like to know if there is a way to disable streamlit widgets like checkbox and others,other,other
480,https://github.com/deezer/spleeter/issues/480,[Bug] Running stops after a while,"description hi, while training spleeter, i got an error after a few thousands steps (occurs after 6000-12000 steps). the error is: (0) unknown: failed to writefile: cache/training3133]] step to reproduce - conda install -c conda-forge spleeter-gpu==1.5.3 - conda install numba==0.48 - pip install librosa==0.7.2 2. run ""_11core\python\client\session.py"", line 1365, in call file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowrun11core\python\client\session.py"", line 1443, in tfimpl.unknownerror: 2 root error(s) found. (0) unknown: failed to writefile: cache/training3133]] (1) unknown: failed to writefile: cache/training0/site-packages/spleeter/_0/site-packages/spleeter/_0/site-packages/spleeter/_1111estimator\python\estimator\training.py"", line 473, in trainevaluate file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflow11estimator\python\estimator\training.py"", line 714, in run11estimator\python\estimator\estimator.py"", line 370, in train file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowtrain11estimator\python\estimator\estimator.py"", line 1195, in model11estimator\python\estimator\estimator.py"", line 1494, in withspec file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 754, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 1259, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 1360, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\six.py"", line 703, in reraise file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 1345, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 1418, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowsession.py"", line 1176, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflow11core\python\client\session.py"", line 1180, in 11core\python\client\session.py"", line 1359, in run file ""c:\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowdoimpl.unknownerror: 2 root error(s) found. (0) unknown: failed to writefile: cache/training11core\python\framework\ops.py:1748) ]] [[iteratorgetnext/0.data-00000-of-00001.tempstate12880671843826882180 : there is not enough space on the disk. ; unknown error [[node iteratorgetnext (defined at \users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflow0/site-packages/spleeter/_0/site-packages/spleeter/_0/site-packages/spleeter/_1111estimator\python\estimator\training.py"", line 473, in trainevaluate file ""\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflow11estimator\python\estimator\training.py"", line 714, in run11estimator\python\estimator\estimator.py"", line 370, in train file ""\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowtrain11estimator\python\estimator\estimator.py"", line 1188, in model11estimator\python\estimator\estimator.py"", line 1025, in featureslabelsinput11estimator\python\estimator\util.py"", line 65, in parsefn11core\python\data\ops\iteratornext file ""\users\myusername\anaconda3\envs\run8\lib\site-packages\tensorflowdatasetget11core\python\framework\oplibrary.py"", line 794, in op11core\python\util\deprecation.py"", line 507, in new11core\python\framework\ops.py"", line 3357, in create11core\python\framework\ops.py"", line 3426, in op11core\python\framework\ops.py"", line 1748, in __ ------------------- -------------------------------------- os windows installation type conda / pip (librosa only) ram available 32gb hardware spec gpu nvidia geforce rtx 2080 ti",question,question
727,https://github.com/iperov/DeepFaceLab/issues/727,x Seg and AMD,"hi, is there any way i could use xseg with the opencl version (windows) ? i'm currently using an amd rx580.",question,question
548,https://github.com/deepfakes/faceswap/issues/548,Should originalhighres not have a preview?,"just a quick question, im training fine, graph output etc. but no preview. is this normal?",question,question
824,https://github.com/microsoft/recommenders/issues/824,Using recommenders in my own codebase,"description looking for best practices on how to use recommenders and especially recoutils that way? i see #774 but until it is available via pip or conda, i'm not sure how best to do this. other comments",question,other
1352,https://github.com/microsoft/recommenders/issues/1352,[FEATURE] Consider adding BiVAE model,"description hi, i would like to contribute a notebook explaining model presented in wsdm'21. the implementation of bivae is under library which covers the bpr tutorial added earlier. let me know if this contribution is favorable. thanks! expected behavior with the suggested feature other comments",other,other
237,https://github.com/deezer/spleeter/issues/237,Using pretrained models (SavedModel) ,"i am trying to use a savedmodel, (saved in a .pb file found on `/temp/serving`). i have very little experience in tensorflow 1.x (i learned with 2.0). so i can't make it work. someone could provide me a script to load and run the model, or guide me on how i do it? thanks!",other,question
563,https://github.com/deezer/spleeter/issues/563,[Discussion] Python Version,"when installing it says. > specifications: > > - spleeter -> python[version='>=3.6,=3.7, > your python: python=3.8 > what does this mean? > >=3.6,=3.7,<3.8.0a0 i mean, to which version i have to switch?",question,question
3353,https://github.com/streamlit/streamlit/issues/3353,Streamlit not showing anything,"summary i was making a ml project in vs code with sreamlit. whenever i am trying to run it from the terminal it is appearing a blank white screen on my localhost.steps to reproduce code snippet: import streamlit as st import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.modeltestscore st.title('streamlit example') st.write(""""""explore different classifier and datasets which one is the best? """""") datasetname} dataset"") classifierdataset(name): data = none if name == 'iris': data = datasets.loadwine() else: data = datasets.loadcancer() x = data.data y = data.target return x, y x, y = getname) st.write('shape of dataset:', x.shape) st.write('number of classes:', len(np.unique(y))) def addui(clfname == 'svm': c = st.sidebar.slider('c', 0.01, 10.0) params['c'] = c elif clfdepth = st.sidebar.slider('maxdepth'] = maxestimators = st.sidebar.slider('nestimators'] = nparametername) def getname, params): clf = none if clfname == 'knn': clf = kneighborsclassifier(nestimators=params['ndepth=params['maxstate=1234) return clf clf = getname, params)classification #### xtest, ytest = trainsplit(x, y, teststate=1234) clf.fit(xtrain) ytest) acc = accuracytest, yname}') st.write(f'accuracy =', acc)plot dataset ####project the data onto the 2 primary principal components pca = pca(2) xtransform(x) x1 = xprojected[:, 1] fig = plt.figure() plt.scatter(x1, x2, c=y, alpha=0.8, cmap='viridis') plt.xlabel('principal component 1') plt.ylabel('principal component 2') plt.colorbar() #plt.show() st.pyplot(fig) (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,other
449,https://github.com/iperov/DeepFaceLab/issues/449, extract by fanseg  Error,"i am using the extract by fanseg in the 2019-10--15 version, the following error occurred：",other,question
309,https://github.com/deezer/spleeter/issues/309,[Bug] name your bug,"i tried to install spleeter-gpu package from conda-forge and while librosa package was missing, i get the following error: reproduce conda create -n spleeter-gpu -c conda-forge spleeter-gpu conda install librosa wget spleeter separate -i audio_example.mp3 -p spleeter:2stems -o output",Error,question
1076,https://github.com/streamlit/streamlit/issues/1076,Get index from selectbox,"problem i wonder how to get index from select box. i checked api reference and couldn't find any solution. it would be better if selectbox returns `index` instead of `option`, because i think in most case we can get `option` by index.",other,other
3961,https://github.com/streamlit/streamlit/issues/3961,Visual overlapping of slider values,"summary slider values can visually overlap which makes them hard to read. especially when using longer names for the slider values.steps to reproduce code snippet: the values should be aligned differently, thus readable. ** date and time when not close to eachother: close to eachother: is this a regression? nodebug info - browser version: chrome",Error,Error
802,https://github.com/deepfakes/faceswap/issues/802,"Can you cut the avatar into a transparent one, so you can better integrate into another face","i am a newbie.the result of my training has a lot of obvious black borders. the effect is not very natural. i wonder if this can be done by setting the training picture to a transparent background, which can eliminate the influence of surrounding colors. for example: change it to",other,question
3878,https://github.com/streamlit/streamlit/issues/3878,Toggle Wide Mode,"problem cannot toggle layout pagesolution i can access the mode via the settings button - i.e. the checkbox for wide mode. it would be nicer if each page in the app can decide by itself if it wants to be in ""wide"" mode or ""centred"" - without the user having to manually adjust the page layout. ** if we could nest columns that would be awesome, and i wouldn't need to fiddle with wide mode. i would create a parent column that can fit my whole page",other,other
1664,https://github.com/streamlit/streamlit/issues/1664,Support for unix sockets,"problem server.address configuration option should also support listening to unix socket files since this is very common in linux servers environments and useful for proxying the application without taking a system port. very useful feature for shared servers. would be cool running something like: `streamlit run --server.address ""unix:~/.streamlit.sock"" whatever.py` and having it creating the sock file and listening on it.solution never played with tornado but i believe it has built in unix sockets support so this might be easy to implement. with a fast google i found: so bindsocket seems to be the function for it!",other,other
478,https://github.com/streamlit/streamlit/issues/478,Show warning when two widgets have the same id,see: need to come up with exact text for the warning,Error,other
113,https://github.com/iperov/DeepFaceLab/issues/113,It's not a technical issue，just want to help DeepFaceLab get better.,"hi, i am a deepfacelab fans from china, many chinese programmer want to learn faceswap technology. when i research a lot of products about ai faceswap software, i think deepfacelab is the best. for the following reasons: 1. update most frequently 2. easiest to use 3. high quality video output so i want to build a china deepfacelab official website about technology. it will help many chinese programmer. it is amazing! now i've built a simple website. it will promotion deepfacelab in china. site url: please contact with me. thanks. email: lubootisbadguy@gmail.com work: senior programmer (java、php、go)",other,other
2870,https://github.com/streamlit/streamlit/issues/2870,"Memory leak, streamlit keeps hoging memory and crashes","summary when running the code below and updating images, after a while streamlit reaches 100% memory usage and gets killed.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. run streamlit 2. start uploading images for inference ** memory usage keeps increasing until it is killed by the kernel.is this a regression? nodebug info - streamlit version: 0.77.0 - python version: 3.7.6 - using virtualenv - os version: macos 10.5 - browser version: firefox 85.0.2additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",Performance,Error
436,https://github.com/mozilla/TTS/issues/436,Replace librosa audio processing with native pytorch functionality,mozilla tts still uses ** if available: so i suggest to replace librosa code with pytorch native implementations where necessary.,other,other
822,https://github.com/deepfakes/faceswap/issues/822,Windows GUI error,"after training stop, this error occurs: it seems all working good, but something not so good.",deployment,Error
3897,https://github.com/streamlit/streamlit/issues/3897,PyDeck map state should not reset on rerun,"summary when you pan/zoom a pydeck charts from st.map (and maybe st.pydeckwithin that region_ as the data changes. this bug breaks this interaction.steps to reproduce code snippet: 1. run the code above 2. pan/zoom/tilt the map to a completely different place (africa, or something) 3. use hamburger menu to rerun the script ** if you manually panned/zoomed/tilted the map, the pan/zoom/tilt state should be preserved between reruns even if the data changes. if you did not manually pan/zoom/tilt, then the map should just do what it does by default: that is, it should act as if it was removed and readded to the app.is this a regression? maaaaaybe. we've had similar problems with pydeck before, but it's unclear whether this exact combination is a regression.debug info - streamlit version: 1.0.0 - python version: 3.9",Error,Error
507,https://github.com/microsoft/recommenders/issues/507,missing SAR metrics in README in staging,we no longer have sar metrics in readme.,other,other
369,https://github.com/mozilla/TTS/issues/369,problem in synthesizing audio,"hi, i have tried training with tacotron2 for the persian language with the attached configuration any advice is appreciated. fahim",other,other
617,https://github.com/iperov/DeepFaceLab/issues/617,Errors while using RCT color transfer,"i'm training a standard model with following parameters: =================== model summary ==================== == == == model name: 128 df 256 64 64 16type: f == == modelsondims: 256 == == edims: 64 == == ddims: 16 == == learndropout: false == == randompower: 5.0 == == truepower: 0.0 == == facepower: 1.0 == == bgpower: 1.0 == == ctpreviewiter: 0 == == randomsize: 10 == == autobackupinternal\deepfacelab\core\imagelib\colorscalars a = (astdsrc / astdtar) a f:\df\dfl2.0\transfer.py:244: runtimewarning: divide by zero encountered in floatinternal\deepfacelab\core\imagelib\colorinternal\deepfacelab\core\imagelib\colorrange = (max([mn, 0]), min([mx, 255])) f:\df\dfl2.0\transfer.py:320: runtimewarning: invalid value encountered in greater scaleinternal\deepfacelab\core\imagelib\colorscalars a = (astdsrc / astdtar) a f:\df\dfl2.0\transfer.py:244: runtimewarning: divide by zero encountered in floatinternal\deepfacelab\core\imagelib\colorinternal\deepfacelab\core\imagelib\colorrange = (max([mn, 0]), min([mx, 255])) f:\df\dfl2.0\transfer.py:320: runtimewarning: invalid value encountered in greater scaleinternal\deepfacelab\core\imagelib\colorscalars a = (astdsrc / astdtar) a f:\df\dfl2.0\transfer.py:244: runtimewarning: divide by zero encountered in floatinternal\deepfacelab\core\imagelib\colorinternal\deepfacelab\core\imagelib\colorrange = (max([mn, 0]), min([mx, 255])) f:\df\dfl2.0\transfer.py:320: runtimewarning: invalid value encountered in greater scaleinternal\deepfacelab\core\imagelib\colorscalars a = (astdsrc / astdtar) a f:\df\dfl2.0\transfer.py:244: runtimewarning: divide by zero encountered in floatinternal\deepfacelab\core\imagelib\colorinternal\deepfacelab\core\imagelib\colorrange = (max([mn, 0]), min([mx, 255])) f:\df\dfl2.0\transfer.py:320: runtimewarning: invalid value encountered in greater scaleinternal\deepfacelab\core\imagelib\colorscalars a = (astdsrc / astdtar) a f:\df\dfl2.0\transfer.py:244: runtimewarning: divide by zero encountered in floatinternal\deepfacelab\core\imagelib\colorinternal\deepfacelab\core\imagelib\colorrange = (max([mn, 0]), min([mx, 255])) f:\df\dfl2.0\transfer.py:320: runtimewarning: invalid value encountered in greater scale_range = (max([mn, 0]), min([mx, 255])) i've been experiencing following errors even with other/older models in dfl 1.0 but never really though about reporting them to get to know what causes them, why the models seems to work/train fine despite them and if there is something that can be done to fix them. dfl 2.0 03.02.2020 version win 10 pro gtx 1060 6gb 16gb ram",other,question
373,https://github.com/deezer/spleeter/issues/373,[Bug] Spleeter NotImplementedError,"description i am trying to train a new model that is more tailored to vocal removal in tv/film. i have a large data set, but for the purpose of 'proof of concept', i am using only 1 tv episode which has been cut into the appropriate slices [vocals + other]. i used musdb as a template to structure my directories and filenames. step to reproduce 1. open powershell in my config directory and type `spleeter train -p f:\spleettest\configs\musdb1:0) to a numpy array.` error output environment ----------------- ------------------------------- os windows 10 (fully updated) installation type pip ram available 16gb hardware spec rtx2080 / ryzen r2600 additional context",Error,question
1029,https://github.com/deepfakes/faceswap/issues/1029,Detecting fake/real image - sourcechangewarning #48,"when i try to detect the video using detectvideo.py - !python classification/detectvideo.py -i video/trumpvideo.mp4 -m classification/faceforensics++c23.p -o outputpatches = true and use the patch tool to revert the changes. warnings.warn(msg, sourcechangewarning) /usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: sourcechangewarning: source code of class 'pretrainedmodels.models.xception.xception' has changed. you can retrieve the original source code by accessing the object's source attribute or set torch.nn.module.dumppatches = true and use the patch tool to revert the changes. warnings.warn(msg, sourcechangewarning) /usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: sourcechangewarning: source code of class 'torch.nn.modules.batchnorm.batchnorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set torch.nn.module.dumppatches = true and use the patch tool to revert the changes. warnings.warn(msg, sourcechangewarning) /usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: sourcechangewarning: source code of class 'pretrainedmodels.models.xception.block' has changed. you can retrieve the original source code by accessing the object's source attribute or set torch.nn.module.dumppatches = true and use the patch tool to revert the changes. warnings.warn(msg, sourcechangewarning) /usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: sourcechangewarning: source code of class 'pretrainedmodels.models.xception.separableconv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set torch.nn.module.dumppatches = true and use the patch tool to revert the changes. warnings.warn(msg, sourcechangewarning) /usr/local/lib/python3.6/dist-packages/torch/serialization.py:453: sourcechangewarning: source code of class 'torch.nn.modules.linear.linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set torch.nn.module.dumpmodels/full/xception/fullfromfullnetwork(*input, *input, *input, **kwargs) file ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py"", line 343, in forward return self.conv2dforward self.padding, self.dilation, self.groups) runtimeerror: input type (torch.floattensor) and weight type (torch.cuda.floattensor) should be the same 0% 1/595 [00:00<04:49, 2.05it/s]",other,question
1180,https://github.com/streamlit/streamlit/issues/1180,Need hash_func for type _io.StringIO,this should work out of the box:,Error,other
532,https://github.com/mozilla/TTS/issues/532,Overtraining MelGAN causes high freq noise in results,i realized that training melgan vocoder too long (>1m steps for universal vocoder and >850k steps for german) reduces the quality and introduces a high freq noise to the results. i just call this out for anyone who is interested to dwell into it more. my guess is that it might be about the combination loss weights or later enabled discriminator emphasizes the wrong quality of the voice. any thoughts?,Performance,Performance
816,https://github.com/microsoft/recommenders/issues/816,"[ASK] In NCF Deep dive and ncf_movielens notebook, I used my own dataset instead of movie lens, its has userID itemID and ratings (i used counts here as rating like implicit data). The notebook throws the following error? could someone help me out with this problem?","description other comments data set looks like this rating userid itemid 0 12 3468 3644 1 3 3816 3959 2 1 2758 2650 3 1 5056 1593 4 30 3029 192 when i run this cell in the notebook i got the following error data = ncfdataset(train=train, test=test, seed=seed) error: --------------------------------------------------------------------------- typeerror traceback (most recent call last) in 1 seed = 10 ----> 2 data = ncfdataset(train=train, test=test, seed=seed) ~/recommenders/reconeg, ntest, colitem, colinitdata() ---> 61 self.testutils/recommender/ncf/dataset.py in testinteractinteractstatus, on=self.colinteractitem + ""interactitem + ""item + ""test""], axis=1) 186 testinteractuser, self.colnegative""]], on=self.col64.egg/pandas/core/frame.py in apply(self, func, axis, broadcast, raw, reduce, resultresult() 6488 6489 def applymap(self, func): ~/.local/lib/python3.6/site-packages/pandas-0.24.2-py3.6-macosx-10.7-x86result(self) 149 return self.applystandard() 152 153 def applyresult(self): ~/.local/lib/python3.6/site-packages/pandas-0.24.2-py3.6-macosx-10.7-x86standard(self) 255 256 # compute the result using the series generator --> 257 self.applygenerator() 258 259 # wrap results ~/.local/lib/python3.6/site-packages/pandas-0.24.2-py3.6-macosx-10.7-x86seriesgen): --> 286 results[i] = self.f(v) 287 keys.append(v.name) 288 except exception as e: ~/recommenders/recointeractinteractstatus, on=self.colinteractitem + ""interactitem + ""item + ""test""], axis=1) 186 testinteractuser, self.colnegative""]], on=self.col_user, how=""left"") 187 typeerror: (""unsupported operand type(s) for -: 'float' and 'set'"", 'occurred at index 854') ...",question,question
185,https://github.com/mozilla/TTS/issues/185,export model PyTorch -> ONNX model -> Tensorflow?,"sir i tried to convert tts pytorch pretrained model to tensorflow by using onnx open source platform. i tried but i got some issues, doubts. i can share that in our repo. we have to pass one input variable (dummy variable) with the correct shape. in this place i was struggled lot sir. i have a doubt. 1) what is our input variable shape? e.g [1,2] is shape of the input variable(text). (or ) tactorn five dimentional arguments. e.g (61, 256, 1025, 80, 2) sir i was created issue in pytorch. i asked this issue about onnx community also sir. sir. i will show you my error logs.:) gpu. cpu only. i was passed huge dimensional input variable for our tts. that is why memory allocation error occurred. i don't know full model architecture. sir please help me to improve this thought. export model pytorch to tensorflow. sir please give some suggestion about input from the architecture. thank you so much tts team.:)",question,question
132,https://github.com/mozilla/TTS/issues/132,Update outdated notebooks and colab example.,some of the notebooks might be outdates with the recent changes. - [x] benchmark - [x] generate spectrogram - [x] readarticle - [x] colab,other,other
39,https://github.com/microsoft/recommenders/issues/39,add official Microsoft COPYRIGHT notice to the source code,add header to all python files in the repo before releasing.,other,other
3073,https://github.com/streamlit/streamlit/issues/3073,Background image,"i was using the code below to place an image as the background of a streamlit page with an earlier version, 0.58.0. i upgraded to version 0.79.0 and the background stopped working. if i return to version 0.58.0, the code works again.",question,other
3805,https://github.com/streamlit/streamlit/issues/3805,Can`t load streamlit app o browser after succesfull deployment on aws server,summary after successsful deployement of streamlit app from aws ec2 instance i can`t open it on my browser. in case of local system it runs smoothly. i cross checked aws logs but there is nothing like tha to not load my app on client side.debug info - streamlit version:0.88.0 - python version: 3.x,other,other
291,https://github.com/mozilla/TTS/issues/291,ModuleNotFoundError: No module named 'WaveRNN',"hi there, actually i'm tryin to test the tts server with my checkpoint using also my wavernn training. `root@genesis-desktop:/opt/tts# python3 server/server.py -c server/conf.json > loading tts model ... > model config: /opt/tts/results/mailabs-september-15-2019133000.pth.tar > setting up audio processor... > samplemels:80 > mindb:-100 > framems:12.5 > framems:50 > refdb:20 > numlimnorm:true > symmetricfmin:0 > melnorm:1.0 > cliptrimfft:4096 > hoplength:800 > using model: tacotron2 traceback (most recent call last): file ""server/server.py"", line 14, in synthesizer = synthesizer(config) file ""/opt/tts/server/synthesizer.py"", line 32, in _wavernn(config.wavernnpath, config.wavernnfile, config.wavernncuda) file ""/opt/tts/server/synthesizer.py"", line 71, in loadinstall ) actually, using my tts training ( italian dataset 130k steps ) and using gl for speech synthesis, my jetson nano takes about 15 seconds with a short simple phrase in terms of ""timing"", how differ in using the wavernn model instead of the griffinlim? regards massimo",question,question
2689,https://github.com/streamlit/streamlit/issues/2689,Color picker not working on Streamlit Share,"summary color picker returns `securityerror: permission denied to access property ""document"" on cross-origin object` on firefox or `securityerror: blocked a frame with origin "" from accessing a cross-origin frame.` on chrome, upon color selection. firefox : chrome: steps to reproduce app: code: is this a regression? yes",Error,other
694,https://github.com/iperov/DeepFaceLab/issues/694,Face extraction not working: Failed to load the native TensorFlow runtime,"i'm using deepfacelab9.2src or datacudasse\tensorflow.py"", line 58, in file ""d:\dfl\deepfacelab9.2internal\python-3.6.8\lib\site-packages\tensorflow\python\pywrapinternal.py"", line 28, in file ""d:\dfl\deepfacelab9.2internal\python-3.6.8\lib\site-packages\tensorflow\python\pywrapinternal.py"", line 24, in swighelper file ""imp.py"", line 243, in loaddynamic importerror: dll load failed: the specified module could not be found. during handling of the above exception, another exception occurred: traceback (most recent call last): file ""d:\dfl\deepfacelab9.2internal\deepfacelab\joblib\subprocessorbase.py"", line 59, in run file ""d:\dfl\deepfacelab9.2internal\deepfacelab\mainscripts\extractor.py"", line 72, in oncudasse\all file ""d:\dfl\deepfacelab9.2internal\deepfacelab\nnlib\nnlib.py"", line 180, in importcudasse\importcudasse\cudasse\cudasse\tensorflow.py"", line 74, in importerror: traceback (most recent call last): file ""d:\dfl\deepfacelab9.2internal\python-3.6.8\lib\site-packages\tensorflow\python\pywrapcudasse\tensorflowcudasse\tensorflowimportmodule file ""imp.py"", line 343, in load_dynamic importerror: dll load failed: the specified module could not be found. win 10 home 64-bit nvidia geforce gtx 1050 processor: intel core i5 ram 8gb python 3.8 cuda 9.2 installed with its compatible cudnn tensorflow installed automatically with python i guess cuz i can't get around installing it from the website.... i really don't have any programming experience at all. please help",other,question
236,https://github.com/iperov/DeepFaceLab/issues/236,Does RCT/LCT works properly in newest build ?,"lct and rct does not change anything in mask colour. this is not tech support for newbie fakers post only issues related to bugs or code expected behavior actual behavior steps to reproduce other relevant information - ** 3.5, 3.6.4, ... (if you are not using prebuilt windows binary)",question,other
680,https://github.com/mozilla/TTS/issues/680,How Can I build my own vocoder model,hi everybody i have just finished training stage and i would like to build my own vocoder model. in particular i would like to know how can i build the following files to make a demo.py configstatsmodel.pth.tar thanks a lot for your help.,question,question
378,https://github.com/iperov/DeepFaceLab/issues/378,ValueError: cannot convert float NaN to integer,"this is not tech support for newbie fakers post only issues related to bugs or code expected behavior ran ""train hd64"" against data set, for which it stopped around 37k iterations. conditions: used model defaults for the model. i then cleared the workstation, and then tried to rerun the model with the same result. now when i run the model, it simply gives me a valueerror (see attached) other relevant information - ** 3.5, 3.6.4, ... (if you are not using prebuilt windows binary)",question,other
543,https://github.com/deezer/spleeter/issues/543,[Bug] name your bug,description step to reproduce 1. installed using `...` 2. run as `...` 3. got `...` error output environment ----------------- ------------------------------- os windows / linux / macos / other installation type conda / pip / other ram available xgo hardware spec gpu / cpu / etc ... additional context,other,question
140,https://github.com/mozilla/TTS/issues/140,synthesize a batch of sentences,"i noticed that maybe long text could be split into several independent parts, and then let the `text2mel` model generate corresponding `mel` simultaneously, and finally concatenate them as one part . is it available for synthesizing batch sentences simultaneously using `tts` ?",other,other
671,https://github.com/streamlit/streamlit/issues/671,Warn users when they try to run Streamlit on an `ipynb` file.,"problem have tried to run streamlit on a jupyter notebook (`.ipnyb`) file and are confused when this doesn't work.solution it would be great if for non `.py` file, streamlit behaved as follows:",other,other
696,https://github.com/streamlit/streamlit/issues/696,text_area Ctrl-Enter action does not update,"summary the textarea widget and an st.write() for the output 2. enter some text 3. hit ctrl-enterexpected behavior: page runs and value reported by st.write()actual behavior: no effect workaround is to click elsewhere on the pageis this a regression? that is, did this use to work the way you expected in the past? nodebug info - streamlit version: 50.2 - python version: 3.7.3 - using conda? for python install, pip for everything else - os version: windows 10 - browser version: chrome 78.0.3904.97additional information",Error,Error
1382,https://github.com/microsoft/recommenders/issues/1382,[Typo?] Code clone tab points to old master and not to new main,description i cloned the repository as below but it seems to point to an old version of master.,question,Error
1472,https://github.com/streamlit/streamlit/issues/1472,module xlwings import fails,"summary when i streamlit run my .py script, i get an error message in the browser window : ' couldn't find a license key.' this concerns the module 'xlwings'. at first the module was not found but then i installed streamlit and xlwings in the correct virtual env. but now the issue concerns the licence key for the pro usage that offers xlwings. however, i do not have any issue when using my script without streamlit.steps to reproduce 1. install xlwings in anaconda 2. activate the venv where xlwings has been installed and install streamlit via pip 3. streamlit run the .py scriptexpected behavior: to display the web page with the created title and markdown without any errorsactual behavior: the web page opens with the error message and nothing else: systemexit: couldn't find a license key.",question,question
366,https://github.com/deezer/spleeter/issues/366,[Discussion] Command line works but not api? Spleeter-GPU,"this boggles my mind, has anyone had similar experience?",question,other
272,https://github.com/iperov/DeepFaceLab/issues/272,DF 256 model,"hi i would like to upgrade df model from 128 to 256 i just change the parameter in model.py from 128 to 256 , but it didnt work, are there any other parameters else in other files i should change. btw my graphic card have 24gb vram, i think it's enough to run. appreciate any reply^^",question,question
35,https://github.com/deepfakes/faceswap/issues/35,python faceswap.py train unormal message!!!!!,"when i train using above command, it doesn't start training and just print the positional arguments and optional arguments message ,then it stop and receive no error. do you know why? @joshua-wu @clorr",question,question
1318,https://github.com/streamlit/streamlit/issues/1318,Uber Demo: InternalHashError: Type aliases cannot be used with isinstance().,"summary type here a clear and concise description of the bug. aim for 2-3 sentences. trying the example code at to reproduce what are the steps we should take to reproduce the bug: copied and pasted the full code from the end of the md into pycharm 2019.2 ran streamlit run uber.py (my name for it) in a pycharm terminal window expected behavior: explain what you expect to happen when you go through the steps above, assuming there were no bugs. expected to see the ny uber histogram. actual behavior: the web page is present with the expected heading followed by the internalhasherror: type aliases cannot be used with isinstance(). explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem. this is too long to capture it all in one go but after line 2 in the traceback it looks like everything is internal to streamlit. is this a regression? that is, did this use to work the way you expected in the past? yes / no unknown - my first foray into streamlit.debug info - streamlit version: (get it with `$ streamlit version`) 0.57.3 - python version: (get it with `$ python --version`) 3.5 - using conda? pipenv? pyenv? pex? pycharm 2019.2 - os version: windows 10 os build 18362.720 - browser version: firefox 74.01 (64 bit)additional information numpy 1.18.2 pandas 0.24.2 altair 4.0.1 if i comment out the st.cache decorator i get a new error:- typeerror: 'ellipsis' object is not iterable which appears to originate from st.barvalues) should i log a bug report for that too? if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here! copied and pasted code from end of that page.",question,Error
3530,https://github.com/streamlit/streamlit/issues/3530,Image Caption beyond Image Width,"summary image caption beyond image widthsteps to reproduce code snippet: ** python3.8, streamlit 0.79 - streamlit version: 0.84 - python version: 3.8 - using conda - os version: server: ubuntu, - browser version: chrome hope to return to 0.79 effect",Error,other
36,https://github.com/mozilla/TTS/issues/36,Min DB and Ref DB,"neither the tacotron 2 or tacotron paper mentioned anything about any decibel normalization, can you help me understand why this is necessary? relevant config:",question,question
434,https://github.com/streamlit/streamlit/issues/434,Improve handling of common widget implementation bits?,"we have a growing number of conventions that each widget implementation should follow (e.g. taking a ""key"" parameter; calling `widgetvalue` with the proper type, etc). can we consolidate some of these into a class structure, or some other organizing structure that doesn't abuse our (already difficult to grok) deltagenerator decorators? (see motivating conversation here:",question,other
1862,https://github.com/streamlit/streamlit/issues/1862,Create and Replace Modal Component,"we know of six locations for the modal component (and subcomponents). instead, we will use baseweb's goal is to mimic the same look and feel as before and announce if there are any changes that are unattainable. please display before and after images in pull requests.",other,other
30,https://github.com/iperov/DeepFaceLab/issues/30,convert(seamless) has Exception,"expected behavior convert successful actual behavior converting: 35%███████████████████████▌ 208/601 [00:11<00:22, 17.57it/s]exception while process data [undefined]: traceback (most recent call last): file ""d:\ai\fake\deepfacelab-master\utils\subprocessorbase.py"", line 233, in subprocess file ""d:\ai\fake\deepfacelab-master\mainscripts\converter.py"", line 159, in onclientprocessdata file ""d:\ai\fake\deepfacelab-master\models\convertermasked.py"", line 169, in convertface cv2.error: opencv(3.4.3) c:\projects\opencv-python\opencv\modules\core\src\matrix.cpp:465: error: (-215:assertion failed) 0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows in function 'cv::mat::mat' steps to reproduce convert(hist-match) is ok but convert(seamless) and convert(seamless-hist-match) throw the exception while proccessing about 32%(always in different srcfiles) python main.py convert --input-dir input\ --output-dir porn\ --aligned-dir sort\ --model-dir m\ --model liaef128 --mode seamless other relevant information - ** 3.6",question,Error
1451,https://github.com/microsoft/recommenders/issues/1451,[FEATURE] Avoid triggering the tests when there is a change in markdown files,description expected behavior with the suggested feature other comments,other,other
1856,https://github.com/streamlit/streamlit/issues/1856,"For beta_set_page_config, page_title still includes ""Streamlit""","when running `st.betapagetitle=""streamlit-embedcode documentation"",)`, the streamlit app appends an extra ` 路 streamlit` value to the string: `streamlit-embedcode documentation 路 streamlit` in letting the user set the value, we should not display anything other than what the user wrote.",other,Error
242,https://github.com/streamlit/streamlit/issues/242,Streamlit failed to hash an object,"i am trying to get the dataframe demo from `streamlit hello` to work. i am running this using anaconda and python 3.6. i get the following error in the browser: i get a bunch of additional errors on `streamlit failed to hash an object of type .,` and then this error blob:",Error,question
84,https://github.com/iperov/DeepFaceLab/issues/84,Always error when try Converter AVATAR.,"error: `running converter. loading model... ===== model summary ===== == model name: avatar == == current epoch: 1177 == == options: == == batchgpu : false == == createdgb : 11 == running on: == == [0 : geforce gtx 1080 ti] ========================= running on cpu0. running on cpu1. running on cpu2. running on cpu3. running on cpu4. running on cpu5. converting: 0% 0/866 [00:00<?, ?it/s]exception while process data [c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datainternal\bin\deepfacelab\utils\subprocessorbase.py"", line 244, in subprocess file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\utils\dflpng.py"", line 262, in load valueerror: no dfl data found in c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datadst\00006.png]: traceback (most recent call last): file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\mainscripts\converter.py"", line 159, in onclientprocessdata file ""c:\users\zerocool22\documents\deepfacelabtorrent\dst\00006.png exception while process data [c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datainternal\bin\deepfacelab\utils\subprocessorbase.py"", line 244, in subprocess file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\utils\dflpng.py"", line 262, in load valueerror: no dfl data found in c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datadst\00003.png]: traceback (most recent call last): file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\mainscripts\converter.py"", line 159, in onclientprocessdata file ""c:\users\zerocool22\documents\deepfacelabtorrent\dst\00003.png exception while process data [c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datainternal\bin\deepfacelab\utils\subprocessorbase.py"", line 244, in subprocess file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\utils\dflpng.py"", line 262, in load valueerror: no dfl data found in c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datadst\00001.png]: traceback (most recent call last): file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\mainscripts\converter.py"", line 159, in onclientprocessdata file ""c:\users\zerocool22\documents\deepfacelabtorrent\dst\00001.png ` ** `running converter. loading model... ===== model summary ===== == model name: avatar == == current epoch: 1177 == == options: == == batchgpu : false == == createdgb : 11 == running on: == == [0 : geforce gtx 1080 ti] ========================= running on cpu0. converting: 0% 0/866 [00:00<?, ?it/s]exception while process data [c:\users\zerocool22\documents\deepfacelabtorrent\workspace\datainternal\bin\deepfacelab\utils\subprocessorbase.py"", line 244, in subprocess file ""c:\users\zerocool22\documents\deepfacelabtorrent\internal\bin\deepfacelab\utils\dflpng.py"", line 262, in load valueerror: no dfl data found in c:\users\zerocool22\documents\deepfacelabtorrent\workspace\data_dst\00001.png ` yes, epoch number is low because i want to see if it works or not, before i let the pc training all night and like i suspect it shows a error. all other converters works fine, the only one i have issues is with avatar. i have the last repo version.",question,question
306,https://github.com/deepfakes/faceswap/issues/306,Is it possible to implement occlusion masks to original model?,i think gan model's most interesting feature is occlusion masks. but original model is more stable than gan and the output of gan code here is not good. so my question is can we implement this occlusion mask feature to original model? or is it exclusive to gan?,other,question
528,https://github.com/deezer/spleeter/issues/528,A question about the length of separation of audio files,"excuse me.why do i use a 20 minute audio file,but output is two 10 minute audio files,when i use 2stems model.",question,question
631,https://github.com/microsoft/recommenders/issues/631,[BUG] FastAI quickstart notebook failure,"description it looks like the failure of fastai is owing to the train-valid split in model building. cell 17 in the creates a new test dataset that removed the seem items. however, `traindf` also includes the user-item pairs in the validation set (not sure if there is a way to find out the user-item pairs used in the validation dataset in the model building). these items are not seen in the training set for building the model. in which platform does it happen? azure data science virtual machine how do we replicate the issue? clone the notebooks and run the fast ai expected behavior (i.e. solution) it should run successfully other comments",Error,Error
333,https://github.com/streamlit/streamlit/issues/333,map ScatterplotLayer getRadius unpredictable,"i am trying this script to show a map using scatterplotlayer , by default it works fine, now when i try to customize the radius of the dots, i use this function 'getradius' here is a reproducible example sometimes, it show empty map, when i manually change the value of 'getradius' in the python file to something else then it works.",other,question
283,https://github.com/deezer/spleeter/issues/283,[Bug] spleeter seperating my own mp3 failure by using docker image  ,"description running the default command was successful: docker run -v $(pwd)/output:/output researchdeezer/spleeter separate -i audiopath=/model researchdeezer/spleeter separate -i /input/song1.mp3 -o /output` 3. got `terminate called after throwing an instance of 'std::badalloc environment ----------------- ------------------------------- os windows7 64bit installation type docker image,virtual machine 2 processor,2g ram ram available total 8g ram hardware spec intel i5-6200 additional context",question,question
87,https://github.com/deezer/spleeter/issues/87,all stems are same as original / no separation,"description spleeter loads audio (in all modes 2/4/5), creates to output files, but all files are the original file. spleeter does no separtion run in cpu mode step to reproduce 1. installed using miniconda in terminal 2. run as seperat spleeter:2stems, spleeter:4stems, spleeter:5stems 3. no errors output last login: tue nov 12 21:38:48 on ttys000 (base) carstens-macbook-pro:~ carstenkupsch$ conda activate spleeter-cpu (spleeter-cpu) carstens-macbook-pro:~ carstenkupsch$ spleeter separate -i spleeter/audioexample.mp3' from 0.0 to 600.0 info:spleeter:audio data loaded successfully info:spleeter:file output/audioexample/accompaniment.wav written (spleeter-cpu) carstens-macbook-pro:~ carstenkupsch$ spleeter separate -i spleeter/audioexample.mp3' from 0.0 to 600.0 info:spleeter:audio data loaded successfully info:spleeter:file output/audioexample/drums.wav written info:spleeter:file output/audioexample/other.wav written (spleeter-cpu) carstens-macbook-pro:~ carstenkupsch$ spleeter separate -i spleeter/audioexample.mp3' from 0.0 to 600.0 info:spleeter:audio data loaded successfully info:spleeter:file output2/audioexample/piano.wav written info:spleeter:file output2/audioexample/bass.wav written info:spleeter:file output2/audio_example/other.wav written (spleeter-cpu) carstens-macbook-pro:~ carstenkupsch$ environment ----------------- ------------------------------- os mac os 10.12.6 installation type miniconda hardware spec 2.3 i7, 16 gb, 512 nvidia gt 650 m512 mb",Performance,other
2827,https://github.com/streamlit/streamlit/issues/2827,Ability to specify dataframe sorting via code,"via @tvst make this work: `st.dataframe(df, sortorder='ascending')`",other,other
1560,https://github.com/microsoft/recommenders/issues/1560,[BUG] Errors in benchmark notebook,"description when running the benchmark notebook for movielens, several errors occur. first, the als model crashes on `recommendals(model, test, train, topseen) 100 (dfsuseruserpred[defaultcol] == train[defaultcol]), --> 102 how=""outer"", 103 )` in `/content/benchmarkinput = tf.compat.v1.placeholder(tf.int32, shape=[none, 1])` in `recommenders/models/ncf/ncfcreate_model(self)` in which platform does it happen? google colab notebook how do we replicate the issue? by running expected behavior (i.e. solution) it should run without errors other comments",deployment,Error
201,https://github.com/deepfakes/faceswap/issues/201,MyFakeApp,"not an issue, but i am letting you know, that i created ui application for windows x64. you can read about it here: let me know if it works or not. thanks.",other,other
716,https://github.com/deepfakes/faceswap/issues/716,"When I try to convert images,i got following errors",** add any other context about the problem here.,Error,question
1087,https://github.com/microsoft/recommenders/issues/1087,[FEATURE] Adding TF-IDF recommender using COVID-19 dataset,"description i am planning on contributing a simple tf-idf recommender using the in the form of a quick start notebook and a deep dive notebook (with the appropriate util scripts and unit tests). expected behavior with the suggested feature content-based recommendation with tf-idf is a great starting point for content-rich scenarios. other comments i feel this simple implementation could serve as a useful starting point for those looking to get started with something simple. in addition, the use of this new dataset may be more interesting than the movielens dataset for some readers.",other,other
261,https://github.com/mozilla/TTS/issues/261,Installing with pip under Anaconda,"hello, i'm using mozilla tts for a project where i have a python app that synthesize text to audio. so i'm using mozilla tts as a lib, i need to import it in my app. right now i'm installing using: my current usage is: `from models.tacotron import tacotron`, but should be `from mozillatts folder, regarding setup.py guidelines. also this will be a pain to backport to every branch. let me know what you think",question,other
1190,https://github.com/streamlit/streamlit/issues/1190,Need hash_func for type <class ‘sqlalchemy.engine.base.Engine’>,"summary a community member noticed a ""cannot hash of type"" issue. more information can be found in .full error message this is the error the community member received: ```streamlit cannot hash an object of type <class qlalchemy.engine.base.engine",Error,Error
60,https://github.com/mozilla/TTS/issues/60,How to train a male voice?,how train a male voice?,question,question
3537,https://github.com/streamlit/streamlit/issues/3537,PR to solve st.image() svg issue,"summary `st.image()` doesn't work when the `image` argument is a url pointing to an svg image, or any svg file, really this is understandable, since the docstring on says `image` takes in `an svg xml string like `steps to reproduce code snippet: or any svg file: ** can i do a pr for this (here's a protoype): this way, the user needn't enter an svg xml string to pass in a `.svg` file. so, directly displays the svg onto the streamlit application, without ever entering in the svg xml as the `image` argument. `requests` and `bs4` are dependencies that might have to be included in `requirements.txt` according to this proposed pr. tested and works.",Error,Error
656,https://github.com/mozilla/TTS/issues/656,ERROR Key when I try synthesize,"hi. i try mozillatts in a pc and i didn't have problems to synthesize some audios. i try mozillatts in an other pc but now when i try to synthesize it wasn't possible, but the message was: key already registered with the same priority: groups partial softmax what can i do to solve this problem? thanks a lot",other,question
905,https://github.com/streamlit/streamlit/issues/905,Print path of the uploaded file,"i am uploading a zip file on streamlit and extracting it using following code: the file extracts properly but when i print uploadedfile path i get , what i want is the complete path of this file. like . how do i get that ?",question,question
321,https://github.com/deezer/spleeter/issues/321,google colab error,nameerror traceback (most recent call last) in () ----> 1 audio(song.mp3) nameerror: name 'song' is not defined,question,other
247,https://github.com/microsoft/recommenders/issues/247,Improve metrics explanation in evaluation notebook,"*what* is affected by this bug? this notebook: expected behavior (i.e. solution) definitions are ok but proper guidance would be something like: - you have ratings: use rmse or mae, here’s why and what they mean - you have binary click data: use x depending on what you care about, e.g. x = precision@k means that you care about presenting mostly relevant results (but not necessarily all relevant results)",other,question
583,https://github.com/streamlit/streamlit/issues/583,Streamlit stuck on loading screen on Docker,"summary when attempting to run streamlit under docker it is stuck on the ""connecting"" screen.steps to reproduce 1. `docker run -it --rm -p 42851:8501 python:3.7 /bin/bash` 2. `pip install streamlit` 3. `streamlit hello` 4. go to 5. streamlit is stuck ""connecting"" the issue also appears when using a dockerfileexpected behavior: streamlit should finish ""connecting"" and start the hello appactual behavior: the application is stuck on ""connecting"" screen (checked for over 1h before giving up): output from the terminal: is this a regression? nodebug info - streamlit version: streamlit, version 0.49.0 - python version: python 3.7.5 (same behaviour on python 3.8 and latest anaconda) - using docker: docker version 19.03.4, build 9013bf583a - os version: linux myserver 4.9.0-9-amd64 #1 smp debian 4.9.168-1+deb9u5 (2019-08-11) x86_64 gnu/linux - browser version: edge 79.0.309.5 (official build) dev (64-bit",deployment,deployment
417,https://github.com/streamlit/streamlit/issues/417,add CONTRIBUTING.md file,add contributing.md file with a short description and a link to the contribution guide wiki. this will add a nice contribution message on the top of issues list similar to what (react)[ has in its.,other,other
400,https://github.com/streamlit/streamlit/issues/400,Ability to download data from Streamlit,"[edited by tvst, since we narrowed the focus of this feature request] i'd love to be able to: do stuff with it - easy with streamlit, as users can tweak stuff till they get the results they are happy with. * the existing streamlit object which displays stuff on screen could have a download option for objects like dataframes. so st.write(data) or just st.dataframe(data) should have a download button somewhere. sort of how some of the plotting tools in python like plotly displays a download button for charts.",other,other
2231,https://github.com/streamlit/streamlit/issues/2231,Unexpected behavior of st.write() for strings including brackets '[]',"summary st.write() cuts out parts of strings that include brackets, e.g. '[text2] text1] text_3'. in contrast st.text() with the same string works as expected. functions st.header(), st.title() are affected, too.steps to reproduce just try out the following code: run 'streamlit run test.pyexpected behavior: st.write(), st.header(), st.title() should print the same string as st.text().actual behavior: debug info - streamlit version: 0.67.1 - python version: 3.8.2 - os version: macos catalina, version 10.15.6 - browser version: safari version 13.1.2 (15609.3.5.1.3)",question,Error
416,https://github.com/iperov/DeepFaceLab/issues/416,Command line show   no attribute 'extract'   when I try to extract the faces ,"i don't know how to describe it, and it just showed these: exception while process data [d:\deepfacelabtorrent\workspace\datainternal\bin\deepfacelab\joblib\subprocessorbase.py"", line 68, in run file ""d:\deepfacelabtorrent\data attributeerror: 'dlibextractor' object has no attribute 'extract' and then paused. i read the source code and find nothing...where can i find this dlibextractor?",Error,question
3242,https://github.com/streamlit/streamlit/issues/3242,"When I create the image docker and try it to run localy I recive ""Streamlit requires raw Python (.py) files, but the provided file has no extension.\nFor more information, please see https://docs.streamlit.io""","summary i built the images docker and when i try to run locally i receive the next error ""streamlit requires raw python (.py) files, but the provided file has no extension.\nfor more information, please see steps to reproduce build images and run packages i used config.toml and credentials.toml as mark recommend it i know abhinand5 added this pull request to the code in case the people don't use .py extensions. however, this is not the case. i have app.py file in my folder.debug info - streamlit==0.81.1 - python version: 3.7.10 - ubuntu 18.04fix i fixed it changing the docker file to i know this is not the best way to fix it, that's why i created the issue. best adonai,",question,question
779,https://github.com/iperov/DeepFaceLab/issues/779,Can I get a resolution higher than 192 with a 6gb RTX 2060 ?,"can i get a resolution higher than 192 with a 6gb rtx 2060 ? because when i try higher values, it seems to crash. and of course, i use multiples of 16.",question,question
3322,https://github.com/streamlit/streamlit/issues/3322,Add complex validation for user email,"right now in `credentials.py`, we check only that the email is not empty and the `'@'` symbol exists. so for example `'@'` string itself passes our validation as an email. suggestion: use more complex email validation. for example this one (used by pydantic) or django emailvalidator regex",other,other
273,https://github.com/deezer/spleeter/issues/273,[Bug] Failed to load the native TensorFlow runtime.,"description step to reproduce 1. put the seperate command 2. pressed enter 3. got `failed to load the native tensorflow` error output ps c:\users\purple flippy\music> spleeter separate -i 'song.wav' -p spleeter:4stems -o splits traceback (most recent call last): file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywraptensorflowtensorflowimportmodule file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\imp.py"", line 342, in loadrunasrunlogging file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\site-packages\spleeter\utils\logging.py"", line 27, in getlogger file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\site-packages\tensorflow\_tensorflow.py"", line 74, in importerror: traceback (most recent call last): file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywraptensorflowtensorflowimportmodule file ""c:\users\purple flippy\appdata\local\programs\python\python37\lib\imp.py"", line 342, in load_dynamic importerror: dll load failed: a dynamic link library (dll) initialization routine failed. see for some common reasons and solutions. include the entire stack trace above this error message when asking for help. environment ----------------- ------------------------------- os windows 10 installation type powershell ram available 16gb hardware spec gpu / cpu / etc ... additional context",question,question
224,https://github.com/deepfakes/faceswap/issues/224,Align rotation of input faces for GAN conversions,"currently, the extractor finds a rotation matrix for each face using umeyama so it can generate a faceset with all the faces mostly upright. unfortunately this rotation matrix isn't stored in the alignments file, only the bbox (of the un-rotated face) and facial alignments. for the gan model, when it comes time to convert, the faces aren't rotated upright before being fed through the model so i doubt anyone has been able to get good results for faces that aren't completely upright. i propose we store the rotation matrix in the alignments file during extract, then at conversion, re-apply it to the cropped face to make it upright before feeding through the model. the swapped output face then needs to be rotated in the inverse direction to match it with the frame again. hopefully this is possible.",other,other
643,https://github.com/mozilla/TTS/issues/643,Adding the training step to best_model.pth.tar,"this is a suggestion/discussion on changing the way the training scripts save the best model parameters. currently the best model is saved as `bestmodelpath)['step']`, but that is a bit involved if you want to sort through models via a file browser or terminal, - several best models could be saved in the experiment folder as training progresses. 2. it would allow for continuing training from the model output at the last saved step. currently the model with the last creation time is used for continuing training via `args.restoreofmodel.pth.tar` so it's easy to use the models in synthesis by providing just the path name. this can be mitigated by creating a symbolic link in the experiment folder `bestmodels_xxx.pth.tar`. this can also be avoided by deleting the previous model when saving the new one. even better the user can select if he only wants to keep the last model or all models along the way. personally, i would prefer to keep the last checkpoint and all the best models (after some minimum step say 10k), rather than a single best model and a lot of checkpoints.",other,other
182,https://github.com/streamlit/streamlit/issues/182,Fix deltagenerator replacement in sidebar,try this: x = st.sidebar.text('hi') x.test('hello'),other,other
1202,https://github.com/streamlit/streamlit/issues/1202,FileUploader API enhancements,"file uploader was released but has since accumulated several bugs and feature requests. this project will iterate on the file uploader and address items that were passed over as part of the mvp and items identified by the community. changes include the following: - [x] support multiple file uploads. return a list if multiple files allowed. otherwise return a singleton. - [x] return a binary buffer with the following attributes 1. file name 2. file type 3. size - [x] update the ux to provide file upload progress and file information - [x] map file extension aliases (i.e. jpg + jpeg, xls + xlsx) - [x] create disabled state - [ ] ensure binary buffers are reset to the beginning after being read - [ ] add a check to make sure backend and frontend are in sync - [ ] optimize caching - [ ] bug fixes for mobile support nice to haves - [ ] allow a max upload size less than 1 mb - [ ] keep file display list a constant height so actions within file display does not shift the page up/down (i.e. paginating between pages with 2 vs 3, deleting 5th item) - [ ] toggle to show/hide uploaded files for multiple file uploader - [ ] make file a link to download/open - [ ] when in single-file mode, keep the gray box and the ""browse"" buttons, but then replace everything else with the uploaded file so users can replace a file in one action.",other,other
383,https://github.com/streamlit/streamlit/issues/383,Streamlit's fonts don't display properly on Microsoft Edge,"summary i haven't tried this myself, but apparently, streamlit's fonts .",question,Error
911,https://github.com/streamlit/streamlit/issues/911,Embedding Streamlit on a webpage,"first, streamlit is amazing. is it possible to embed a streamlit app into an existing webpage? i made a simulation that would be great for marketing, but i need to be able to style the page to match the brand more than i currently can with the library, so embedding would be the best option. thanks and wishing an amazing year to streamlit!",question,other
118,https://github.com/deezer/spleeter/issues/118,[Discussion] How many examples do I need for training new models,"i only have a basic understanding of machine learning but just interested to know the minimum number of examples i need to train a new model? also, suppose i have already used spleeter to produce a 1000 acapellas, and i already have the corresponding studio acapellas, can i then go ahead and train it on these so that i could try and remove the imperfections on any other random diy acapellas?",question,question
1150,https://github.com/deepfakes/faceswap/issues/1150,Is faceswap not compatible with RTX 3090?,"** destop information is attach to attachment, it's a window10 information. but ubuntu20 has the same problem.",question,other
273,https://github.com/deepfakes/faceswap/issues/273,TypeError: unorderable types: NoneType() < int(),@torzdf you didnt add backward compatibility in your image rotating system,Error,other
174,https://github.com/deezer/spleeter/issues/174,[Bug] no start spleeter GPU,description step to reproduce 1. installed using `...` 2. run as `...` 3. got `...` error output environment ----------------- ------------------------------- os windows10 installation type conda / cudnn 7.6.4 / cuda10.0_0 ram available 12 hardware spec gpu gt 1030 / cpu amd 1075t additional context,other,question
160,https://github.com/mozilla/TTS/issues/160,Is there any interest in providing a model trained in Brazilian Portuguese?,"hello, i recently developed a dataset for voice synthesis in brazilian portuguese using my own voice, called tts-porguese corpus, the base has approximately 10 hours of talk, is available at: i have already successfully trained the dctts model in the dataset. i created a branch in my repository to support the dataset in this repository, see here: in initial tests the model unfortunately did not converge, at the moment i'm out of time, and machine to train the model. is anyone interested in training/adjusting the hyperparameters of the model to get a tts model in portuguese?",other,other
243,https://github.com/microsoft/recommenders/issues/243,Create python script to benchmark different models with a dataset,"*whatwhereazure data science virtual machine.azure databricks.* expected behavior (i.e. solution) i am hoping that i can clone the repo, convert to python scripts (perhaps using a supplied makefile or script), setup the dependencies, and run the python scripts to be impressed by the power of the repository. other comments",other,other
404,https://github.com/deezer/spleeter/issues/404,[Discussion] Will higher cpu cores and ram speed up spleeter?,i currently have a quad core with 8gb ram and it gets stuck with spleeter. will using for example an 8 core cpu with 16gb ram make it faster or it wont make a difference?,question,question
594,https://github.com/deezer/spleeter/issues/594,[Discussion] Skip to download pretrained_models in Docker,"i made a aws lambda using docker and i found that every time lambda is called, it downloads a folder, ""pretrainedmodels"" together so that it does not have to download. however, it failed. secondly, i upload the folder ""pretrainedmodels"" and i load it too. however, this also does not work. i checked the directory where the folder should be located and there was no problem. are there any things that i did not think about or solutions? i want to get an answer of this problem.",question,question
288,https://github.com/mozilla/TTS/issues/288,Syntethized speech is too slow / too fast while sample rate is the same as in DS,"hello everyone! i trained the tacotron model and now want to test it. when i listen to the speech from tensorboard it sounds fine. but when it comes to do inference myself through synthesize.py or jupyternotebook, resulting speech becomes too slow. i attached few examples. i tried to change sample rate to lower and upper values - the only thing changes is pitch, while speed remains the same. tried to change some parameters in config, in utils/syntesize.py and even in audio.py - no success. please help! thank you! __rate:22050 > numlevelshiftlengthlevelfreq:1025 > power:1.5 > preemphasis:0.98 > griffiniters:60 > signalnorm:true > melfmax:10500.0 > maxnorm:true > dosilence:true > nlength:275 > win_length:1100 so, that's obviously the parameters from config.json, which i used to train the model.",question,Performance
684,https://github.com/iperov/DeepFaceLab/issues/684,SAEHD stuck,"hello. i'm trying to use ""train saehd"" on my pc, but i can't start it. it asks me all the questions about batch size, iterations etc. but get stuck, always, at initializing models 40% 2/5. the timer of the elapsed time also doesn't work. with quick96 everything goes well, but saehd can't be used. what's the problem? i'm running it on a gtx 1660",question,question
595,https://github.com/deezer/spleeter/issues/595,[Bug] AttributeError: module 'ffmpeg' has no attribute '_run',"- [x] i didn't find a similar issue already open. - [x] i read the documentation (readme and wiki) - [x] i have installed ffmpeg - [x] my problem is related to spleeter only, not a derivative product (such as webapplication, or gui provided by others) description ** `spleeter separate -p ""/home/mohammedmehditber/documents/basemwf"" -f {filename}-{instrument}.{codec} -d 7200 -c mp3 -b 320k --mwf -b tensorflow ""/media/mohammedmehditber/0e2e5dd12e5db287/users/windo/music/muhammad al muqit/anasheeds, vol. 4/1-04 graduation.mp3""` output environment ----------------- ------------------------------- os / linux / installation type pip ram available 6go hardware spec intel hd 3000 / core i5 / product name toshiba satellite l750/l755",question,question
2400,https://github.com/streamlit/streamlit/issues/2400,Can't copy/paste when `st.code()` is within an expander.,"summary can't see the `copy to clipboard` trigger aside the usual code widget whenever that code widget is within a `st.beta_expander()`. steps to reproduce sufficient code: expected behavior: i'd expect both to have the ""copy to clipboard"" icon. actual behavior: only the first code block has it.debug info - streamlit version: `streamlit, version 0.71.0` - python version: `python 3.6.10` - using `conda` - os version: `10.14.6` - browser version: `version 87.0.4280.67 `",Error,Error
293,https://github.com/mozilla/TTS/issues/293,"TypeError: expected str, bytes or os.PathLike object, not NoneType","while training the tts model i am getting this error. can you help me to solve this. traceback (most recent call last): file ""train.py"", line 614, in c = loadpath) file ""/home/jadi/tts/tts/utils/genericconfig with open(config_path, ""r"") as f: typeerror: expected str, bytes or os.pathlike object, not nonetype",question,question
334,https://github.com/streamlit/streamlit/issues/334,"Docs mention inexistent ""Share app"" menu item","summary ""tutorial: create a data explorer app"" includes a step to share a static version of the app: > that it, youe made it to the end. the last thing to do is share your findings. locate the hamburger menu in the upper-right corner of the app and select share app. the aforementioned feature is not in the menu though.steps to reproduce behavior: i expect to find a ""share app"" item in the hamburger menuactual behavior: no ""share app"" feature can be found in the hamburger menu. nor elsewhereis this a regression? i think so.debug info - streamlit version: 0.47 - python version: 3 - using conda? pipenv? pyenv? pex? no - os version: 10.14.6 - browser version: safari 12.1.2additional information either add the ""share app"" feature (please!) or remove it from the tutorial (i can do this if you want).",other,other
424,https://github.com/microsoft/recommenders/issues/424,Add an experimentation example with AML,this article from @danielsc can be used as a base,other,other
159,https://github.com/deepfakes/faceswap/issues/159,Commits in the last day seem to have reset training progress,"loss has jumped back up to the original values, and are descending no faster than when i originally started training a few days ago. is this an intended change? will future commits require a full retrain?",Performance,question
18,https://github.com/iperov/DeepFaceLab/issues/18,@andenixa,@andenixa fix it please,Error,other
3720,https://github.com/streamlit/streamlit/issues/3720,st.metric without delta doesn't have correct margin,"summary @willhuang1997 another super small bug with st.metric i just realized: if `delta=none`, the margin at the bottom is a lot larger than if delta has a value (see screenshot below). i assume this is because the delta is still rendered, just empty. super small thing and we can fix this for the next release!",Error,Error
2493,https://github.com/streamlit/streamlit/issues/2493,st.number_input doesn't accept reasonable int arguments,"summary using `st.number_input` in a very reasonable way: causes an exception to be thrown expected behavior: this should ""just work,"" in the sense that it should create a number input that accepts `int`s between 0 and 10, with an initial default value of 0.actual behavior: you get the exception above. you can ""trick"" streamlit into providing the right behavior by forcing the `value` parameter to have type `int` as follows: but i think this should just work without that extra parameter.is this a regression? ??debug info - streamlit version: `streamlit, version 0.73.0` - python version: `python 3.8.5` - python environment: `pipenv, version 2020.11.4` - os version: `ubuntu 20.04.1 lts`",Error,other
428,https://github.com/streamlit/streamlit/issues/428,Customize widgets,"problem i started developing web-app with streamlit see one everything is so easy and fast to learn and develop but in the long run i realised it is similar to jupyter notebook widgets. i was planning to build a custom login page but soon i realised i cannot change the dimensions of text boxes, neither their position. when there is some error occurs say password was wrong i am able to show a st.error() below the password filed but it takes lot of space what if i could be able to change the border color of text box to red to make it more appealing and it would take less space. solution ** i see streamlit more as a tool to design web based front-end applications then just showing data just like jupyter notebook, more customization is really needed, specially for changing the position and dimensions of the elements. a little bit animation would be awesome.additional context see this app i built, i can't customize it anymore i wish i could",other,other
641,https://github.com/microsoft/recommenders/issues/641,[BUG] Papermill Record is deprecated in Papermill 1.0 and has been replaced by scrapbook,description this applies to any notebook using pm.record to capture logs. in which platform does it happen? this applies to all platforms and any notebook using papermill.record how do we replicate the issue? run the sars notebook. the last cell uses pm.record and generates a warning: /data/anaconda/envs/recolauncher.py:3: deprecationwarning: function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality. this is separate from the ipykernel package so we can avoid doing imports until expected behavior (i.e. solution) other comments scrapbook does not appear to be a drop in replacement for pm.record .,Error,Error
75,https://github.com/streamlit/streamlit/issues/75,st.sidebar is showing under other widgets,"summary when i have two date widgets in the sidebar it is positioning one under the other. see photo attached.steps to reproduce what are the steps we should take to reproduce the bug: 1. place two st.dateinput('start date', datetime.date(2011,01,01)) st.sidebar.date_input('end date', datetime.date(2011,12,31))expected behavior: when widget is open it should pop over anything else in the sidebar until it is closed.actual behavior: shows underneath the other widget.is this a regression? uncleardebug info - streamlit version: 0.45.0",other,Error
177,https://github.com/iperov/DeepFaceLab/issues/177,YAW meaning? ,"thanks for all you hard work. "" sort by face yaw"", what exactly does the word ""yaw"" mean in this context? also, model ""liaef128"", what is the meaning of ""liaef"" thanks in advanced",question,question
342,https://github.com/streamlit/streamlit/issues/342,Mapping Demo does not work,"summary when type hello streamlit, all other demos work except for the mapping demo.steps to reproduce what are the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....'expected behavior: explain what you expect to happen when you go through the steps above, assuming there were no bugs.actual behavior: explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem. is this a regression? that is, did this use to work the way you expected in the past? i've never used it beforedebug info - streamlit version: streamlit, version 0.47.4 - python version: 3 - using conda? pipenv? pyenv? pex? pip - os version: - browser version:additional information if needed, add any other context about the problem here.",Error,question
803,https://github.com/streamlit/streamlit/issues/803,deck_gl_chart does not react to viewport state change.,"summary when viewport state is changed and streamlit rerenders the page with new state, `deckchart` map does not rerender with new viewport values. steps to reproduce expected behavior: deckgl accepts a parameter called viewport to determine what area and zoom level the map pans to. if the viewport parameters change in streamlit, i expect that the map changes what area it looks at. actual behavior: when i change the viewport values, the map doesn't re-render or pan to a different area. here is an example of a small tool i'm putting together. the intention is for a user to input the name of a region and have a map pan to that area, or at least rerender with it centered. first pass: i write an address, press enter, and it finds approriate viewport parameters and renders a deckgl map around verdun, montreal in canada. second pass: i write an address, press enter, and expect it to recenter around boston ma. this time, even though new viewport parameters are generated. the mao does not rerender. it stays centered on verdun, montreal, canada debug info - streamlit, version 0.51.0 - python 3.7.5 - virtualenv - osx 10.15.1 - chrome 78.0.3904.108 (official build) (64-bit)",other,Error
2943,https://github.com/streamlit/streamlit/issues/2943,Timit_asr dataset repeats examples,summary when loading timitasr dataset ** when loading timit64additional information you can check the same behavior on,question,other
2431,https://github.com/streamlit/streamlit/issues/2431,[File uploader] Displayed size of file uploader is not correctly calculated,examples: actual: 214.9kb expected: ~220kb actual: 207.8kb expected: ~213kb actual: 2.3mb expected: 2.4mb,Error,Performance
207,https://github.com/deepfakes/faceswap/issues/207,train.py loop behavior,"i'm speaking about: this can cause the thread to hang after last iteration. i did look into the issue, and unfortunately there are only two ways to solve it. 1. complicated way with additional imports. sadly, baseline python doesn't include tools we would need to implement desired behavior. we would need to import package with specialized key listeners. in my opinion that is an overkill. we could also just write our own code from scratch. but the readability would suffer and for very minimal gain. therefore i would prefer option 2. 2. drop the `enter` interrupt and revert to traditional `ctrl + c` behavior. everybody who works with cli is familiar with it and python has all the necessary stuff. it is omniplatform code for it is as simple as humanly possible. on chance that somebody doesn't understand it, we can always add some message explaining it. right now the old behavior provides little benefit anyway, since we can already limit the epochs. i'm asking because i don't want to make mess, submitting useless pr and so on. granted we could always keep things as they are. but you have to agree, right now it feels sloppy.",Error,other
678,https://github.com/streamlit/streamlit/issues/678,Text input with value sometimes reverts input,"summary when using an `st.text_input` and setting the `value=` argument, sometimes the user input is reset to the previous input.steps to reproduce run this streamlit script that reads a string from a file, sets it as value of a text input, and saves the returned value of the input to the file again: expected behavior: `a` + `enter` = before: a after: a input: a `b` + `enter` = before: a after: b input: b `c` + `enter` = before: b after: c input: cactual behavior: `a` + `enter` = before: a after: a input: a `b` + `enter` = before: a after: b input: b `c` + `enter` = before: b after: b input: b ** this happens on every second input, whenever `before` and `after` values are not the same. it works as expected if you use the input without a value: debug info - streamlit version: 0.50.2 - python version: 3.7.4 might be related to",Error,Error
1898,https://github.com/streamlit/streamlit/issues/1898,Images are adding in the url  while playing a video,images are coming continuously but i want it to be displayed on the same window. i tried to change the mediamanager.py file by changing the add function but i couldn't change it. please help me resolve the issue. please overwrite it on the first window it self.,other,other
1516,https://github.com/microsoft/recommenders/issues/1516,Run tests in the appropriate extra dependencies.,"description our recommender package has several `extra` dependencies to address the different compute environments: `recommender[spark,gpu,example,dev]`. we should run our tests against the specific extra dependency so we can ensure users of the library would have minimum problems installing these extra dependencies. expected behavior with the suggested feature for example: - a user `pip install recommenders[gpu, examples]` should be able to run recommender utilities tested by `pytest -m ""gpu and notebooks and not spark` other comments",other,other
883,https://github.com/deepfakes/faceswap/issues/883,How to train model with a mask in faceswap ?,** add any other context or screenshots about the feature request here.,question,question
48,https://github.com/streamlit/streamlit/issues/48,"""Always Run"" takes effect only after the Streamlit Window/tab is selected","summary when editing code and saving in my ide with enabled, the rerun is not immediately kicked off, but it seems i have to click on the streamlit ui in the browser.steps to reproduce 1. enable 'always run' 2. edit a script in an ide (i was using pycharm for this) 3. keep the 'focus' on the ide, but look at the streamlit ui, possibly on another monitor or on some other part of the screenexpected behavior: i expect to see the streamlit ui re-generate the results every time i save.actual behavior: nothing happens on the ui. if click on the ui, the computation starts and results are regenerated.is this a regression? that is, did this use to work the way you expected in the past? n/adebug info - streamlit version: streamlit v0.45.0 - python version: 3.7.4 - using conda? pipenv? pyenv? pex? - os version: macos 10.14.6 - browser version: chrome 76.0.3809.100",Error,Error
189,https://github.com/microsoft/recommenders/issues/189,Set the GPU environment,related to,other,other
1141,https://github.com/streamlit/streamlit/issues/1141,Allow configurable user-set CSS,"problem ichbestimmtnicht on the streamlit forum : > a quick idea: there is already the static/static/css/ folder. with a quick test for a corporate.${companyplaceholder}.css or some other indication method it could be possible to add (a folder within) corporate design css to overwrite the defaults. -> without having to rely on unsafehtml=truesolution ** better thing: allow users to create their own custom css folder in the root of their streamlit project. use the config option to name the path of that directory, and have streamlit load css files found there ahead of streamlit's.additional context",other,other
423,https://github.com/microsoft/recommenders/issues/423,generate_conda_file.sh package versions are too old (specifically tensorflow),is affected by this bug? gpu venv tensorflow package version is set as 1.5 which is quite outdated (cur ver is 1.12) and doesn't include high-level tensorflow apis. maybe other packages too. expected behavior (i.e. solution) * good to use the latest packages unless we have dependencies requiring specific versions of some packages),other,deployment
475,https://github.com/deezer/spleeter/issues/475,[Discussion] Dependency hell,"i get a huge amount of dependency issues on a fresh conda install on windows and ubuntu. i'm talking an absolutely huge number of conflicts, so many that it exceeds pastebin's character limit. they look like: this prevents me from using spleeter, which i'd really like to use. anyone know what's going on?",question,question
453,https://github.com/iperov/DeepFaceLab/issues/453,Bug in umeyama,fix is referred here it looks like it helps avoiding flipped faces.,Error,other
228,https://github.com/iperov/DeepFaceLab/issues/228,"Location of batch files source. I have a bug-fix for setenv, and other additional features if you like them.","expected behavior expected source tree to start at the same location as the build, or possibly has a link to another repository which hosts the batch files. actual behavior batch files are not included, nor am i aware of any other repository. steps to reproduce n/a comments hey iperov, awesome work on this project. i've been modifying your batch files for some time as i like to open a single command prompt instance and run the batch files from that instead of double-clicking them in explorer. this is due to me doing other workspace work like teeing logs of the program out to the workspace folder and backing up the model at intervals, etc. if you aren't aware, if you run this from a single instance of the command-line you will run into an error after a few runs where the scripts no longer work. i believe this is due to the way the path is set. i normally download the new version of builds and modify them every time. i've added other features as well which i could submit, but this is the one that i do immediately every time regardless. one of the other features is a user configuration .ini that allows people to override the master setenv values, as well as add their own (for example, if they want to set a base directory and then use that one of the overrides of a value that you use). the new setenv also shows the environment variables read in in a nicely formatted way, as well as fixing the bug i mentioned originally. the latter feature doens't have to be added, or i can make a fork, but it'd be nice to be able to submit the first one. i can: 1) create my own repository with just the batch files. upload the patch there and send you the link and you can pick and choose which ones you want, or start using it yourself. 2) until you decide what you want, simply upload the fix here for you. thanks! i acutally have a bunch of ideas that i think would combine with yours and literally make this into a suite of tools that would make this even more unstoppable and the only choice for people to use for deepfakes. looking forward to your response! (also, sorry about the length. i'd been wanting to ask this for a while and it's just been building up..)",other,other
1131,https://github.com/streamlit/streamlit/issues/1131,Hot load doesn't work in a package setting,"summary when changing an imported module/function's code, streamlit won't reload properly.steps to reproducesettingcode assume the following tree structure: with the following code: environment create a conda environment with python 3.7 and install streamlit using pip. in addition install the package using `pip install -e .`.what next? as expected, `5` is rendered. ` `6` is not rendered. rerunning the app doesn't help. only killing the app and restarting it picks up the change in `foo.py`.expected behavior: i would hope that streamlit will pick up changes in depending filesactual behavior: in order for the app to reflect changes in the imported code, the app has to be restarted.is this a regression? nodebug info - streamlit version: 0.56.0 - python version: 3.7.6 - using conda - os version: macos 10.14.6 - browser version: chrome 80.0.3987.106 additional information related to #358 as per @tconkling request.",deployment,Error
327,https://github.com/streamlit/streamlit/issues/327,Plug-in architecture for Streamlit,"problem it would be nice if users could create their own plugins for streamlit without forking the repo. this issue has also .starting point for the discussion i think it would be cool if the basic approach was to simply write a react component in pure and then run it through a streamlit compiler, e.g. which would produce a (e.g. ` my_widget-versionn-py2.py3-none-any.whl`) which could be pip installed or uploaded to .what about your ideas? please add comments below with your thoughts on requirements for this feature.",other,other
150,https://github.com/deepfakes/faceswap/issues/150,Multi-GPU training,i've read reports of people succesfully training on multiple gpu's using the following code: i could add support for this but i can't test it as i only have a single gpu. anyobe here with a multi-gpu setup that would like to have a go at this?,deployment,other
679,https://github.com/deezer/spleeter/issues/679,[Discussion] A concise script of Python for Spleeter,"i'm trying to make a concise and a short python script to run the spleeter project. my project requires the python code for spleeter to be easier to read and follow. everything seems to work well for ""2stems vocals"" except a small issue. the result of my script seems to add a small noise (i used audacity to visualize it). does anyone know what causes these types of noises? i can't wrap my head around it. i'm worried about the effect that they may cause on the audio results. here is my python code: any suggestions would be greatly appreciated!",question,other
607,https://github.com/mozilla/TTS/issues/607,Vocoder can't adapt to TTS Model,"hello, i tried to train taco2 and vocoder separately and i can't synthesize effective voices with the pre-trained vocoder. i'm very strange that use griffin lim, the synthesized voice seems to be pretty good. here synthesized voice with wavegrad and gl: here config.json of taco2: ` { ""model"": ""tacotron2"", ""rundescription"": ""tacotron2 with ddc and differential spectral loss."", // audio parameters ""audio"":{ // stft parameters ""fftlength"": 1024, // stft window length in ms. ""hoplengthlength' is used. ""framems"": null, // stft window hop-lengh in ms. if null, 'hoprate"": 22050, // dataset-related: wav sample-rate. ""preemphasis"": 0.0, // pre-emphasis to reduce spec noise and make it more structured. if 0.0, no -pre-emphasis. ""refdb"": 20, // reference level db, theoretically 20db is the sound of air. // silence trimming ""dosilence"": false,// enable trimming of slience of audio as you load it. ljspeech (true), tweb (false), nancy (true) ""trimlimmels"": 80, // size of the mel spec frame. ""melfmax"": 7600.0, // maximum freq level for mel-spec. tune for dataset!! ""specnorm"": true, // normalize spec values. mean-var normalization if 'statslevelnorm"": true, // move normalization to range [-1, 1] ""maxnorm, maxnorm] ""clippath"": null // do not use with multistatistics.py'. if it is defined, mean-std based notmalization is used and other normalization params are ignored }, // vocabulary parameters // if custom character set is not defined, // default set in symbols.py is used // ""characters"":{ // ""pad"": ""layers"": [], // give a list of layer names to restore from the given checkpoint. if not defined, it reloads all heuristically matching layers. // training ""batchtraining'. ""evalsize"":16, ""r"": 7, // number of decoder frames to predict per iteration. set the initial values if gradual training is enabled. ""gradualstep, r, batchsize' as you proceeed. ""mixedmasking"": true, // enable / disable loss masking against the sequence padding. ""decoderalpha"": 0.5, // original decoder loss weight. if > 0, it is enabled ""postnetalpha"": 0.25, // original postnet loss weight. if > 0, it is enabled ""postnetspecdiffalpha"": 0.25, // differential spectral loss weight. if > 0, it is enabled ""decoderalpha"": 0.5, // decoder ssim loss weight. if > 0, it is enabled ""postnetalpha"": 0.25, // postnet ssim loss weight. if > 0, it is enabled ""gaposeval"": true, ""testepochs"": 10, //until attention is aligned, testing only wastes computation time. ""testfile"": null, // set a file to load sentences to be used for testing. if it is null then we use default english sentences. // optimizer ""noamclip"": 1.0, // upper limit for gradients for clipping. ""epochs"": 100000, // total number of epochs to train. ""lr"": 0.0001, // initial learning rate. if noam decay is active, maximum learning rate. ""wd"": 0.000001, // weight decay weight. ""warmuplensize"": -1, // only tacotron - size of the memory queue used fro storing last decoder predictions for auto-regression. if wave file [path to wave] or // -> dictionary using the style tokens {'token1': 'value', 'token2': 'value'} example {""0"": 0.15, ""1"": 0.15, ""5"": -0.15} // with the dictionary being len(dict) 0 ""lrschedulerstep"": 50, // number of steps to log traning on console. ""printstep"": 5000, // number of training steps expected to plot training stats on tb and save model checkpoints. ""checkpoint"": true, // if true, it saves checkpoints per ""savemodelstats"": true, // true, plots param stats per layer on tensorboard. might be memory consuming, but good for debugging. // data loading ""numworkers"": 4, // number of training data loader processes. don't set it too big. 4-8 are good values. ""numloadersplitpath"": ""/data1/mozillawz/wavegrad_models/"" } `",deployment,question
3856,https://github.com/streamlit/streamlit/issues/3856,0.89 menu changes hides developer options when dev-ing on cloud IDEs,"hey folks, i'm doing my dev work on a project via gitpod, and so the development version of the app is, essentially, cloud hosted. nothing in the documentation shows me how to override the auto-hiding of the development options. i'm about to dig through your code to see if there's a way anyway, but i'd rather flag it now. thanks! edit: but the hotkeys for the menu items (e.g. hitting `c`) will still run the menu items even when not they're not visible. while handy, i'd prefer not to have my users be able to fat finger remove the state of cached items.",other,other
2243,https://github.com/streamlit/streamlit/issues/2243,Add a polyfill for Array.flatmap,"array.flatmap is not supported in certain browsers. find a polyfill for it so that we are compatible with more browser versions, particularly ie/edge",other,other
5321,https://github.com/iperov/DeepFaceLab/issues/5321," F tensorflow/core/common_runtime/dml/dml_upload_heap.cc:56] HRESULT failed with 0x887a0005: chunk->resource->Map(0, nullptr, &upload_heap_data)","expected behavior train model actual behavior when training model (both quick96 and saehd) the console will give me an error after training for some time (very random, sometimes it lets me train for hours, sometimes within minutes). this started happening a week after using the program and googling the error does not give me anything relevant ================== model summary =================== == == == model name: vassize: 4 == == == ==------------------ running on ------------------== == == == device index: 0 == == name: nvidia geforce gtx 1060 6gb == == vram: 5.16gb == == == ==================================================== starting. press ""enter"" to stop training and save model. [18:15:56][#002900][0305ms][0.5700][0.5533] [18:19:16][#004034][0189ms][0.5084][0.4724] 2021-04-30 18:20:51.793210: f tensorflow/core/commonuploadheap_data) press any key to continue . . . what could be the reason for this error? steps to reproduce just starting training and it will happen eventually other relevant information - ** windows 10",other,question
860,https://github.com/streamlit/streamlit/issues/860,Plotly chart flickers when hovering on it.,"summary when first loading a plotly chart, the chart flickerssteps to reproduce run the above script. hover with the mouse on the chart.expected behavior: nothing remarkable happensactual behavior: the chart momentarily disappears. see video.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: 0.51.0 - python version: 3.7 - using conda? pipenv? pyenv? pex? - os version: macos - browser version: chromeadditional information",other,Error
3600,https://github.com/streamlit/streamlit/issues/3600,st.select_slider resets its state upon interaction(v0.84),"summary this came up in a forum issue, but it seems like we've broken something in `st.select_slider`: to reproduce code snippet: ** moving the slider resets its widget state right away. i sometimes get this error message is this a regression? yes, verified that this works fine in versions prior to 0.84debug info - streamlit version: 0.84.0 - python version: 3.8.7",Error,Error
564,https://github.com/deepfakes/faceswap/issues/564,ValueError: No Graphics Card Detected! FAN is not currently supported on CPU. Use another aligner,"i'm sorry, but i don't know if the problem i encountered was bug or installation error. i installed all this in docker, and the software is up-to-date, including faceswap code, and cpu only, but when i entered the command ""python faceswap.py extract -i ./photo/trump -o ./photo/data/trump"" the following error occurred. 12/29/2018 12:53:48 info loading detect from mtcnn plugin... 12/29/2018 12:53:48 info loading align from fan plugin... 12/29/2018 12:53:48 warning no gpu detected. switching to cpu mode 12/29/2018 12:53:48 info starting, this may take a while... 12/29/2018 12:53:49 info initializing face alignment network... 12/29/2018 12:53:49 warning no gpu detected. switching to cpu mode 12/29/2018 12:53:49 error caught exception in child process: 6556 12/29/2018 12:53:49 info initializing mtcnn detector... 12/29/2018 12:53:50 warning no gpu detected. switching to cpu mode 2018-12-29 12:53:50.907418: i tensorflow/core/platform/cpuguard.cc:141] your cpu supports instructions that this tensorflow binary was not compiled to use: avx2 fma 12/29/2018 12:53:52 warning using cpu extracting faces: 0% 0/30 [00:00<?, ?it/s]12/29/2018 12:53:52 info initialized mtcnn detector. 12/29/2018 12:53:57 error got exception on main handler: valueerror: no graphics card detected! fan is not currently supported on cpu. use another aligner. i have a nvidia graphics card on my computer and it's mounted on docker, but the program doesn't work. i want to know what is "" another aligner"".",question,question
160,https://github.com/deezer/spleeter/issues/160,Please help with custom dataset,"hi, i am writing to get some help. i tryed to use `spleeter train` with with a dataset of electronic music samples, each one with a duration of 8 seconds. when i try to use the model obtained by train, to separate some electronic song, all outputs sound exactly the same. maybe it is due to the fact i used only 18 samples to feed the training? i launched it with command with this where i have csv files with the following content respectively and it produces a ** folder then i try to separate an audio file, for example and it outputs files and all sounds the same, it looks like it did not separated the audio. it just output 4 files with the same sounds as the original. i expected to separate input audio into 4 different files. any hint?",Error,question
233,https://github.com/deezer/spleeter/issues/233,1.49 release not downloading the new finetune training files,description step to reproduce 1. installed using `...` 2. run as `...` 3. got `...` error output environment ----------------- ------------------------------- os windows / linux / macos / other installation type conda / pip / other ram available xgo hardware spec gpu / cpu / etc ... additional context,other,other
746,https://github.com/microsoft/recommenders/issues/746,[BUG] in NNI integration test,description in which platform does it happen?,deployment,Error
5349,https://github.com/iperov/DeepFaceLab/issues/5349,Any way to acceralete inference of SAEHD tf-model?,"hello there, i'm working on lightweight-deepfake, i'm wondering how to optimize the pretrained saehd model aimed to lower inference time, ways including quantization onnx tensorrt etc. if u have any good idea or practice before , plz share in this issue.",question,question
1062,https://github.com/streamlit/streamlit/issues/1062,A st.group() or st.row() Container would be helpful.,"problem i want to define a group of empty elements that may or may not be populated later with content. for instance, i might want to define a summary card containing a header, some markdown, a plot and a table... depending on some interactions i want to add swap elements of this group of elements. currently, i have to define a custom container class and instantiate a bunch of st.empty() as parameters (that i later fill with content) for all possible elements which is clunky.solution add a st.group() container element",other,other
427,https://github.com/deepfakes/faceswap/issues/427,train.py doesn't stop after specified number of epochs,"first of all, a big thanks to all contributors for technical work and ethical standards. love the contributors' spirit! expected behavior i want to automate the whole pipeline of extraction, training, conversion and stitching back to a video. so each script needs to finish in a defined state. actual behavior this works almost perfectly with one exception: the train.py script doesn't stop automatically, it requires a manual input of ctrl+c. steps to reproduce this may be trivial but nevertheless: training: this doesn't stop automatically python3 faceswap.py train -a $inputdir -b $styledir -ep $numimageimagedir other relevant information - ** gpu",other,other
469,https://github.com/streamlit/streamlit/issues/469,Streamlit crashes with RuntimeError when using matplotlib with sliders,"summary i'm using matplotlib with a few sliders to plot some data. but after having changed the sliders some times streamlit crashes with the a runtimeerror i've experienced this in other applications as well.steps to reproduce run the below code and change the sliders until streamlit crashes with a runtimeerror expected behavior: no erroractual behavior: streamlit crashes with this error is this a regression? nodebug info - streamlit version: 0.47.4 - python version: 3.7.2 - not using conda, pipenv, pyenv or pex. i'm just using `python -m venv .venv` - os version: windows 8.1 - browser version: chrome - matplotlib. 3.1.1",Error,other
634,https://github.com/deezer/spleeter/issues/634,[Feature] Using provided instrumental for enhanced vocals separation,"description what about allowing user to optionally provide an instrumental track for spleeter to work on? i don't know if it's possible or not, but i think that it would better understand what is going vocals, and it doesn't need to separate accompaniment, because it's already provided. additional information currently adobe audition can do sort of the same thing (using sound model and sound remover effect), but it relies on calculations and some in-house algorithm. i think. if there is will be ai involved in process, then vocals separation could be much more accurate and spleeter won't confuse vocals for instruments.",other,other
2053,https://github.com/streamlit/streamlit/issues/2053,"[Layouts] Expander, make header togglable","in `st.expander`, the header currently has a cursor but is not togglable. only show/hide is currently togglable. make the entire header container togglable as well.",other,other
1279,https://github.com/streamlit/streamlit/issues/1279,Why does streamlit need to connect to a remote server?,"it's not clear to me why streamlit needs to connect to the streamlit server. i am following the azure deployment guide and i get to the issue where the app is on ""please wait..."" forever. on the top right it shows ""connecting to streamlit server"". i couldn't find any mention in the documentation or faq about why this needs to be the case, so i'm very confused. in case i missed something in the docs i apologise in advance, otherwise i'm happy to send a pr once i understand this myself.",question,other
24,https://github.com/deezer/spleeter/issues/24,"Install with standard Python, not conda","hi, i'm not a big fan of conda, would you have a install ""howto"" for standard python? thank you very much.",other,question
934,https://github.com/streamlit/streamlit/issues/934,Streamlit apps should use the Streamlit favicon!,"just using the same icon as the site would be fine. even snazzier might be to use the ""dark mode streamlit logo"" for apps to distinguish them from the website when they're both open.",other,other
264,https://github.com/iperov/DeepFaceLab/issues/264,No CUDA device found with minimum required compute capability 3.5,"i was using deepfacelab with cuda 10.1 version, but when i run ""4) data_src extract faces mt all gpu debug.bat"" to see whether my environment is ok or not. the message is shown on the screen. my graphic card is dual geforce 780m with a compute capability 3.0. then how could i change the minimum required compute capability to 3.0?",question,question
739,https://github.com/iperov/DeepFaceLab/issues/739,OOM error with xSeg training method,oom when allocating tensor with shape. in xseg train i can't reduce model parameters (resolution) that probably could solve that problem.,other,other
695,https://github.com/microsoft/recommenders/issues/695,[BUG] Remove contrib from azureml,"description the product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass in which platform does it happen? question to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?",question,other
1182,https://github.com/microsoft/recommenders/issues/1182,"[ASK]  how to create 3 files embedding.npy, uid2index.pkl and word_dict.pkl in notebook","description other comments i wanna ask how to create 3 files embedding.npy, uid2index.pkl and word_dict.pkl in notebook",question,question
275,https://github.com/mozilla/TTS/issues/275,preprocess.py can't load M-AILABS dataset,"hi, actually preprocess.py can't load metafiles values and running train.py gives me this error: valueerror: zero-size array to reduction operation maximum which has no identity csvfiles is none: to: if not meta_files: best regards massimo",question,Error
460,https://github.com/deezer/spleeter/issues/460,Support Python 3.8 with Pip,description allow installation of the pip package with python 3.8 (e.g. all latest ubuntu installations) additional information,other,other
206,https://github.com/streamlit/streamlit/issues/206,`ignore_cache` option of `st.cache` may be an old reference (error message),this error message shows up when the body a function contains some non-hashable piece of code. in this case a function. the message mention a `ignore_cache` option to `st.cache` that may have been renamed.,other,Error
430,https://github.com/iperov/DeepFaceLab/issues/430,How to feed FUNIT / TrueFace model?,should we supply the funit model with standard celeba / celebb facesets? it seems generalization is especially important for this gan model. should we stick to standard faceset approach or should the image selection be modified?,question,question
3280,https://github.com/streamlit/streamlit/issues/3280,"[FEATURE REQUEST] Research, Google Forms, CSV, Streamlit Application!","problem/context hello! for context and my dev experience, i'm new to using streamlit, and recently, i made as the solution to an assignment for my current course, ""artificial intelligence"". if you're interested in checking out the code, you can . other than that, i have used voila, matplotlib, seaborn, and pyplot for visualizations. recently, i've been working on a research paper, and for that, i had to develop a questionnaire using google forms. the research basically aims at gathering data about their opinions, beliefs, academic/situational/learning factors that have affected their semester progression throughout the university. i have gotten plenty of data from the university's students, which i believe is immensely interesting. now i wish that i can easily share their results with other students in my university to show the impact that online education has influenced learning capabilities. google forms does give you access to see all the responses and the fancy graphs they make, but you can't make that public. you can also download all of that data as a .csv file, and i immediately thought of streamlit, ""man, i wish i can just give this csv file to a streamlit application, tell it the format is from google forms, and then ask streamlit to generate graphs similar to the graphs that google forms generates, and then share a link with everyone - maybe even more kinds of graphs!"" unfortunately, i do not have enough time to write code from scratch for a streamlit application and put it into production right now, and i also do not know of other technologies which can potentially do this. maybe a potential solution is either powerbi or tableau, but i have no experience with them, and neither do i know of any easy way of putting an online web application that shows all these statistics. in my opinion, google forms itself has an option, but being frankly honest, i find google forms to be visually very unappealing. its ui looks outdated and makes filling forms a very boring experience. there is always `surveymonkey`, and `typeform`, but they aren't as free out of the box as google forms is. i don't think anyone sharing google form response results will appeal to anyone but a few executives or managers wanting to understand the data in whatever way.solution ** if you don't like the mvp above, tell us why, and what you'd like done instead. you need somewhere to deploy to show the results, which is not always possible. speaking for myself, i'm out of free heroku dynos. :/additional context not really, just had this idea. thank you!",other,other
584,https://github.com/mozilla/TTS/issues/584,RuntimeError: Error opening 'LJSpeech-1.1/wavs/\ufeffH_2.wav': System error.,i got this error when i tried to run compute_statistics.py how to fix this?,question,question
363,https://github.com/microsoft/recommenders/issues/363,NCF code and notebook,ncf implementation and notebook in tf. related to #339,other,other
1226,https://github.com/streamlit/streamlit/issues/1226,Bokeh Charts are not displayed,"bokeh charts are not displayed, not even the simple example from the documentation. the bokeh chart object is working, as can be seen using bokeh.plotting.show(). running on a linux ubuntu system. what are the steps we should take to reproduce the bug: load the example from ony my system an empty website is opened and nothing else happensdebug info - streamlit version: 0.56.0 - python version: 3.6.9 - using pipenv - os version: linux ubuntu - browser version: mozilla firefox 74.0 (linux) microsoft edge 44.18362.449.0 (windows) mozilla firefox 60.7.2esr (windows)",Error,Error
260,https://github.com/deezer/spleeter/issues/260,[Discussion] How to separate multiple music files with one call function of separate on the terminal?,"hi, this the quick start method to separate the music files to vocals and accompaniment, `git clone `conda install -c conda-forge spleeter` `spleeter separate -i spleeter/audio_example.mp3 -p spleeter:2stems -o output` how to call this separate function once for multiple mp3 files(100+) in a folder so that i do not need to convert one by one manually?",question,question
505,https://github.com/mozilla/TTS/issues/505,Web server giving empty audio for most of unseen input text,"i have developed tts model on some custom dataset with 100k epochs trained model. after everything i executed `python -m tts.server.server` and opened the url, provided few input texts. for some inputs it is giving a valid audio output but for many inputs **"" with just addition of question mark, it is generating audio of 7 seconds long. i am using griffin lim as default while synthesizing audio.",question,Performance
669,https://github.com/microsoft/recommenders/issues/669,[FEATURE] Update to Databricks 5.0 in setup,"description we require databricks 4.3 / spark 2.3 in our setup / o16n notebooks due to cosmosdb not supporting spark 2.4. cosmosdb now supports spark 2.4, so we should recommend the latest / default databricks env. other comments",other,other
195,https://github.com/microsoft/recommenders/issues/195,Track github statistics of Recommenders repo,"basically, the idea is to use all the metrics developed to track and save them to a cosmosdb. we can set up a cron task for executing this every day",other,other
218,https://github.com/mozilla/TTS/issues/218,Strange EvalAudios and TrainAudios as well as test results,"hello to everybody. really enjoying the simplicity in use of the tts and promising results with other languages. i am doing now training on 11 hours 1 speaker russian dataset and the training has already reached 40000 global steps. it was expected (by me) that by that stage the tts will be able to produce somewhat speech-like or at least something from what could be said - it is training well. but what i have instead is some strange noise while testing, which changes with different text provided, but seems like changes randomly. what is more interesting, checking on the tensorboard on-fly samples makes me confused. evalaudios sometimes are ok, but sometimes have some non-speech noises, a lot of them. if check the trainaudios - they are usually ends with a terrible screamer-like loud noises which take about half durations. it seems like on their spot must be a silence. if the tts is learning on those files, obviously, it can hardly learn. the dataset has various samples with duration from 0.5 to 38.5 second with the majority on around 5 secs. here are some pics, one of the trianaudios wavs and config.json. any suggestions are really appreciated. p.s. during the training and inference on the cmd ""decoder stopped with 'maxsteps"" is outputted ----- { ""runmasking"", ""runmels"": 80, // size of the mel spec frame. ""numrate"": 22050, // dataset-related: wav sample-rate. if different than the original data, it is resampled. ""framems"": 50, // stft window length in ms. ""framems"": 12.5, // stft window hop-lengh in ms. ""preemphasis"": 0.97, // pre-emphasis to reduce spec noise and make it more structured. if 0.0, no -pre-emphasis. ""mindb"": -100, // normalization range ""refdb"": 20, // reference level db, theoretically 20db is the sound of air. ""power"": 1.1, // value to sharpen wav signals after gl algorithm. ""griffiniters"": 60,// #griffin-lim iterations. 30-60 is a good range. larger the value, slower the generation. // normalization parameters ""signalnorm"": false, // move normalization to range [-1, 1] ""maxnorm, maxnorm] ""clipfmin"": 0.0, // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. tune for dataset!! ""meltrimlayers"": [], ""model"": ""tacotron2"", // one of the model in models/ ""graddecay"": false, // if true, noam learning rate decaying is applied through training. ""warmupsize"": 5, // only tacotron - memory queue size used to queue network predictions to feed autoregressive connection. useful if r < 5. ""attentiontype"": ""original"", // only tacotron2 - ""original"" or ""bn"". ""prenetforwardagent"": false, // only tacotron2 - enable/disable transition agent of forward attention. ""locationdisable location sensitive attention. it is enabled for tacotron by default. ""losseoschars"": false, // enable/disable beginning of sentence and end of sentence chars. ""stopnet"": true, // train stopnet predicting the end of synthesis. ""separatemodelstats"": false, // true, plots param stats per layer on tensorboard. might be memory consuming, but good for debugging. ""batchbatchstep"" ""savestep"": 10, // number of steps to log traning on console. ""batchsize"": 0, //number of batches to shuffle after bucketing. ""rundelaysentencespath"": ""/ssd/ttsfiletrain.txt"", // dataset-related: metafile for training dataloader. ""metaval"": ""ttscache"" for pre-computed dataset by extractseqseqpath"": ""../keep/"", // dataset-related: output path for all training outputs. ""numworkers"": 8, // number of training data loader processes. don't set it too big. 4-8 are good values. ""numloadercachephonemes"", // phoneme computation is slow, therefore, it caches results in the given folder. ""uselanguage"": ""ru"", // depending on your target language, pick one from ""textcleaners""",question,Performance
645,https://github.com/deepfakes/faceswap/issues/645,faceswap gui crashes after building v0.95b over macOS,"after successful installation of packages, faceswap seems to be crashing over macos, when `python3 faceswap.py gui` is executed. please find logs below: macos crashlog below: ```terminal process: python [18420] path: /library/frameworks/python.framework/versions/3.6/resources/python.app/contents/macos/python identifier: python version: 3.6.8 (3.6.8) code type: x86-64 (native) parent process: bash [18397] responsible: python [18420] user id: 501 date/time: 2019-03-04 01:03:42.312 +0530 os version: mac os x 10.14.3 (18d109) report version: 12 bridge os version: 3.0 (14y674) anonymous uuid: 9d1cf699-0c2b-def6-57cd-d39257512d21 sleep/wake uuid: e2558b48-157a-445d-a20d-4b637a975b10 time awake since boot: 25000 seconds time since wake: 11000 seconds system integrity protection: enabled crashed thread: 0 dispatch queue: com.apple.main-thread exception type: exccorpsekernel.dylib 0x00007fff6382523e _kill + 10 1 libsystemkill + 285 2 libsystempywrapinternal.so 0x0000000117519dd0 tensorflow::internal::logmessagefatal::~logmessagefatal() + 32 4 tensorflowpywrapinternal.so 0x00000001138cf0e9 tensorflow::bfloat16pytype() + 105 6 tensorflowwrapbfloat16object) + 44 7 org.python.python 0x000000010092c7cb fastcalldict + 491 8 org.python.python 0x00000001009ae747 callpyevalpyevalevalcode + 100 12 org.python.python 0x00000001009a18c4 builtincall + 258 14 org.python.python 0x00000001009ab30a evalframedefault + 28666 15 org.python.python 0x00000001009af1af evalcodewithname + 2447 16 org.python.python 0x00000001009afae1 fastfunction + 401 18 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 19 org.python.python 0x00000001009afa3d fastfunction + 401 21 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 22 org.python.python 0x00000001009afa3d fastfunction + 401 24 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 25 org.python.python 0x00000001009afa3d fastfunction + 401 27 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 28 org.python.python 0x00000001009afc5c fastcalldict + 348 29 org.python.python 0x00000001008e36b7 fastcalldict + 247 30 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 31 org.python.python 0x00000001009cc13b pyimport_call + 98 34 org.python.python 0x00000001009ab30a evalframedefault + 28666 35 org.python.python 0x00000001009af1af evalcodewithname + 2447 36 org.python.python 0x00000001009afae1 fastfunction + 401 38 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 39 org.python.python 0x00000001009af1af evalcodewithname + 2447 40 org.python.python 0x00000001009afdfb fastcalldict + 763 41 org.python.python 0x00000001008e36b7 fastcalldict + 247 42 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 43 org.python.python 0x00000001009cc3ea pyimportpyevalpyevalevalcode + 100 47 org.python.python 0x00000001009a18c4 builtincall + 258 49 org.python.python 0x00000001009ab30a evalframedefault + 28666 50 org.python.python 0x00000001009af1af evalcodewithname + 2447 51 org.python.python 0x00000001009afae1 fastfunction + 401 53 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 54 org.python.python 0x00000001009afa3d fastfunction + 401 56 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 57 org.python.python 0x00000001009afa3d fastfunction + 401 59 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 60 org.python.python 0x00000001009afa3d fastfunction + 401 62 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 63 org.python.python 0x00000001009afc5c fastcalldict + 348 64 org.python.python 0x00000001008e36b7 fastcalldict + 247 65 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 66 org.python.python 0x00000001009cc13b pyimportpyevalpyevalevalcode + 100 70 org.python.python 0x00000001009a18c4 builtincall + 258 72 org.python.python 0x00000001009ab30a evalframedefault + 28666 73 org.python.python 0x00000001009af1af evalcodewithname + 2447 74 org.python.python 0x00000001009afae1 fastfunction + 401 76 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 77 org.python.python 0x00000001009afa3d fastfunction + 401 79 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 80 org.python.python 0x00000001009afa3d fastfunction + 401 82 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 83 org.python.python 0x00000001009afa3d fastfunction + 401 85 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 86 org.python.python 0x00000001009afc5c fastcalldict + 348 87 org.python.python 0x00000001008e36b7 fastcalldict + 247 88 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 89 org.python.python 0x00000001009cc13b pyimportpyevalpyevalevalcode + 100 93 org.python.python 0x00000001009a18c4 builtincall + 258 95 org.python.python 0x00000001009ab30a evalframedefault + 28666 96 org.python.python 0x00000001009af1af evalcodewithname + 2447 97 org.python.python 0x00000001009afae1 fastfunction + 401 99 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 100 org.python.python 0x00000001009afa3d fastfunction + 401 102 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 103 org.python.python 0x00000001009afa3d fastfunction + 401 105 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 106 org.python.python 0x00000001009afa3d fastfunction + 401 108 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 109 org.python.python 0x00000001009afc5c fastcalldict + 348 110 org.python.python 0x00000001008e36b7 fastcalldict + 247 111 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 112 org.python.python 0x00000001009cc13b pyimportpyevalpyevalevalcode + 100 116 org.python.python 0x00000001009a18c4 builtincall + 258 118 org.python.python 0x00000001009ab30a evalframedefault + 28666 119 org.python.python 0x00000001009af1af evalcodewithname + 2447 120 org.python.python 0x00000001009afae1 fastfunction + 401 122 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 123 org.python.python 0x00000001009afa3d fastfunction + 401 125 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 126 org.python.python 0x00000001009afa3d fastfunction + 401 128 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 129 org.python.python 0x00000001009afa3d fastfunction + 401 131 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 132 org.python.python 0x00000001009afc5c fastcalldict + 348 133 org.python.python 0x00000001008e36b7 fastcalldict + 247 134 org.python.python 0x00000001008e46d8 callmethodidobjargs + 520 135 org.python.python 0x00000001009cc13b pyimportpyevalfunction + 381 138 org.python.python 0x00000001009ae721 callpyevalfunction + 381 141 org.python.python 0x00000001009ae721 callpyevalfunction + 381 144 org.python.python 0x00000001009ae721 callpyevalpyevalevalcode + 100 148 org.python.python 0x00000001009da381 pyrunsimplefileexflags + 882 150 org.python.python 0x00000001009f3b12 pykernel.dylib 0x00007fff638227de _cvwait + 10 1 libsystempthreadwait + 724 2 libopenblasp-r0.3.0.dev.dylib 0x00000001081ea72b blasserver + 187 3 libsystempthreadpthread.dylib 0x00007fff638dc26f start + 70 5 libsystemstart + 13 thread 2: 0 libsystemcvwait + 10 1 libsystempthreadwait + 724 2 libopenblasp-r0.3.0.dev.dylib 0x00000001081ea72b blasserver + 187 3 libsystempthreadpthread.dylib 0x00007fff638dc26f start + 70 5 libsystemstart + 13 thread 3: 0 libsystemcvwait + 10 1 libsystempthreadwait + 724 2 libopenblasp-r0.3.0.dev.dylib 0x00000001081ea72b blasserver + 187 3 libsystempthreadpthread.dylib 0x00007fff638dc26f start + 70 5 libsystemstart + 13 thread 4: 0 libsystempyread + 121 3 org.python.python 0x000000010092c7cb fastcalldict + 491 4 org.python.python 0x00000001009ae747 callpyevalpyevalfunction + 545 8 org.python.python 0x00000001009ae721 callpyevalpyevalfunction + 545 12 org.python.python 0x00000001009ae721 callpyevalpyevalfunction + 545 16 org.python.python 0x00000001009ae721 callpyevalpyevalfunction + 545 20 org.python.python 0x00000001009ae721 callpyevalfunction + 381 23 org.python.python 0x00000001009ae721 callpyevalpyfunctionpyobjectpyobjectprepend + 149 28 org.python.python 0x00000001008e34f0 pyobjectpyevalfunction + 381 31 org.python.python 0x00000001009ae721 callpyevalfunction + 381 34 org.python.python 0x00000001009ae721 callpyevalpyfunctionpyobjectpyobjectprepend + 149 39 org.python.python 0x00000001008e34f0 pyobjectbootstrap + 70 41 org.python.python 0x00000001009efbe9 pythreadpthread.dylib 0x00007fff638d9305 body + 126 43 libsystempthreadpthread.dylib 0x00007fff638d8415 threadkernel.dylib 0x00007fff638227de _cvwait + 10 1 libsystempthreadwait + 724 2 org.python.python 0x00000001009effad pythreadlocktimed + 111 4 org.python.python 0x00000001009f6b40 lockacquirepycfunctionfunction + 439 7 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 8 org.python.python 0x00000001009af1af evalcodewithname + 2447 9 org.python.python 0x00000001009afae1 fastfunction + 401 11 org.python.python 0x00000001009aaeb7 evalframedefault + 27559 12 org.python.python 0x00000001009af1af evalcodewithname + 2447 13 org.python.python 0x00000001009a42bb pyevalcall + 381 15 org.python.python 0x00000001008e34f0 pyobjectpyevalfunction + 381 18 org.python.python 0x00000001009ae721 callpyevalfunction + 381 21 org.python.python 0x00000001009ae721 callpyevalpyfunctionpyobjectpyobjectprepend + 149 26 org.python.python 0x00000001008e34f0 pyobjectbootstrap + 70 28 org.python.python 0x00000001009efbe9 pythreadpthread.dylib 0x00007fff638d9305 body + 126 30 libsystempthreadpthread.dylib 0x00007fff638d8415 thread_start + 13 thread 0 crashed with x86 thread state (64-bit): rax: 0x0000000000000000 rbx: 0x00000001044f65c0 rcx: 0x00007ffeef32be58 rdx: 0x0000000000000000 rdi: 0x0000000000000307 rsi: 0x0000000000000006 rbp: 0x00007ffeef32be90 rsp: 0x00007ffeef32be58 r8: 0x0000000000000004 r9: 0x0000000000000000 r10: 0x0000000000000000 r11: 0x0000000000000206 r12: 0x0000000000000307 r13: 0x0000000100be6048 r14: 0x0000000000000006 r15: 0x000000000000002d rip: 0x00007fff6382523e rfl: 0x0000000000000206 cr2: 0x00007fff964bc188 error code: 0x02000148 trap number: 133",question,deployment
492,https://github.com/iperov/DeepFaceLab/issues/492, Face Super resolution tools development request ...,hi:iperov /team,other,other
45,https://github.com/deezer/spleeter/issues/45,How to build docker image,"could you give some guidance in the readme how to build the docker images? thanks a lot, amazing project!",question,question
643,https://github.com/iperov/DeepFaceLab/issues/643,Extractor is running on CPU even though it says it's using the GPU,this is not tech support for newbie fakers post only issues related to bugs or code expected behavior for the extractor to run on the gpu not the cpu. actual behavior extractor says it's running on the gpu but it's on the cpu. steps to reproduce when choosing gpu: and when choosing cpu: other relevant information - ** python 3.7.5,other,other
222,https://github.com/streamlit/streamlit/issues/222,DataFrame demo: Streamlit failed to hash an object of type <class 'code'>,"summary running the dataframe demo on a fresh install results in `streamlit failed to hash an object of type `steps to reproduce what are the steps we should take to reproduce the bug: 1. $ `pip install streamlit` 2. $ `streamlit hello` 3. in browser select `dataframe demo`expected behavior: see the demoactual behavior: stacktrace shown in browser app debug info - streamlit version: `streamlit, version 0.47.1` - python version: `python 3.7.3` - using conda - os version: macos 10.14.5 - browser version: chrome 77.0.3865.90",Error,Error
3113,https://github.com/streamlit/streamlit/issues/3113,"UnhashableTypeError: Cannot hash object of type torch.nn.parameter.Parameter, found in the body of get_most_likely().","recently, when i loaded an nlp pre-training model, i got this error, which did not happen before, and i don't know what caused it.",question,question
681,https://github.com/streamlit/streamlit/issues/681,Remove mapbox token from source code,summary decision: let's just put it in a json file in s3 and load it when needed. because this allows us to rotate the token if it comes to that,other,other
1580,https://github.com/streamlit/streamlit/issues/1580,st.video() not working with a .mp4 file,"summary i have trouble running st.video() on videos that run fine with vlc.details i have a very simple app.py in the main of which i try to open a video : i generated the video twice, using a a list of numpy arrays, each one corresponding to a frame: once by using the open cv videowriter function : this produces a file which i can select, and which i can also play in vlc mediaplayer but for which streamlit says : **reproduce bug i am running the streamlit app on a distant machine to which i am connected on my work's vpn and forwarding the app onto a port on my laptop. however i have determined that this is not the cause of the problem by reproducing this error on my local machine with the following : you just need a folder : -theapp.py -media --footage ---thelike=debian prettyid=""18.04"" homeurl="" bugurl="" privacyurl="" versioncodename=bionic ' (same os my laptop and the distant machine) - browser version: firefox 77.0.1",question,question
732,https://github.com/streamlit/streamlit/issues/732,Is there a way to clear the cache from python script ?,"i would like to create a button to manually free the cache, e.g. at the end of processing all frames. this option will help me to create a workaround to the conflict that occurs when caching cap object (from cv.videocapture) and cap.release()",other,other
547,https://github.com/iperov/DeepFaceLab/issues/547,Exact params for training command? (res/opt/dims/batchsize),"hello, i am trying to find the params for these, the saehd model specifically, so that i can bypass having to skip/enter each one sequentially once the initial command is run. i was able to find the commands for fps/bitrate on converting, but that is ffmpeg mainly. i am assuming that for the training params, i will need to have some sort of python function to pipe them with the command?? i don't know, i didn't find much while searching through issues, and when looking at the code, i see references to all of these, but i am not sure what the correct procedure is for piping them in initially. thank you.",question,question
434,https://github.com/microsoft/recommenders/issues/434,[BUG] check smoke and integration tests for SAR and fastai,is affected by this bug? sar and fastai nightly builds are failing in platform does it happen? azure data science virtual machine.for example: the tests should pass successfully.,deployment,other
801,https://github.com/iperov/DeepFaceLab/issues/801,5.2 data_dst sort option 9 - revert original filenames not working on 06_22_2020 build,"this is not tech support for newbie fakers post only issues related to bugs or code expected behavior - reverts alligned filenames to original filenames after sorting by similarity (option 4) and deleting faces that are misalligned or zoomed wrong, i need to revert to original filenames so that i can do a manual extract on frames. actual behavior after loading samples, screen flashes, and filenames stay the same steps to reproduce run 5.2 data_dst sort. choose option 4. after it is complete,, run it again and choose option 9. filenames will not revert. have tried on multiple videos other relevant information - ** 3.5, 3.6.4, ... (if you are not using prebuilt windows binary)using prebuilt windows binary",question,question
450,https://github.com/microsoft/recommenders/issues/450,Plug AzureML for testing pipeline in Linux,"description right now, we have a gpu vm that runs all the time and it has an agent where we run the smoke and integration tests. if we create an scenario where from a master machine we start a gpu machine, run the gpu tests, report the logs and then kill the machine (we can do the same with databricks). we could do the tests in parallel. the idea is to still run the orchestration on a worker from a pool, but then the worker would spin up the compute in the aml cluster, run the notebook there and then capture the outputs/results. the worker in the pool would of course be a cheap cpu machine in platform does it happen? azure data science virtual machine. other platforms.*",other,other
460,https://github.com/streamlit/streamlit/issues/460,vegalite chart height ignored in 0.48.1,"summary before 0.48.1, when i pass a vegalite ""height"" parameter in the vegachart() spec it works correctly. after 0.48.1, this height parameter somehow doesn't work anymore; i don't even see if when i open it up with the vega editor. the ""width"" parameter still works.steps to reproduce what are the steps we should take to reproduce the bug: try: expected behavior: the height of the chart should be set according to the ""height"" in the spec.actual behavior: ""height"" in the spec is ignored.is this a regression? that is, did this use to work the way you expected in the past? yesdebug info - streamlit version: streamlit, version 0.48.1 - python version: python 3.6.8 - using conda? pipenv? pyenv? pex? conda - os version: ubuntu 18.04 - browser version: chrome version 77.0.3865.120 (official build) (64-bit)",Error,Error
2202,https://github.com/streamlit/streamlit/issues/2202,Wrong values passed to variables from streamlit,"problem while using streamlit i've noticed that some values of checkboxes or selectboxes can be different to what is displayed to the user. it is hard to point out when this happens, the behaviour is quite random. all the keys are unique across the application.example",other,Error
667,https://github.com/deepfakes/faceswap/issues/667,ImportError: numpy.core.multiarray failed to import,** - device: pc - os: win 10,question,question
443,https://github.com/iperov/DeepFaceLab/issues/443,Converting: Incrase for Multi CPU?,"hi if i start converting, there is the output of countig cores and startin tensor backend. on my 16 core, i counts only to six? does that be a bug? or tecnical reason? or can i increase that? normaly i get only 2/2.4 it/s. maybe of this message for using only 6 cores? if i sort by blur, it counts up to 32 (16 smt cores). how can i change that for testing if it brings up some performance? greets and thanks for answering :)",question,question
911,https://github.com/deepfakes/faceswap/issues/911,"Attribute Error when doing alignment manual, extract, merge, anything with alignment. object has no attribute 'copy'. Sometimes 'batchsize.","every time i try to sort or merge or do something with alignments, it fails, i looked up online, and it seems that i have to change or downgrade my keras version, i'm pretty familiar with codes. i don't code at all, but i can copy and paste a command line. for the life of me, i spent a few hours to figure out that ""python"" is a different terminal than windows' own cmd. i tried many codes, nothing worked. i installed faceswap through the installer, and it seems something didn't install right .. but i don't even know how would i try to fix it. my hardware is amd ryzen cpu, and amd vega 56 gpu",Error,question
2566,https://github.com/streamlit/streamlit/issues/2566,Ctrl /cmd + c triggers clear cache,"summary if you try to copy text from an app it'll trigger the clear cache modal. it's fine when you copy text from an input field. this is very likely a regression introduced on 12/28, likely from #2525",Error,other
54,https://github.com/deezer/spleeter/issues/54,[HELP Please!]Successfully installed but can't be used,"i used pip to install `pip install spleeter `pip install spleeter -i and i have to use the doubanio mirror because i am in china but i have successfully installed spleeter: however! i cannot use it. it always tells me 'system cant find this file '. i am pretty sure that the file path is correct. after many trials, i still don't know why. can anybody give me some suggesstions please? thanks ""系统找不到指定的文件""means that 'errornot_found system can't find the file specificed.'",question,question
3534,https://github.com/streamlit/streamlit/issues/3534,Changing default overrides incoming user selected value,"summary as outlined in when a new initial value is passed into a widget, it overwrites any incoming user selected value from the widget - see the example in that comment.steps to reproduce run this code snippet and toggle back and forth between the options. after the first click, when a new option is selected the radio will flip back to the previously selected option. this is due to passing in a new default value from the query string. code snippet: ** when the ""state"" is pulled out of the url and passed to the controls, each render, the url state is ""one behind"" the user selected value and then is overridden by code which was intended to make sure controls are updated if the default is changed. this was introduced by is this a regression? i don't think this is a regression - both sessionstate and query string parameters are fairly new features.debug info - streamlit version: 0.84.0 - python version: 3.8.2 - pyenv/pipenv - os version: manjaro/macosx - browser version: chromium/chromeadditional information i discovered this bug while building a feature in an app i was working on where i store the app state in redis, and then put a key=$hash into the query string and noticed that the controls behaved oddly when toggling. i used the workaround in the above linked pr (described by @karriebear) of setting session state variables the first time url parameters are extracted.",Error,Error
709,https://github.com/streamlit/streamlit/issues/709,Thoughts on Compatibility with Nteract's VDOM,"nteract has developed a specification for displaying arbitrary html in jupyter notebooks: - are there existing ways to accomplish this, or would it be useful to integrate nteract's vdom specification into streamlit? i've actually taken this vdom specification and written a server-side rendering library around it: - repo: - docs: my server-side rendering library takes a very different, and functional approach, to creating interactive applications in python, but maybe there's some knowledge to be gained via comparison.",other,other
3673,https://github.com/streamlit/streamlit/issues/3673,streamlit: command not found in CentOS7,"summary type here a clear and concise description of the bug. aim for 2-3 sentences. after successfully installing streamlit via pip on centos7, and i can see the installation package on the pip list. however, using streamlit hello in bash shows `streamlit: command not found centos`. but it is perfect that i can repeat the step in windows 10.steps to reproduce python3.6 centos7 code snippet: ** streamlit: command not found centosis this a regression? that is, did this use to work the way you expected in the past? nodebug info - streamlit version: 0.86.0 - python version: 3.6 - using pip - os version: centos7 - browser version: firefox version 78.12.0esr (64-bit)additional information path: pip list:",question,other
3502,https://github.com/streamlit/streamlit/issues/3502,Mysterious Blue Highlight Input Text Field in Selectbox,hello! i have observed some blue highlights text fields as shown in the image. this comes with the use of `st.selectbox()` on streamlit with the following versions: - python v3.8 - streamlit v0.83 **,Error,other
330,https://github.com/mozilla/TTS/issues/330,Demo server.py : GPU usage for _griffin_lim() function,"hi, while experimenting with server.py script, we have realized that lim() function does not utilize gpu's because librosa stft() and istft() methods don't run on gpu. is there a separate branch or modification addressing this issue? thanks.",question,other
678,https://github.com/iperov/DeepFaceLab/issues/678,Manual extractor - accuracy mode,"expected behavior speedup when pressing a in manual extractor. actual behavior seems to be very slow and laggy, even if a is pressed. this previously used to make a huge difference. steps to reproduce run manual extractor and press ""a"" for accuracy mode",Performance,other
2964,https://github.com/streamlit/streamlit/issues/2964,"Cached themes may be missing newly added derived colors, causing weird behavior","this is an issue that can only affect people that have played with a pre-release/early version of theming (so, essentially just internal streamlit developers) for now, but we'll need a proper solution to this issue before we make any additions to colors calculated in . we store themes in `localstorage` to 1. save user preferences 2. prevent the theme from rapidly flashing our light/dark theme before a custom theme is loaded after the first app load. these cached themes may be missing newly added derived colors, which means that the new colors will be undefined when we try using them with styled components. a user can fix this by changing their active theme to something else then changing it back, but of course this isn't great. one way to fix this would simply be to recompute a theme's derived colors whenever the theme changes to make sure we can't miss any new ones.",Error,other
47,https://github.com/microsoft/recommenders/issues/47,Mismatch between Spark and Python evaluation,evlauation metrics between from two implementations are not matched.,Error,Error
121,https://github.com/iperov/DeepFaceLab/issues/121,A problem with GTX 1050 Ti support?,"hi, i'm quite new into deepfakes so let me describe a problem i've encountered. it looks as follows: `... ` so i can at least run it. in other words it works, but very inefficiently due to engagement of cpu only. here is my question, is there anything that can be done to support gtx1050 ti? any help would be appreciated. thanks in advance.",question,question
268,https://github.com/mozilla/TTS/issues/268,Broken install on dev,the `phonemizer` from bootphon doesn't install from the dev `setup.py` script:,question,deployment
2969,https://github.com/streamlit/streamlit/issues/2969,"Missing ""import urllib"" in ""streamlit hello"" mapping/dataframe demo code","the mapping and dataframe demo (`streamlit hello`, select mapping option on left hand size, have ""show code"" checked) seems to be missing ""import urllib"" in the code section below the live demo; the code uses `except urllib.error.urlerror as e:` but urllib is never imported; copying and pasting the code into an app does show the import error. tested on streamlit 0.78.0, python 3.8. edit 1: make it clearer edit 2: just realized the same thing happens for the dataframe demo, edited.",Error,Error
1048,https://github.com/streamlit/streamlit/issues/1048,[Docs] Update the st.progress doc example,** sleep for 0.1 second for progress bar 1%. so user can see the progress bar animation,other,other
1745,https://github.com/streamlit/streamlit/issues/1745,React side of custom component does not receive default value,"summary when i build a custom component i can specify a default value in the python code. i need to specify a default value in the react component again, and they don't need to have the same value. this can/will lead to unexpected behaviour when they are not aligned.steps to reproduce 1. check out the example custom component 2. change `componentcomponentvalue = func(name=name, key=key, default=5)` 3. click the buttonexpected behavior: i expect to be able to set the initial value in the react componentactual behavior: there is no alignment possible between python part of the component and the react side. only way to solve this is to hard code it at two places.is this a regression? nodebug info - streamlit version: 0.63.0 - python version: 3.7.6 - using conda? pip - os version: osx - browser version: chrome 83.0additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",Error,Error
265,https://github.com/mozilla/TTS/issues/265,TTS + MelNet Can we try?,awesome. voice more realistic. i want more interesting to implement this. please anyone can initiate me. where i need to start process? thank you all,other,other
2266,https://github.com/streamlit/streamlit/issues/2266,file_uploader broken after upgrading streamlit,"summary - upgraded streamlit fileuploader broke our program. on first run through it lets you select the file and works fine. when it runs through a second time it seems to not know what file was uploaded before and throws errors and none of subsequent logic after the filedata(), streamlit encountered an object of type streamlit.uploadedmanager.uploadedfile, which it does not know how to hash."" here is the code we used following the upgrade, based on streamlit site recommendation: data = st.filestdata(data): st.write(""loadcsv(data, sep=',', encoding='utf-8') if data: df = loaduploader upgrade yesterday. the file64) -running on heroku with postgres",question,other
2280,https://github.com/streamlit/streamlit/issues/2280,object string shows when using the new beta_columns for styling,summary `beta_columns` shows the object string of `st.dataframe` when pandas styling is usedsteps to reproduce expected behavior: just show the dataframeactual behavior: shows the object string like the following: is this a regression? nodebug info - streamlit version: 0.69.2 - python version: 3.83 - conda - os version: windows 10 home (1909) - browser version: chrome version 86.0.4240.75,question,Error
322,https://github.com/streamlit/streamlit/issues/322,Does not do anything,"i don't know if i am getting something wrong, but when i execute the demo code of uber data visualization nothing happened, i expected it to open a browser window as it said in the documentation thank you",other,question
1616,https://github.com/streamlit/streamlit/issues/1616,Regression: datepicker is broken when date is Epoch,"the first date input in `e2e/scripts/st_sidebar.py` is broken, but our screenshot tests didn't catch it since it's below the diff threshold :cry: here's what it looks like right now notice that the date picker is disabled, for some reason! here's what it's supposed to look like this is the code in question info streamlit version: 0.62.0",Error,Error
173,https://github.com/deezer/spleeter/issues/173,[Discussion] I tried this mp3，and always be failed,pls unzip it and get a mp3. i do not know why this song can not be done. pls help me.,question,other
102,https://github.com/deezer/spleeter/issues/102,[Discussion] Can't install Spleeter using pip in MacOS 10.13.6,"when i try to install spleeter using `pip install spleeter`, it prints out:",other,deployment
631,https://github.com/iperov/DeepFaceLab/issues/631,(suggestion): export multiple masks and coordinates information ,"i wonder if it's possible to export multiple masks that only include eyes, mouth and nose. for external software purposes. plus some ascii coordinates that track eyes position , nose and mouth. this way it can be much easier to adjust in post",other,other
415,https://github.com/deepfakes/faceswap/issues/415,Conversion tool request,"it would be useful to have a conversion tool that could preview a single video frame, while adjusting blur size and kernel size as sliders in real-time or by a mouse click. whatever the case, it would save hours of guesswork of finding the perfect conversion settings. thanks.",other,other
30,https://github.com/deezer/spleeter/issues/30,No output or error log for most song-length input on Ubuntu-for-Windows (WSL),"description i have tested about 15 different songs, and only 3 of them produced any output at all. my environment is a little unusual, i'm using spleeter-cpu on wsl (ubuntu-for-windows). i get the ""loading audio from 0.0 to 600.0"" message and then it grinds for a few minutes consuming all cpu, and then just exits. ** sometimes only one of the models will produce any output for a given input file. except for audiolengthfile.mp3 -p spleeter:2stems -o audio_output/ 2) notice there is no output or log output none that is relevant to the issue. if the sub-process is crashing or being killed, it seems like the number one priority to avoid user confusion would be for some part of the stack to generate an error log. environment ----------------- ------------------------------- os windows / linux installation type conda ram available 24gb ddr3 hardware spec i7-2600k",other,other
53,https://github.com/streamlit/streamlit/issues/53,Persistent bug with caching,"summary this bug happens every time i run this `uber.py` script. i think it has to do with caching.steps to reproduce run this `uber.py` file: actual behavior: in `v0.45` you will get this: note if you hit `r` to rerun, you will see the right answer.expected behavior: it should run properly the first time.is this a regression? yesdebug info",Error,Error
151,https://github.com/iperov/DeepFaceLab/issues/151,png-export with alpha doesn't work,"expected behavior converter should be able to export final images with alpha channel if option ""export png with alpha"" is set to yes. actual behavior converter exports sequence of .jpgs without alpha channel. (.jpg can't contain an alpha channel by specification). steps to reproduce 1. train model (sae in this case). 2. run converter with default values, set ""export png with alpha"" to ""y"". 3. .jpg sequence is exported. other relevant information - happens with version 642b88a8d348790405a039b7f2347a13b9c384c6.",Error,other
953,https://github.com/microsoft/recommenders/issues/953,[FEATURE] TensorFlow 2.0,description tf2.0 has released. it has some breaking changes from v1.xx. i'm testing the new api and updating tfv2_behavior()`.,other,other
893,https://github.com/deepfakes/faceswap/issues/893,Face Reenactment model? (Similar to AVATAR by DFL),** avatar by dfl,other,other
576,https://github.com/mozilla/TTS/issues/576,CUDA out of memory,how to fix it? i tried to reduce batch size but it does not work :(,question,question
706,https://github.com/mozilla/TTS/issues/706,RuntimeError: CUDA error: unspecified launch failure,"hi everybody: i am training mozillatts with spanish corpus (i am training on windows 10) and i got this error message. i don't know if the error is because of gpu memory. thanks a lot for your help. tracing is: --> step: 594/709 -- globalloss: 0.16054 (0.10841) > postnetloss: 0.03669 (0.16323) > decoderloss: 0.25214 (0.17344) > decoderloss: 0.00669 (0.01984) > gadiffloss: 0.12294 (0.09193) > postnetspecssimssimerror: 0.29491 (0.18657) > maxlength: 466.0 > maxlength: 64.0 > steptime: 0.00 > current10+25am-e9e0784 traceback (most recent call last): file ""tts/bin/traintacotron.py"", line 619, in main trainlossstep = train(traintacotron.py"", line 165, in train decoderoutput, alignments, stopbackwardbackward = model( file ""c:\users\voice-trainner\anaconda3\envs\tf2\lib\site-packages\torch\nn\modules\module.py"", line 722, in impl result = self.forward(input, **kwargs) file ""c:\users\voice-trainner\proyecto\tts\tts\tts\layers\tacotron2.py"", line 327, in forward decoderweights, stopinput = torch.cat((memory, self.context), -1) runtimeerror: cuda error: unspecified launch failure",question,question
614,https://github.com/microsoft/recommenders/issues/614,[BUG] python_splitters.py does not drop split_index,"description the splitter methods in pythonindex column. e.g. `train, validation, test = pythonsplit(data, [0.7, 0.15, 0.15])` `train, validation, test` will have a column called split_index. expected behavior (i.e. solution)",Error,Error
2044,https://github.com/streamlit/streamlit/issues/2044,List experimental and beta components of latest nightly build in the docs,"apologies if this exists somewhere. i find it super helpful to list the latest features that exist in beta or experimental in the nightlies in the documentation. maybe with a brief description and a gif.  cheers, c",other,other
3067,https://github.com/streamlit/streamlit/issues/3067,show,"summary type here a clear and concise description of the bug. aim for 2-3 sentences.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,other
960,https://github.com/microsoft/recommenders/issues/960,[ASK] NCF results are less than the original paper,description hi the ncf results are less than the original paper. i tested 1m movielens dataset and the hr and ndcg results are lower than the results in the paper. what is the problem? tnx other comments recommenders/notebooks/02deep_dive.ipynb,question,question
1136,https://github.com/microsoft/recommenders/issues/1136,[BUG] LSTUR and NRMS in News Rec Module,"description can't save nrms model how do we replicate the issue? run example/00start/nrmsmodel(path)"" change getutils/recommender/newsrec/layers.py. * add masking attention weight in attlayer2 layer.",other,Error
3344,https://github.com/streamlit/streamlit/issues/3344,Add Nested Columns,problem i want to use nested columns in order to support more flexible layouts. currently nesting columns insider columns is not supported and leads to the following error: i'm not sure if this has been asked before or why is this not allowed. can someone please elaborate if there are reasons for not allowing this?,other,other
3577,https://github.com/streamlit/streamlit/issues/3577,Return blank on safari,"summary i wrote a little program, and there was no problem debugging it locally, but when i opened the page with my mobile phone, it showed a blanksteps to reproduce code snippet: ** is this a regression? nodebug info - streamlit version: 0.84.0 - python version: 3.8.8 - using conda? pipenv? pyenv? pex? - os version: windows10 (can run successfully) ios12(can not run successfully) - browser version: firefox90 (can run successfully) safari604.1(can not run successfully)additional information",deployment,other
686,https://github.com/iperov/DeepFaceLab/issues/686,"XSEG won't allow me to use full face, only whole face","in the latest 3/30/20 version, i tried to use xseg to mask out an object in fromt of the face... for a full face model (since there are no fan options now during conversion.) i blocked out some obstructions in xseg and then clicked xseg)train.bat. it automatically chooses wf as face type and starts trying to train. but it's a full face. won't let me pick f because it just moves on automatically. not sure if it's a bug or i'm doing something wrong, but i've tried multiple times and it does that each time. thanks.",Error,other
3844,https://github.com/streamlit/streamlit/issues/3844,Streamlit showing still connecting despite enable cors false,"summary hi team, we are using streamlit but it not getting connected when i deploy on aws while locally works fine. we keep seeing healthz getting called continuously and finally more than 10 mins fails. type here a clear and concise description of the bug. aim for 2-3 sentences.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,other
445,https://github.com/streamlit/streamlit/issues/445,Seaborn integration with streamlit,please provide integration with seaborn charting/plotting library.,other,other
199,https://github.com/iperov/DeepFaceLab/issues/199,TPU support?,just wondering if running it on a tpu would significantly lower the time to train the model and if tpu compatibility is planned. @lbfs,question,other
523,https://github.com/deezer/spleeter/issues/523,We've just updated the [FAQ](https://github.com/deezer/spleeter/wiki/5.-FAQ#why-are-there-no-high-frequencies-in-the-generated-output-files-) for making clearer how you can actually perform separation above 11kHz.,"we've just updated the for making clearer how you can actually perform separation above 11khz. notably we pushed alternate configuration files (`spleeter:2stems-16khz`, `spleeter:4stems-16khz` and `spleeter:5stems-16khz`) that perform separation up to 16khz and explained how you can set upper separation frequency in the config file if you need another value. _originally posted by @romi1502 in",other,other
1236,https://github.com/streamlit/streamlit/issues/1236,Integrate with Pyecharts,pyecharts is a great framewok for data visualization,other,other
2391,https://github.com/streamlit/streamlit/issues/2391,Expander crops copy button from st.code(),summary `st.code()` normally has a button on the top right to enable quick pasteboard copying. when inside a `beta_expander()` block the button is gone. see image below. debug info - streamlit version: `streamlit-nightly==0.70.1.dev20201101` - python version: `3.7` - os version: docker image `python:3.7-buster` - browser version: `chrome 86.0.4240.198`,Error,other
762,https://github.com/deepfakes/faceswap/issues/762,New state of the art neural nets,i've stumbled upon this paper this morning : i thought you should probably look into it as the results are really interesting with really small training data sets. it could probably be applied as a new method of generating faces.,other,other
399,https://github.com/iperov/DeepFaceLab/issues/399,Documentation in Markdown,"excuse me to write here, cant find another way to communicate with author. do you have a russian manual sources? i'd like to make it (documentation) with markdown, not pdf. i think it will be more github-ish way. you can contact me with solo12zw74 (at) yandex.ru.",other,other
756,https://github.com/microsoft/recommenders/issues/756,[ASK] Changes references of SAR to be Simple Algorithm for Recommendation,description sar references should all be changed to: simple algorithm for recommendation,question,other
503,https://github.com/iperov/DeepFaceLab/issues/503,Issues with training H64,"hi, i'm new to this whole thing and was wondering if anyone knows how to fix this issue i'm having. whenever i go to train h64 i can't save the model. it says it's unable to run opencl kernel: clobjectfailure. i'm not sure if that makes any sense to anyone but i'm baffled. cheers for the help.",other,question
3597,https://github.com/streamlit/streamlit/issues/3597,OSError: cannot write mode P as JPEG,"summary when i upload a png image using `st.file_uploader` and try to display it, it throws an error because streamlit doesn't seem to support displaying images with alpha channel.steps to reproduce you can use this image: ** debug info - streamlit version: 0.85 - python version: 3.7 - using conda? pipenv? pyenv? pex? : pip - os version: ubuntu 18.04 - browser version: chrome 91 --- edit: seems to work well for other png files with transparency. so what could be issue?",Error,question
107,https://github.com/microsoft/recommenders/issues/107,Clean up notebooks in git,"- find a way to reduce git changes created by changes in kernel, cell run history. - make sure notebooks are following correct templates, review templates consider using nbdiff",other,other
26,https://github.com/deezer/spleeter/issues/26,[Bug] Invalid device ordinal value while setting up XLA_GPU_JIT with spleeter-gpu,description 1. installed using 2. run as 3. got output environment ----------------- ------------------------------- os ubuntu 16.04 installation type conda ram available 64 go hardware spec geforce gtx titan x (maxwell architecture) additional context the gpu that can be used to do the computations is inserted on slot 1. slot 0 cannot be used with tensorflow.,deployment,question
458,https://github.com/streamlit/streamlit/issues/458,Add server.watchFilesystem config option,"add `server.watchfilesystem` config option. it should be `true` by default. when `false`, we don't start a `localsourceswatcher`. also, we need to show a warning when `runonsave` is `true` and `localsourceswatcher` is `false`. see asserts in `config.py`",other,other
490,https://github.com/iperov/DeepFaceLab/issues/490,Facerelight requirements installation and running errors.,"you didn't resolve the issue, it happened with fresh install of 13.11 release, i installed those after that error appeared to see if it would resolve it but it didn't, please look into that error: lighten the faces? ( y/n default:n ?:help ) : n choose light directions manually? ( y/n default:y ) : y collecting fileinfo: 100%######################################################## 1209/1209 [00:03<00:00, 302.65it/s] using pytorch backend. traceback (most recent call last): file ""f:\df\1.dflavx10.1internal\deepfacelab\main.py"", line 302, in arguments.func(arguments) file ""f:\df\1.dflavx10.1internal\deepfacelab\main.py"", line 278, in processfaceset facesetrelighter.relight (arguments.inputone) file ""f:\df\1.dflavx10.1internal\deepfacelab\mainscripts\facesetrelighter.py"", line 206, in relight dpr = deepportraitrelighting() file ""f:\df\1.dflavx10.1internal\deepfacelab\nnlib\deepportraitrelighting.py"", line 13, in init nnlib.import14.11torch import torch file ""f:\df\1.dflavx10.1internal\python-3.6.8\lib\site-packages\torch.py"", line 81, in from torch._c import * importerror: dll load failed",question,question
1324,https://github.com/streamlit/streamlit/issues/1324,Support for Color Picker,"idea some users would like to add a color picker to streamlit apps. for example: .solution ** - consider other color formats: rgb, hsl - consider setting opacity - also recommends shades: when given a color, the picker shows two extra color that's one shade lighter and one shade darker. also their hex codes.",other,other
3867,https://github.com/streamlit/streamlit/issues/3867,Disable hotkeys when menu options are not shown in Viewer Mode,"summary as part of our hamburger menu updates in in v0.89, we remove certain options when a streamlit app is opened in viewer mode. however, hotkeys corresponding some options like clear cache (c) are still active. this means that a viewer could accidentally clear the cache for everyone by accidentally pressing c. we would like disable such hotkeys in viewer mode. steps to reproduce 1. open an app deployed on streamlit cloud in incognito mode 2. pressing c clears the cache even though we don't show the 'clear cache' option to the viewer cc: @willhuang1997",other,other
415,https://github.com/deezer/spleeter/issues/415,[Bug] IndexError: index 1 is out of bounds ,"dear sirs, some songs can not be spleted after i installed the new spleeter. pls see below: os ubuntu installation type conda hardware spec cpu",question,question
182,https://github.com/deepfakes/faceswap/issues/182,Suggestion: Focal length of source images,i have taken a look at some results and think that the focal lenght of the sources images would make a good feature to enhance the results of the face sawp. extracting the focal length from the training images is not trivial and could also be achieved by means of learning like .,other,other
612,https://github.com/mozilla/TTS/issues/612,Multi-speaker Tacotron model training from scratch,"hi, i'm trying to train a multi-speaker tacotron model from scratch using vctk + libritts databases. the model trains fine until about 50k global steps but after that i start running into ""cuda out of memory"", ""nan loss with key=decoderloss"", or ""nan loss with key=decoderloadervalworkers"": 4, // number of evaluation data loader processes. ""batchsize"": 4, //number of batches to shuffle after bucketing. thanks! additional info: - my branch is based on commit ea976b0543c7fa97628c41c4a936e3113896d18a - config file - tensorboard loss plots, attention alignments, output spectrograms, griffin-lim synthesized audio look/sound as expected before running into these errors - as far as i can tell, the errors occur pretty randomly. it could continue training a couple of thousands steps after 50k steps or fail after 500 steps. i don't also see any specific input files triggering these errors in a consistent manner.",question,question
169,https://github.com/mozilla/TTS/issues/169,update readme.,really i was faced two issues. because of not updated the readme.md. beginners it is useful. thanks mozilla tts team :),other,other
119,https://github.com/iperov/DeepFaceLab/issues/119,"Error: OOM when allocating tensor with shape[16384,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc","hello, i am a rookie, and my english is not very well, i will try to describe the problem i encountered as much as possible. when i used your tool to train, an error occurred. i don't know how to fix it. my operating environment is win10 x64 gtx 1060 3g h64 train all default。 here is the error screenshot at first i thought it was the reason for the lack of storage, but as far as i know someone trained with 2g graphics card, and i can't run.i hope you can help me",other,question
208,https://github.com/deezer/spleeter/issues/208,Installing the new 16 khz cutoff Spletter 1.49?,"are new stems-16khz folders installed when this is done? also, what & how are the newer finetune training models being used? is there a way to convert the output stems to mono channel flac files? thanks, roger",question,question
129,https://github.com/mozilla/TTS/issues/129,Error in notebooks/Benchmark.ipynb?,running `notebooks/benchmark.ipynb` with `best: iter-185k` model produce: is it safe to just set `num_chars` to 61?,question,question
1650,https://github.com/streamlit/streamlit/issues/1650,Tensorflow's new version breaks pytests,"summary `tensorflow` has released a new version (2.3.0rc0) on june 27, 2020 which breaks `testmodel` and `testkeras_model` pytests. i've silenced the issue by downgrading the dependency, but we need to solve it appropriately.",Error,Error
204,https://github.com/deezer/spleeter/issues/204,[Bug] spleeter on docker usnig nvidia gpu return error,"description i run spleeter on a docker container, when i exec nvidia-docker run -v $(pwd)/output:/output researchdeezer/spleeter:gpu i have an error . as i can't get into the container with bash login to see what happen, i'm stuck step to reproduce 1. after installing : ii nvidia-driver-440 440.33.01-0ubuntu1 amd64 nvidia driver metapackage ii cuda 10.2.89-1 amd64 cuda meta-package ii cuda-10-2 10.2.89-1 amd64 cuda 10.2 meta-package ii nvidia-docker2 2.2.2-1 all nvidia-docker cli wrapper ii docker-ce 5:19.03.3~3-0~ubuntu-disco amd64 docker: the open-source application container engine ii docker-ce-cli 5:19.03.3~3-0~ubuntu-disco amd64 docker cli: the open-source application container engine and download container : researchdeezer/spleeter researchdeezer/spleeter:gpu 2. i run sudo docker run -v $(pwd)/output:/output researchdeezer/spleeter separate -i audioexample.mp3 -o /output or sudo docker run --runtime=nvidia -v $(pwd)/output:/output researchdeezer/spleeter:gpu separate -i audiolinux.go:346: starting container process caused ""exec: \""spleeter\"": executable file not found in $path"": unknown. erro[0000] error waiting for container: context canceled output the normal out would be this : info:spleeter:downloading model archive info:spleeter:validating archive checksum info:spleeter:extracting downloaded 2stems archive info:spleeter:2stems model file(s) extracted info:spleeter:file /output/('audioexample', '.mp3')/vocals.wav written environment ----------------- ------------------------------- os ubuntu linux 19.10 installation type docker ram available 8go hardware spec nvidia gtx 760 / intel i5 -4690 cpu @ 3.50ghz/ ssd additional context none",question,question
694,https://github.com/mozilla/TTS/issues/694,Pip install TTS does not work python39.2,"i did pip install tts says it can not find a version that satisfies. this is on windows 10 with 64 bit python. note i also tried on raspberry pi but it said it couldn't find torch. questions will not be answered here!! help is much more valuable if it's shared publicly, so that more people can benefit from it. please consider posting on page or matrix if your issue is not directly related to tts development (bugs, code updates etc.). you can also check for common questions and answers. happy posting!",question,other
698,https://github.com/mozilla/TTS/issues/698,Models saved twice on Windows,"bug: models are saved twice on windows. description: i was just doing inference, and then checked the tts directory for the models. it looks like this: in both of the subfolders of tts/, we find the model. but additionally, in each of them, there is another subfolder with the same name that also contains the model. model files have the same hash. this leads to double the memory consumption. reproduce: 1) install on windows 2) command line: tts --text ""guten tag"" --modelmodels/de/thorsten/tacotron2-dca --vocodermodels/de/thorsten/wavegrad --outqbz5n2kfra8p0\localcache\local\tts\",deployment,question
558,https://github.com/streamlit/streamlit/issues/558,bart_vs_bikes.py example doesn't work on Windows,summary ** function,deployment,Error
747,https://github.com/iperov/DeepFaceLab/issues/747,Merging 100%,"merging gets to 100% then stops. i exited out of the merger and relaunched it using the saved session, and then the preview window just turned black. merging is able to go to 100%, but again, the program is stuck in this state. using quick96, python 3.8, windows 10, gtx 1060 3gb update: merged does end up appearing in the dst folder, but the mp4 lossless created is just a black screen stuck at the end of the video. update 2: mp4 (regular, not lossless) works well. so does avi.",other,other
1017,https://github.com/streamlit/streamlit/issues/1017,Chart data of removed chart is never cleared,"summary if i create a chart and add a number of rows, then replace it with some .markdown, the chart data is not cleared from the chrome tabs memory.steps to reproduce run the code below. (based on the if creates a chart, fills it with random data, then replaces with .markdown('done.') and makes a second chart to continue adding data. expected behavior: i expect the data from the first chart to be cleared from the memory. if this is not happening, each streamlit has a maximum amount of data to upload and it is not suitable for continuous output of data (i have some advanced version of this, where i plot a window of the last n data points but old data never gets cleared. see below.)actual behavior: the data is not dropped from the browser's memory and the tab crashes during about half of the second chart filling (75% of the streamlit progress bar, around 1.4 gb ram in chrome tab).is this a regression? nodebug info - streamlit version: 0.53.0 - using conda - os version: ubuntu linux mint 18.3 - browser version: chrome version 80.0.3987.16additional info based on this discuss topic:",Error,Error
835,https://github.com/deepfakes/faceswap/issues/835,CPU at 100% while GPU at ~60,** add any other context about the problem here.,other,other
1725,https://github.com/streamlit/streamlit/issues/1725,"Regression: missing icons/fonts when deployed under a subpath (not ""/"")","summary when you deploy streamlit at `foo.com/bar/baz`, all of streamlit's static assets will be hosted at `foo.com/bar/baz/assets`. however, streamlit is looking for those assets at `foo.com/assets`. culprit: to reproduce 1. deploy your app at host where your app will be at a path other than the root 2. go to our appexpected behavior: the app loads and all icons (hamburger menu, for example) and fonts are presentactual behavior: the app loads but all icons are replaced with squares, and streamlit doesn't use ibm plex font.is this a regression? yes",deployment,Error
605,https://github.com/mozilla/TTS/issues/605,UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 3096,"using: miniconda, separate env - install was successful. using tree: model files: on running the code, i get error:",Error,question
153,https://github.com/deezer/spleeter/issues/153,[Discussion] How do i actually change the max frequency value?,i'm not really into python and coding in general. i just want to change the max frequency value and i have no idea how to do it. i would appreciate it if one of you kind souls helped me out.,question,question
2682,https://github.com/streamlit/streamlit/issues/2682,Fix e2e/Dockerfile to properly replicate the CI env for cypress tests,"this apparently previously worked but ended up breaking at some point, and existing methods of updating snapshot tests (using ci to generate snapshots or `./scripts/scripts/diffmazing_cropper.sh`) all have cases where they either break or the process is relatively painful.",other,other
84,https://github.com/deezer/spleeter/issues/84,[Bug] Is it works on AMD cpus?,description step to reproduce 1. installed using anaconda. 2. run as `...` 3. got intel mkl fatal error: cannot load mklthread.dll. error output environment ----------------- ------------------------------- os windows installation type conda ram available 16gb hardware spec ryzen 5 2600 additional context,question,question
540,https://github.com/iperov/DeepFaceLab/issues/540,"GTX 1060 6GB, can't extract data_dst faces.","hello! i tried 9.2 and 10.1 cuda builds but the problem still exists. i tried different ""data_dst extract"" bats files, but everywhere i got: ""images found: 0, faces detected: 0"".",question,question
702,https://github.com/microsoft/recommenders/issues/702,[BUG] databricks_install.py fails with ModuleNotFound Error,"description python scripts/databricksinstall.py"", line 33, in from scripts.generatefile import pipinstall.py expected behavior (i.e. solution) the issue as i understand it is due to the python script adding the directory of the script to sys.path, but not script invokation does not add the current working directory. fix incoming where the import comes later, after args.pathrecommenders has been parsed, so that it can make sure it's processed. other comments",Error,Error
1545,https://github.com/streamlit/streamlit/issues/1545,Only load Segment's script if `gatherUserStats` is true,"today, streamlit (correctly) only tracks usage stats if `gatheruserstats` is set to `true`. however, no matter whether `gatheruserstats` is `true` or `false`, we ** load segments's js code. see . we should change this so we load segment's js if `gatheruserstats` is `true`. there are different ways to implement this: 1. check whether segment has an official js client in npm, and use that instead. we'd still need to check whether that client automatically phones home when loaded. if it does, we should make we lazy-load it. 2. use a somewhat hacky solution where we simply append the appropriate `` tag to the dom when `gatherusagestats` is `true`. something like 3. server-side render `index.tsx`, adding the segment script only if `gatherusagestats` is `true`. the simplest solution here is (2), and i'm actually ok with it, even though it's a tiny bit dirty.",Error,other
564,https://github.com/streamlit/streamlit/issues/564,fix add_rows cypress test,"summary addrows graphs should have an expected height every time they are rendered.actual behavior: the height of the graphic is unpredictable, alternates between 2 values.",Error,Error
3338,https://github.com/streamlit/streamlit/issues/3338,Trouble loading streamlit_cropper.st_cropper component,"hi, thanks for this helpful package. i am getting the following error - (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,other
49,https://github.com/streamlit/streamlit/issues/49,SelectBox 'title' font seems different than any other text font,"summary the text in the title of a selectbox seems smaller than other text.steps to reproduce create a selectbox and render it. expected behavior: the size of the text in is similar to the text in the options.actual behavior: the size of the text for is smalleris this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: streamlit v0.45.0 - python version: 3.7.4 - using conda? pipenv? pyenv? pex? - os version: macos 10.14.6 - browser version: chrome 76.0.3809.100additional information if needed, add any other context about the problem here.",Error,Error
166,https://github.com/deepfakes/faceswap/issues/166,Training start from scratch. Files of existing trained model was removed.,"for two days everything worked well for me. i already several times stopped and continued training. but today i tried to continue training using already existing model and it couldn't be loaded. after that, training started immediately from scratch, overwriting the already trained model. i think this is not the correct behavior and the training should be terminated instead of overwriting the model by creating new files. or at least a model backup should be created to avoid such embarrassment.",other,question
706,https://github.com/iperov/DeepFaceLab/issues/706,Unspecified launch failure when training,"expected behavior i'm trying to run `6) train saehd.bat`. i would expect this to train the nn. actual behavior it trains for a short time, then crashes with an ""unspecified launch failure"". steps to reproduce i don't think i did anything unusual. i didn't even use my own videos for the source, i'm just using what came in the prebuilt windows binary. i can upload my workspace folder if you like so you can see it. my graphics card is a gtx 1060 with 3gb of vram, and that's what i'm using to train. is it possible that this is caused by not having enough vram? other relevant information - ** i'm using the prebuilt windows binary",other,question
1710,https://github.com/streamlit/streamlit/issues/1710,[FEA]  support allow fullscreen in embedded iframes,"problem embedded iframe components may have toggle-fullscreen buttons to enlarge presentation content in a way that preserves visual context, allows 'jumping back', and all while providing super snappy performance, vs opening in a new tab. this helps core start/end workflows such as in movies, maps, etc. unfortunately, the new iframe api does not set `iframe allow=""fullscreen""` by default, nor provide a mechanism for manually setting `iframe allow=""fullscreen""` and per-browser equivs. the result is buttons on embedded uis do nothing, so users get a less-featureful experience + more buggy one.solution ** what are other things that could be added to the mvp over time to make it better? -- security audit to determine whether `allowfullscreen` can default to `true` -- feature audit for what other iframe features should be exposed, such as scroll captureadditional context this was noted in cc @randyzwitch @andfanilo",other,other
157,https://github.com/microsoft/recommenders/issues/157,review split utils,review this function `minfilter` and consider making it explicit,other,other
763,https://github.com/deepfakes/faceswap/issues/763,Error thrown when converting,"when converting, the error `(-215:assertion failed) !ssize.empty() in function 'cv::resize'` occurs at `faceswap\plugins\extract\align\cv2image interpolation=interpolation)` and it fails. full stack trace and system info here: i'm using this in anaconda.",Error,Error
129,https://github.com/iperov/DeepFaceLab/issues/129,AMD OpenCL Support?,"how difficult would it be to get it to work on vega 64 cards, i do have a couple of them around?",question,question
467,https://github.com/deepfakes/faceswap/issues/467,extract.py without alignment file exits with AttributeError: 'Namespace' object has no attribute 'multiprocess',"expected behavior in cpu mode, if alignments file doesn't exist, run self-generation of alignment file by extractprocess() by this code: actual behavior however, this is a valid use case when the training is done in a cloud-based gpu, and the conversion is done afterwards on the local machine to save cloud time. steps to reproduce 1. delete alignments.json file in images directory 2. run final conversion other relevant information - ** cpu",deployment,question
310,https://github.com/deezer/spleeter/issues/310,[Bug] no supported devices found for platform CUDA,description when the command 'spleeter separate' is launched an error returns step to reproduce 1. installed using `sudo pip install spleeter` 2. run as `spleeter separate -i filename.ogg -p spleeter:4stems -o xxx` 3. got error you can see here below output environment os linux distribution opensuse leap 15.1 installation type pip ram available 16 gb hardware spec gpu nvidia gk107gl quadro k420 / cpu intel i5-4590 4x @ 3.30 ghz additional context installed cuda platform using nvidia rpm,deployment,question
4086,https://github.com/streamlit/streamlit/issues/4086,Widgets overlapping in Firefox,"summary here is the same app displayed in chrome and in firefox with chrome on the left and firefox on the right. in firefox the widgets overlap one another. steps to reproduce code snippet: c1, c2, c3, c4 = st.columns([1, 2, 3, 4]) ** firefox is not behaving as expectedis this a regression? i am not suredebug info - streamlit version: 0.88 - python version: 3.8 - using conda - os version: ubuntu 20.04 - browser version: firefox/chrome",other,other
492,https://github.com/mozilla/TTS/issues/492,"Master branch: setup.py develop fails with ""ModuleNotFoundError: No module named 'joblib'""","hello. executing ""setup.py"" on current master branch fails with following error. it works when removing **@ in setup.py: ""phonemizer @ phonemizer was updated to version 2.2.1 round about 2 weeks ago (24.07.2020) - maybe that version update in part of problem.",deployment,question
224,https://github.com/deezer/spleeter/issues/224,"[Bug] Training ""assertion failed:"" on large dataset","description i've had this problem for a while now and it does not seem to go away. i partially solved it for testing purposes by removing 94% of my dataset by going from 13000 files down to 800 files, and as you can imagine, that's not really what anyone is looking for. step to reproduce really not much to it. just having a very large dataset and let the error occur. output environment ----------------- ------------------------------- os windows installation type conda ram available 32gb hardware spec i7-9900k / rtx2080 additional context mr @mmoussallam (i think) asked me in a previous opened issue if the file is mono. no. none of the files are mono. all the same format and same duration. currently i run a script that removes the specific failed five from the csv and restarts the training process, and training seem to fail at around file number 860-something. but when this takes several minutes it adds up over time.",other,question
626,https://github.com/deezer/spleeter/issues/626,[Bug] Spleeter Separate on custom trained model tries to download another model,"description i have trained a custom model and the separate function does not work as intended as it tries to download the model from a nonexisting url. step to reproduce 1. created custom model training data / specs annotated in the customconfig.json 2. trained model (succesfully, apparently) using `!spleeter train -p ""customconfig.json"" -d ""pathtocustomdataset""`. i check that the trained model folder is created. 3. when trying to separate a file using `!spleeter separate -o sepmodel_config.json"" ""test.mp3""` the function tries to download the model from a url which obviously does not exists. why is this happening? output environment ----------------- ------------------------------- os linux (colab environment) installation type pip ram available 16gb",question,question
554,https://github.com/streamlit/streamlit/issues/554,EXCEPTION! 'NoneType' object has no attribute 'strip',summary fail to run streamlit on python3.6.3 with anaconda and dockersteps to reproduce just type `streamlit hello`expected behavior: try to run streamlit according to the documentactual behavior: make an exception as **,Error,Error
3257,https://github.com/streamlit/streamlit/issues/3257,Large text inside `beta_container` does not display,"summary when using the new layout function `betacontainer` ** a title that gets trumped if the line overflowsis this a regression? that is, did this use to work the way you expected in the past? it's odd as we have a local development environment and it doesn't happen there, but on our production environment it doesn't work. the only difference between the two is that one is launched directly from the user computer and on production is launched from a docker container.debug info - streamlit version: 0.81.1 - python version: 3.7.10 - using streamlit-launchpad: 0.0.9 - os version: macos 10.15.7 - browser version: it happens in all browsers",other,Error
803,https://github.com/deepfakes/faceswap/issues/803,Can it swap voice?,can it train and swap voices?,question,other
286,https://github.com/deezer/spleeter/issues/286,[Discussion] Bad training results,"as title says. why after i've trained a model, the two resulting stems kind of bleed in to each other. both of them vaguely have audio from each other.",question,question
800,https://github.com/deepfakes/faceswap/issues/800,ValueError: Error initializing Aligner,"** - os: [windows-10-10.0.17763-sp0] - browser [ie] - version [v11] ` 07/20/2019 00:57:46 mainprocess mainthread multithreading start debug started all threads 'saveitemitemaligner debug launching aligner 07/20/2019 00:57:47 mainprocess mainthread multithreading _init': , 'loglevel': 10, 'inqueue': }, daemon: true) 07/20/2019 00:57:47 mainprocess mainthread multithreading start debug spawned process: (name: 'aligner.run', pid: 9360) 07/20/2019 00:57:49 aligner.run mainthread base initialize align: (pid: 9360, args: (), kwargs: {'event': , 'error': , 'logqueue': , 'logqueue': , 'outinit': , 'loglevel': 10, 'inqueue': }) 07/20/2019 00:57:49 aligner.run mainthread gpustats initialize debug os is not macos. using pynvml 07/20/2019 00:57:49 aligner.run mainthread gpudevicestats getdevices debug active gpu devices: [0] 07/20/2019 00:57:49 aligner.run mainthread gpuhandles debug gpu handles found: 1 07/20/2019 00:57:49 aligner.run mainthread gpudriver debug gpu driver: 385.54 07/20/2019 00:57:49 aligner.run mainthread gpudevices debug gpu devices: ['geforce gtx 1060 6gb'] 07/20/2019 00:57:49 aligner.run mainthread gpuvram debug gpu vram: [6144.0] 07/20/2019 00:57:49 aligner.run mainthread gpustats initialize debug os is not macos. using pynvml 07/20/2019 00:57:49 aligner.run mainthread gpudevicestats getdevices debug active gpu devices: [0] 07/20/2019 00:57:49 aligner.run mainthread gpuhandles debug gpu handles found: 1 07/20/2019 00:57:49 aligner.run mainthread gpufree debug gpu vram free: [5860.96484375] 07/20/2019 00:57:49 aligner.run mainthread gpucardfree debug active gpu card with most free vram: {'cardbase getfree verbose using device geforce gtx 1060 6gb with 5860mb free of 6144mb 07/20/2019 00:57:49 aligner.run mainthread fan initialize verbose reserving 2240mb for face alignments 07/20/2019 00:57:49 aligner.run mainthread fan loadbase run error caught exception in child process: 9360 07/20/2019 00:57:53 aligner.run mainthread base.py"", line 112, in run file ""c:\faceswap\plugins\extract\align\session file ""c:\anaconda3\envs\faceswap\lib\site-packages\tensorflow\python\client\session.py"", line 1551, in _impl.internalerror: cudagetdevice() failed. status: cuda driver version is insufficient for cuda runtime version traceback (most recent call last): file ""c:\faceswap\lib\cli.py"", line 122, in executeextraction file ""c:\faceswap\plugins\extract\pipeline.py"", line 171, in launch file ""c:\faceswap\plugins\extract\pipeline.py"", line 206, in launchbranch: master gitcuda: 9.0 gpudevices: gpudevices0 gpuvram: gpumachine: amd64 osrelease: 10 pycondaimplementation: cpython pyvirtualcores: 4 sysram: total: 8128mb, available: 5726mb, used: 2402mb, free: 5726mb =============== pip packages =============== absl-py==0.7.1 astor==0.7.1 certifi==2019.6.16 cloudpickle==1.2.1 cycler==0.10.0 cytoolz==0.10.0 dask==2.1.0 decorator==4.4.0 fastcluster==1.1.25 ffmpy==0.2.2 gast==0.2.2 grpcio==1.16.1 h5py==2.9.0 imageio==2.5.0 imageio-ffmpeg==0.3.0 joblib==0.13.2 keras==2.2.4 keras-applications==1.0.8 keras-preprocessing==1.1.0 kiwisolver==1.1.0 markdown==3.1.1 matplotlib==2.2.2 mkl-fft==1.0.12 mkl-random==1.0.2 mkl-service==2.0.2 mock==3.0.5 networkx==2.3 numpy==1.16.2 nvidia-ml-py3==7.352.0 olefile==0.46 pathlib==1.0.1 pillow==6.1.0 protobuf==3.8.0 psutil==5.6.3 pyparsing==2.4.0 pyreadline==2.1 python-dateutil==2.8.0 pytz==2019.1 pywavelets==1.0.3 pywin32==223 pyyaml==5.1.1 scikit-image==0.15.0 scikit-learn==0.21.2 scipy==1.2.1 six==1.12.0 tensorboard==1.13.1 tensorflow==1.13.1 tensorflow-estimator==1.13.0 termcolor==1.1.0 toolz==0.10.0 toposort==1.5 tornado==6.0.3 tqdm==4.32.1 werkzeug==0.15.4 wincertstore==0.2 ============== conda packages ============== packages in environment at c:\anaconda3\envs\faceswap: # name version build channel select 2.1.0 gpu absl-py 0.7.1 py360 blas 1.0 mkl ca-certificates 2019.5.15 0 certifi 2019.6.16 py360 cudatoolkit 10.0.130 0 cudnn 7.6.0 cuda10.00 cytoolz 0.10.0 py36he7745220 decorator 4.4.0 py361000 conda-forge ffmpeg 4.1.3 h65383350 pypi freetype 2.9.1 ha9979f80 grpcio 1.16.1 py36h351948d0 hdf5 1.10.4 h7ebc959rt 2019.0.0 h0cc432a0 conda-forge imageio 2.5.0 py360 conda-forge intel-openmp 2019.4 245 joblib 0.13.2 py361001 conda-forge keras 2.2.4 0 keras-applications 1.0.8 py0 keras-preprocessing 1.1.0 py0 libblas 3.8.0 8mkl conda-forge liblapack 3.8.0 8mkl conda-forge libmklml 2019.0.3 0 libpng 1.6.37 h76027380 libtiff 4.0.10 h6512ee22 conda-forge lz4-c 1.8.3 he025d500 matplotlib 2.2.2 py36had4c4a90 mkl0 mkl0 mock 3.0.5 py360 numpy 1.16.2 py36h19fb1c00 nvidia-ml-py3 7.352.0 pypi0 opencv 4.1.0 py36hb4945ee1 pathlib 1.0.1 py360 pip 19.1.1 py360 psutil 5.6.3 py36he7745220 pyqt 5.9.2 py36h65383351 python 3.6.8 h9f7ef890 pytz 2019.1 py1 pywin32 223 py36hfa6e2cd0 qt 5.9.7 hc6833c90 scikit-learn 0.21.2 py36h6288b170 setuptools 41.0.1 py360 six 1.12.0 py360 tensorboard 1.13.1 py36h33f27b4py36h9006a92py36h871c8ca0 tensorflow-gpu 1.13.1 h0d30ee61 tk 8.6.8 hfa6e2cd0 toposort 1.5 py0 tqdm 4.32.1 py4 vs20154 werkzeug 0.15.4 py0 wincertstore 0.2 py36h7fe50ca1001 conda-forge yaml 0.1.7 hc54c5091005 conda-forge zstd 1.4.0 hd8a0e53_0 conda-forge `",Error,other
395,https://github.com/iperov/DeepFaceLab/issues/395,Manual extract error,"this is not tech support for newbie fakers post only issues related to bugs or code expected behavior actual behavior steps to reproduce other relevant information performing manual extract... running on geforce gtx 1080 ti. using tensorflow backend. 16%###### 36/219 [23:38 file ""\downloads\deepfacelabcuda10.1avx\extract file ""\downloads\deepfacelabcuda10.1avx\pathpaths ], 'landmarks', imagetype, debugonly=cpuwindowwindowinternal\deepfacelab\jo blib\subprocessorbase.py"", line 222, in run file ""\downloads\deepfacelabcuda10.1avx\result ).tolist() typeerror: unsupported operand type(s) for * windows",Error,other
3334,https://github.com/streamlit/streamlit/issues/3334,MediaFileManager: Missing file on Google App Engine,"hi all :slightlyface:, i have been going through the process of deploying a streamlit application to using google app engine. once it was deployed it presented an interesting bug that i hadn't seen before. the `matplotlib` plots were intermittently not showing up within the interface and the following error message was presented within the app engine logs: --- here is what i saw --- here is what was able to make the app able to see if i dragged the slider to other values and then came back a few times: --- i had a suspicion that this might be due to the new garbage collection process being undergone in `0.82.0`: so i downgraded to version `0.81.1`, but unfortunately the issue was still present. then, wondering if maybe i wasn't provisioning sufficient ram i increased the instance from 5 gb up to 16 gb. however unfortunately this still didn't rectify the issue. i haven't yet pointed my domain to the app engine version, so if you'd like to see this issue in action go to --- i'd be more than happy to go through further debugging if you so desire. --- i have copied in the code i am using to produce this below, of particular note, i am directly passing the `fig` object through to `st.pyplot`, so hopefully shouldn't be running into the issue of using the global `plt.figure()` object. cheers :slightlyface:, simon",other,other
242,https://github.com/mozilla/TTS/issues/242,Special symbol for silence?,is any special symbol for silence exist? i.e. if i want to generate a text like `text = [silence symbol] * 10 + 'hello how are you?' `,question,question
947,https://github.com/microsoft/recommenders/issues/947,"[ASK] In wide and deep, how to add more than one item/user feature?",description request for how to use buildcolumns() in wideutils.py when using item and or user features. other comments the example notebooks use the genre column from the movielens database. please provide an example of how to use itemcol and itemshape if we want to use more than one feature column from item and/or user information.,question,question
1133,https://github.com/streamlit/streamlit/issues/1133,Time caching,i realised that there is no time caching in streamlit the way i do it is with this hack,other,other
971,https://github.com/deepfakes/faceswap/issues/971,Any chance to take animated gif as an input and only one image,** na,other,other
2329,https://github.com/streamlit/streamlit/issues/2329,File Uploader: Consolidate reruns for multiple files,"solution: every time a file uploads, rerun only if there are no other incomplete uploads (in the same widget) alternatives discussed: - parallel upload == 1 rerun. sequential = n - (super fancy) like today, but throttled. only rerun if no upload in progress after. - add fancy parameters - customizable v1. introduce a parameter that lets you switch behaviors - customizable v2. developer can specify number of files, and rerun only when n is hit",other,other
312,https://github.com/streamlit/streamlit/issues/312,Expire cache every so often,"items cached in `st.cache` should expire every now and then, otherwise this can cause memory usage to grow unboundedly in some cases. how about we implement the following config options: the defaults could be `ttl=float(inf)` and `maxsize=float(inf)`. these would be fairly easy to implement if we based our cache on rather than a simple dict.",other,question
307,https://github.com/iperov/DeepFaceLab/issues/307,Training Error H64,"expected behavior i'm trying to train, but everytime i run the batch file i get this error code.",other,question
386,https://github.com/streamlit/streamlit/issues/386,st.checkbox(message) same object for same message.,"summary this is perhaps less of a bug and more of an issue with awkward functionality. using st.checkbox(message) more than once (same message) in code will cause all to be set to true or false if only one is checked. if i slightly change the message in each, it fixes the issue. but, when i'm using this as a feature like ""more information"", i would like to be able to reuse this checkbox message for consistency. steps to reproduce what are the steps we should take to reproduce the bug: code: clicking on the first or second checkboxes causes them both to evaluate to true or false. expected behavior: they should be cached independently.actual behavior: the cached value of st.checkbox is relative to the message in checkbox (checkbox object is created only when message is different). if this is the intended functionality, then is it possible to create a new checkbox object with the same message?debug info - streamlit version: 0.47.4 - python version: 3.6.8 - using conda? pipenv? pyenv? pex? no - os version: ubuntu:18.04 (docker base image) - browser version: chrome",question,Error
101,https://github.com/iperov/DeepFaceLab/issues/101,How to get the mask of face?,"i have a question about the mask. there is only a dataset of images, but no mask. how can the nn learn to predict a mask? is there a trained mask-rcnn or fcn for getting the mask？",question,question
1132,https://github.com/streamlit/streamlit/issues/1132,Interactive text dialogue Box,"can you add the ability to type text in a box and get the content in the python ? in other, which part of html 5 form, are you able to setup in streamlit ?",other,other
888,https://github.com/streamlit/streamlit/issues/888,Access streamlit uploaded image in opencv,how do i open image in opencv. i have this code which gets uploaded image and displays it on streamlit: my question is i want to read and save this uploaded file using: how can i achieve this ?,question,question
3634,https://github.com/streamlit/streamlit/issues/3634,Dtype conversion error in V0.85 of the recent Pyarrow migration,"summary dtype conversion error in v0.85 of the recent pyarrow migrationsteps to reproduce code snippet: ** arrowtypeerror: (""expected bytes, got a 'datetime.datetime' object"", 'conversion failed for column value with type object')is this a regression? that is, did this use to work the way you expected in the past? yesdebug info - streamlit version: 0.85.0 or 0.85.1 - python version: 3.8 - using conda: 4.10.1 - os version: osx 11.5.1 - browser version: safari 14.1.2 - pyarrow version: 4.0.0 - pandas version: 1.2.4 - numpy version: 1.20.2additional information just happened since pyarrow migration. it will work if downgraded to 0.84.",other,Error
1947,https://github.com/streamlit/streamlit/issues/1947,Pin black version to current,"black formatting can be pinned so that any future version upgrades does not cause any breaking changes. as new versions become available, we can consciously decide to upgrade. see #1946 for proposal on new version notifications.",other,other
350,https://github.com/streamlit/streamlit/issues/350,DataFrame Demo failed,summary 77.0.3865.90 - page only shows - 'streamlit failed to hash an object of type 'steps to reproduce what are the steps we should take to reproduce the bug: 1. go to 'choose a demo' 2. click on 'dataframe demo'debug info - streamlit version: 0.47.4 - python version: 3.7.3 - using conda - os version:10.14.6 - browser version: chrome 77.0.3865.90,Error,Error
119,https://github.com/mozilla/TTS/issues/119,Use utf-8 encoded characters,utf-8 encoding can provide a language-independent implementation for tts so people don't need to change character sets manually for different languages. it increases the embedding slayer size a bit but still might be favored for flexibility.,other,other
441,https://github.com/microsoft/recommenders/issues/441,Bug on hypertune_spark_deep_dive notebook dataload,"movielens dataloader uses default column names in the repo. to use custom names, should pass either header or schema. is affected by this bug? run of a notebook.which spark do we replicate the issue? * run the notebook expected behavior (i.e. solution) easily fix by changing the data loading line as: `loaddf(spark, size='100k', header=(colitem, col_rating))`",Error,Error
44,https://github.com/mozilla/TTS/issues/44,The Checkpoints are invalid after download and trying to untar.  What should I do?,will you be posting new version of the following in a location other than google drive? 1. iter-62410 2. iter-170k 3. iter-270k thanks!,other,question
1945,https://github.com/streamlit/streamlit/issues/1945,Unpin setuptools,"summary setuptools is pinned as part of #1944 while an is opened with them . once the issue is resolved, we can unpin setuptools.",other,other
462,https://github.com/microsoft/recommenders/issues/462,Python chrono splitter takes too long,is affected by this bug? the python chrono splitter takes around 5h to split movielens 10m in platform does it happen? azure data science virtual machine.* expected behavior (i.e. solution) try to reduce that time,other,Performance
518,https://github.com/streamlit/streamlit/issues/518,Improve docs about st.slider() support for ranges,** it's not super clear that you can pass a tuple to create a range slider. and perhaps we should also mention it in,other,other
271,https://github.com/deezer/spleeter/issues/271,[Bug] Crashes with protobuf 3.11.1 on macOS,description newer versions of protobuf crashes tensorflow and spleeter. see steps to reproduce environment ----------------- ------------------------------- os macos catalina installation type pip ram available plenty workaround,deployment,deployment
1032,https://github.com/streamlit/streamlit/issues/1032,Allow hiding tracebacks,"currently, when a streamlit app throws an exception, we print the traceback to the browser. this isn't necessarily the right thing to do for all apps; we should allow this to be configurable. maybe a `[client] showtracebacks = false` option? and presumably, if tracebacks are disabled, we should filter them at the server level, so that the client never even receives the string, in case the user is worried about leaking internal app details. (related discussion here:",other,other
4082,https://github.com/streamlit/streamlit/issues/4082,Form updates session state only after every second submission,"summary using a form and a submit button to change a session state variable, e.g., a string, does only work for every second submission.steps to reproduce code snippet: 1. change text in text input 2. click on 'submit' -> session state changes to new text 3. change text in text input 4. click on 'submit' -> input in text field is reset to previous value and session state is not updated 5. change text in text input 6. click on 'submit' -> session state changes to new text this repeats, only every second submission is successful. ** i have no explanation for this behavior, but i assume it is unexpected. am i missing something?is this a regression? unknown, i did not use older streamlit versions.debug info - streamlit version: 1.2.0 - python version: 3.8 - os version: ubuntu 20.04.3 lts 64 bit - browser version: firefox 94.0",question,other
318,https://github.com/mozilla/TTS/issues/318,Problem with finetune model,"@erogol @reuben @twerkmeister @ekr hey guys, thanks for you great work here! i'm trying training tacotron2 with a custom dataset and generally it runs well but still with some issues that i failed to resolve. it would be kind if you could give me some ideas about them. the nearest problem i got yesterday is when i tried to finetune my model with bn version prenet as mentioned by @erogol in other comment. but with distributed training launched by 'python3.7 distribute.py --restoremodel.pth.tar', i soon got cuda memory error while found the gpu situation showed below. if i understand well, the main gpu device 0 has been used by all the other 7 subprocess and ran out of memeory while the other 7 still had free memories. i did some search and this probably relates to the restore of adam optimizer since someone comment that adam has to restore all parameters only from main gpu device? any idear about this? other doubts are about some training details that i posted here i woud be grateful if you could share some ideas with me, thanks in advance!",deployment,question
297,https://github.com/streamlit/streamlit/issues/297,add print to pdf,"i know the whole point is to show interactive visuals, but it would be very useful to have an option export to pdf, a lot of users that all what they want just a pdf of the report. very good product by the way",other,other
815,https://github.com/deepfakes/faceswap/issues/815,CRITICAL An unexpected crash has occurred during conversion,"attached is my crash report. extracted about 100 photos from both parties, trained for about 2hrs. 40 mins. applying photo face to a video of me talking. not sure what's going on. i'm using a microsoft surface book ii, with a gtx 1060, i7, 16gb ram. need help asap please! working on a time sensitive project! thanks for everyone's hard work on this very impressive and cool program. thought i would have way more issues up until now. thanks.",Error,question
1085,https://github.com/deepfakes/faceswap/issues/1085,Being informed on manual preview refresh,"on slower hardware and with demanding model configurations it can take several minutes until a manual preview refresh actually completes. for that reason i suggest that another message ""refresh preview done"" will be added, so that the user can focus on other things in the meantime and still reliably tell whether the refresh has completed or not.",other,other
227,https://github.com/deezer/spleeter/issues/227,24bit audio,"just a quick question. i have noticed 2 things when running spleeter. 1. the limit on the frequency (upto 11k, and i will have a play with the 16k models soon), which i have been reading about in the wiki. what about 24bit audio. i have also noticed that the files get converted to 16bit rather than retaining the full 24bits. the bitrate option is that only for lossy compression? if i wanted to go to 24bit, what do i need to change in the code to do this? thanks, alec",question,question
233,https://github.com/iperov/DeepFaceLab/issues/233,NameError: name 'NotImplementeError' is not defined,"when i convert via fansegmentator, raise error following:",Error,Error
536,https://github.com/iperov/DeepFaceLab/issues/536,Extract unaligned faces (avatar) is broken in 12/23 commit,this is not tech support for newbie fakers post only issues related to bugs or code expected behavior: run 5) datadst extract unaligned faces outputs misaligned faces. all faces are tilted sideways. steps to reproduce run 5) data_dst extract unaligned faces with the latest commit. running the same command with a september 2019 version of dfl produces correct alignments. other relevant information running prebuilt dfl (windows 10) with nvidia 2080ti,Error,other
46,https://github.com/streamlit/streamlit/issues/46,Parse ./.streamlit/config.toml in the current directory.,"problem i'd like to run several streamlit apps simultaneously on the same computer, each with its own port.solutionmvp parse `./.streamlit/config.toml` in the current directory. this should be parsed `~/.streamlit/config.toml` in the home directory, and overrride any settings there.example if `~/.streamlit/config.toml` contains: and `./.streamlit/config.toml` contains: then the final config would be : possible additions ultimately, we should also allow the user to .",other,question
490,https://github.com/deepfakes/faceswap/issues/490,"When using alignments tool, it will crash when not supplying a faces folder.","*describe, in some detail, what you are trying to do and what the output is that you expect from the program.describe, in some detail, what the program does instead. be sure to include any error message or screenshots.* #123, #124... - ... (for example, installed packages that you can see with `pip freeze`)",Error,other
317,https://github.com/deezer/spleeter/issues/317,[Discussion]Spleeter only supports win64 bit system and cannot be used on Win32 bit system?,spleeter only supports win64 bit system and cannot be used on win32 bit system?,other,question
961,https://github.com/streamlit/streamlit/issues/961,Always rerun doesn't work with VSCode,"summary the ""always re-run"" button doesn't seem to work when editing files with vscode. it only appears the first time after i refresh localhost (and edit my file), which is normal. but, for any further source code edits i need to manually re-run the file (press r inside localhost). this works as intended while editing the file with other file editors (tested on notepad++, notepad).steps to reproduce 1. create new basic main.py file: 2. start server: `streamlit run main.py --server.port=80` 3. open main.py in vscode and edit it 4. click on always re-run (the file re-runs and displays the edit) 5. edit main.py in vscode again (this time the file doesn't re-runs)expected behavior: the result in localhost should have updated based on the edited source code.actual behavior: the localhost generated file doesn't automatically re-run after editing the main.py file in vscode. is this a regression? this is my first time using streamlit.debug info - streamlit version: 0.52.2 - python version: 3.7.4 - using conda 4.8.1, with pip 19.2.3 - os version: windows 10 - browser version: google chrome 79.0.3945.117additional information i followed this:",deployment,other
463,https://github.com/deezer/spleeter/issues/463,Unresolved conflicts installing to OSX 10.13.6 ... Is 10.15 required?,"description installing to osx 10.13.6 results in a number of unresolved conflicts and the installation fails. step to reproduce 1. verified python3 installed. (v3.8.3) 2. installed anaconda3 3. installed spleeter using conda with the command ""conda install -c conda-forge spleeter"" 4. a number of unresolved conflicts were listed and spleeter was not installed (or the install was too incomplete to do anything) 5. tried both gui and command line versions of anaconda. same results. the conda install appears ok and produces no errors itself. output enter: spleeter separate -h -bash: spleeter: command not found enter: python -m spleeter separate -h /users/jn/anaconda3/bin/python: no module named spleeter environment ----------------- ------------------------------- os macos 10.13.6 installation type conda ram available 16gb hardware spec intel hd graphics 3000 512 mb / 2.5 ghz intel core i7 / additional context i don't really have a preference for osx version if it might require difficult installation. what version of osx will allow for the simple install in a couple steps as noted?",question,question
407,https://github.com/streamlit/streamlit/issues/407,Disabling one checkbox disables the following checkboxes but not their output,"summary when disabling checkboxes, succeeding checkboxes present a weird behaviour: they are disabled but their output acts as if they were enabled. this is not the same but it is related to #386 steps to reproduce the issue can be reproduced using this checkbox-snip: the output looks like this: --- 1. enable checkbox1 --- 2. enable checkbox2 --- 3. disable checkbox1 ---expected behavior: after step 3 i would expect the checkbox2 to be ** left enabled: a) --- b)actual behavior: explained above.is this a regression? dunnodebug info - streamlit version: 0.47.4 - python version: 3.7.4 - os version: 10.14.6 - browser version: 12.1.2additional information na",Error,Error
363,https://github.com/mozilla/TTS/issues/363,How many Tesla K80s are recommended when training Tacotron?,"right now i am training on an instance with 8 gpus, but they are not full while the model is training.",question,question
174,https://github.com/iperov/DeepFaceLab/issues/174,Feature Request - Time Stamp on saving,"i think it would be helpful and advantageous to have a time stamp sent to stdout each time a save happens. as i train throughout the day, its helpful to see how much time has passed vs. number of epochs. that way i can get a rough idea as to how long it will take to hit a certain epoch. eg;",other,other
251,https://github.com/iperov/DeepFaceLab/issues/251,Fakeapp Faded Color Issues,"firstly, i accidentally swapped two of the faces on the same model ( for curiosity ) and turns out it ruines color accuracy for ( data a ) and now does not train accurately with faded color. i have taken steps such as resetting the os which has not fixed the issue, reinstalling all nvidia drivers and reinstalling fake-app. i have also tried resetting monitor color profiles as-well... still with no luck. advice: never swap data sets! hope anyone has any suggestions or inquiries, thanks 👍",other,question
336,https://github.com/iperov/DeepFaceLab/issues/336,Preview Window is not showing up on linux,"hi, i know that the linux project has its own thread, but i can't find the way to submit an issue there... so probably you could advise me here. i usually use dfl on a headless linux server machine with preview_training , etc.. but today i try to train some model on my local linux machine, but for some reason preview windows is not showing up, even when converting with debug... what could be the reason? regards",question,question
3715,https://github.com/streamlit/streamlit/issues/3715,Data/Object download widget,"problem in using streamlit to make very useful tools for people, i've encountered many scenarios where i need to write the output of a computation or visual to a local disk/server for use in other specialized tools. other programs often have an input box much like the text input widget with a small file icon that could pop up a window for a user to specify a full filepath. this could already be used in conjunction with the form submit button. i have seen some workarounds posted out there that create a href, perform some encoding (such as base64), or both. but this seems clunky and will not be a universal form of i/o from this awesome platform. is your feature request related to a problem? please describe the problem here. ex. i'm always frustrated when [...]solution ** if you don't like the mvp above, tell us why, and what you'd like done instead.additional context add any other context or screenshots about the feature request here. for example, did this fr come from or another site? link the original source here!",other,other
3400,https://github.com/streamlit/streamlit/issues/3400,Scheduling for Duplicate Checker not working,"summary hi guys, a few weeks ago, we implemented the duplicate checker for a customer. however, the customer requested to have a schedule implemented for that ml script. thus, we tried to use the scheduling functionality that is implemented into the script. however, after the schedule should have been run for the first time, it just crashed with the following message:steps to reproduce code snippet: to reproduce, i literally just clicked through the duplicate checker streamlit sheet and set the schedule. attached, you can find how the schedule had been executed. ** i obviously would expect, that the schedule runs through and the input i entered is basically saved for the re run of the checker. if you have any ideas how this could be fixed, please let me know. best regards, sk",other,other
292,https://github.com/mozilla/TTS/issues/292,CUDA OOM while indeed there is plenty of memory left!,"hello everyone! i try to train tacotron2 model. first tried with dev branch, now with master. closely to end of the 1st epoch i got error: traceback (most recent call last): file ""train.py"", line 647, in main(args) file ""train.py"", line 554, in main ap, globalinput, textinput, speakerids) file ""/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in _outputsoutputs) file ""/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in _smi 410.93 thank you!",question,question
997,https://github.com/microsoft/recommenders/issues/997,"[BUG] ValueError: numpy.ufunc size changed, may indicate binary incompatibility","description in the nightly windows pipelines, there is a problem with numpy and sklearn in which platform does it happen?",Error,Error
283,https://github.com/streamlit/streamlit/issues/283,Watchdog error on install (Mac),"i've tried to install it in a mac using pip and pip3 and i get the same error: command ""/usr/local/opt/python/bin/python3.7 -u -c ""import setuptools, tokenize;_3902mpks9lm0000gn/t/pip-install-3cscl6kq/watchdog/setup.py';f=getattr(tokenize, 'open', open)(_3902mpks9lm0000gn/t/pip-record-qr49cvd0/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/0s/rkvsfhzn2930_3902mpks9lm0000gn/t/pip-install-3cscl6kq/watchdog/ i'm using: macos mojave 10.14.5",deployment,deployment
405,https://github.com/deezer/spleeter/issues/405,Support for Python 3.8?,"i notice the pip package is flagged as supporting <3.8, is this due to an incompatibility or has it just not been updated yet?",other,other
359,https://github.com/deezer/spleeter/issues/359,[Bug] Crash after processing a few files,description i'm using spleeter's api and after a while it ends with the error presenter below. step to reproduce 1. installed using `conda` 2. run as python script using api 3. got error below output environment ----------------- ------------------------------- os windows 10 installation type conda ram available 32go hardware spec rtx2080 / i9-9900k,question,question
233,https://github.com/microsoft/recommenders/issues/233,Explore capability of utilizing Azure services for a recommendation system,"when we want to run reco models on big data, some of azure services, like storage or aml will be useful as well as spark on databricks. maybe good to have a quickstart notebook showing how our recommender repo can be setup with those services and utilized.",other,other
460,https://github.com/mozilla/TTS/issues/460,RuntimeError: DataLoader worker (pid 3069) is killed by signal: Killed.,i'm getting the following error mid-training when using lj speech dataset on a gtx 1070 with pytorch 1.5.1 and python 3.6.9 on ubuntu 18.,deployment,question
3532,https://github.com/streamlit/streamlit/issues/3532,st.write(st.session_state) does not correctly print float values ending in .0,"summary when printing `st.session_state` on the screen, float values ending with .0 are printed without it.steps to reproduce code snippet: values ending with .0 should be correctly printed, eg. `1.0` (note: the ** is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: 0.84",other,Error
546,https://github.com/iperov/DeepFaceLab/issues/546,train H64 uses CPU instead of GPU,"hi everybody! i'm new here, as i'm new in deepfacelab use i have a problem that don't know how to solve and hope that somebody here can gime some useful info! when i'm using train h64, i see that my pc is working with cpu instead of gpu i've installed cuda 9,2 i was wondering if working with gpu will be faster that cpu... and also if someone can give some suggestions about the best batch size to use...i wrote 20 and it worked, now i'm trying 12 but i don't personally think that is a problem...but i'll really appreciate some suggestions about here're the features of my pc (i try to write everything, don't know what is useful or not :d) windows 10 intel core i7 mainboard asus g750jz ram 16gb nvidia geforce gtx 880m 4gb expected behavior process should runs on gpu, but it runs only on cpu actual behavior process run on cpu steps to reproduce i've tried to check device.py but don't know properly what parts i can change and in case, how... other relevant information i've installed cuda 10, but with cuda 9.2 it's faster, 14-15 s/iteration while with cuda 10 it was around 30 s... nvidia drivers are updated to latest version",other,question
659,https://github.com/deepfakes/faceswap/issues/659,"Failed to get device properties, error code: 30",** add any other context about the problem here.,other,other
90,https://github.com/deezer/spleeter/issues/90,Configuration file configs/musdb_config.json not found,"description when tried to train it by musdb18, it saying the .json file is not found when it is clearly in that folder step to reproduce 1. installed using `anaconda` 2. run as: spleeter train -p configs/musdbconfig.json not found output traceback (most recent call last): file ""c:\users\osoho\anaconda3\envs\spleeter-cpu\lib\runpy.py"", line 193, in modulemain file ""c:\users\osoho\anaconda3\envs\spleeter-cpu\lib\runpy.py"", line 85, in code file ""c:\users\osoho\anaconda3\envs\spleeter-cpu\scripts\spleeter.exe\_configuration oserror: configuration file configs/musdb_config.json not found environment installation type conda / hardware spec i5-7400 ; 8 gb ; nvidia gtx1050ti additional context",question,question
3604,https://github.com/streamlit/streamlit/issues/3604,"st.image does not render .svg files with other, common tags","as mentioned in , `st.image` breaks when trying to display `.svg` files when there are other top-level tags that aren't svg tags. it's unfortunately reasonably common for an svg file to look something like the example below, which means many downloaded svg files aren't compatible with `st.image` without modification. we should strip top-level non-svg tags when displaying an svg so that this works as expected.",Error,other
153,https://github.com/deepfakes/faceswap/issues/153,Outputting training graphs,once i'm done with my current task i'd like to add a feature to output a graph of the training performance. - add graph depicting epoch vs loss - add max-epoch parameter to be able to run for a specified amount of epochs please assign this to me. i'd like to hear more ideas on this subject.,other,other
685,https://github.com/mozilla/TTS/issues/685,`bidirectional_decoder` appears broken in dev,"with the latest dev (547bfc4ce99e7782f98d0403429d15ec23cdc91d), trying to use `bidirectional_decoder` yields the following error during training: are additional parameters required in the `config.json` file?",Error,question
3608,https://github.com/streamlit/streamlit/issues/3608,Set_page_config Error when using on_change function,"summary when using the multiselect widget, and specifying the `onpagepage_config` ------------------------------- if applicable, please provide the steps we should take to reproduce the bug: 1. run the code snippet as a streamlit web application ****debug info - streamlit version: 0.84.0 - python version: (get it with `$ python --version`) - installed with pip - os version: windows 10 - browser version: google chrome 92.0.4515.107",Error,Error
310,https://github.com/iperov/DeepFaceLab/issues/310,bat files not working,"python 3.7.1 is already installed in my pc i clicked on every bat file it is giving me the same error windows 10, gtx 1070 8gb",Error,question
460,https://github.com/iperov/DeepFaceLab/issues/460,VRAM calculator for faster Model creation,"hi, i am trying around for about 5 hours in the last days do get the biggest possible network to train on my rtx 2080. so i looked around if it is possible to calculate the vram usage of a nn. after some search i found this. i am not really deep in ai so i can't write this feature myself. i think it will make a lot of try and error obsolete. @iperov what do you think? i am looking forward to hearing from you. yours sincerely maxinger15",other,other
80,https://github.com/iperov/DeepFaceLab/issues/80,pickle.unpicklingError:invalid load key，‘0x00’,h128_data.dat is all '0x00' so that error occurs. how can i solve this？,question,question
888,https://github.com/deepfakes/faceswap/issues/888,During Extract: Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED,error on extrace: e tensorflow/streamdnn.cc:329] could not create cudnn handle: cudnnalloc_failed ** extraction log:,deployment,other
722,https://github.com/iperov/DeepFaceLab/issues/722,Some problem about merge Quick96.bat,"** but if i do not use interactive merger, it will work well.how could i handle this?",question,question
1770,https://github.com/streamlit/streamlit/issues/1770,New API: st.set_page_config,"proposal background we want to support the following in streamlit: 1. ability to set page favicon 2. ability to set page title 3. ability to make a page start in wide mode (today you need to toggle it in ""preferences"") 4. ability to make a page start with the sidebar collapsed (today you need to collapse via the ui)must be first st command for all of these settings, we'd prefer if the user sets them near the top of their script. because it would be weird to have an app load, then show a bunch of stuff, change into wide mode, for example.immutable, for now for title and favicon, it makes a lot of sense to support mutation. but it's not what 99% of the users will be doing, so it's ok if we don't support it at first. for wide-mode and expand-sidebar, mutations make a lot less sense. (copied from our internal spec )",other,other
2754,https://github.com/streamlit/streamlit/issues/2754,Excercise update / Error when adding exercise information,"summary upon login, i was unable to update my daily calories (1305). upon submission, i received an error (attached).steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to ' 2. click on the carrot to get to the login page. 3. scroll down to provide username and password. 4. click on the carrot to close the menu. 5. check box to update exercise. 6. in calorie field, type '1305'/and click submit. 7. note the error (2 screen shots attached)expected behavior: after clicking submit, the calories are logged for the appropriate date and a prediction is provided. explain what you expect to happen when you go through the steps above, assuming there were no bugs.actual behavior: explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes - not previously tested with a four digit input debug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",question,other
280,https://github.com/iperov/DeepFaceLab/issues/280,Glass Frame Issue,"expected behavior when training the fake, it appears to have learned glasses frames, but when actual result is rendered out, no side frames on the side of the head are shown.",other,Performance
312,https://github.com/mozilla/TTS/issues/312,Type error in setup_model,"typeerror traceback (most recent call last) in () 4 # load the model 5 numphonemes else len(symbols) ----> 6 model = setupchars, config) 7 8 # load the audio processor typeerror: setup_model() missing 1 required positional argument: 'c'",other,other
529,https://github.com/deepfakes/faceswap/issues/529,Crash at extraction with latest pull,"it seems to crash when initializing the detector. i tried with and without -a fan, with and without -mp and with mtcnn or dlib-cnn: same result. it freezes a little while after the first traceback (freeze_support etc..) then the rest follows. pip freeze: absl-py==0.2.2 astor==0.6.2 bleach==1.5.0 click==6.7 cloudpickle==0.5.3 cmake==3.11.0 cycler==0.10.0 dask==0.18.1 decorator==4.3.0 dlib==19.13.1 face-recognition==1.2.2 face-recognition-models==0.3.0 ffmpy==0.2.2 gast==0.2.0 grpcio==1.13.0 h5py==2.8.0 html5lib==0.9999999 keras==2.2.4 keras-applications==1.0.6 keras-preprocessing==1.0.5 kiwisolver==1.0.1 markdown==2.6.11 matplotlib==2.2.2 networkx==2.1 numpy==1.14.5 nvidia-ml-py3==7.352.0 opencv-python==3.4.1.15 pathlib==1.0.1 pillow==5.1.0 protobuf==3.6.0 pyparsing==2.2.0 python-dateutil==2.7.3 pytz==2018.5 pywavelets==0.5.2 pyyaml==3.12 scandir==1.7 scikit-image==0.14.0 scikit-learn==0.20.0 scipy==1.1.0 six==1.11.0 tensorboard==1.8.0 tensorflow-gpu==1.8.0 tensorflow-tensorboard==1.5.1 termcolor==1.1.0 toolz==0.9.0 tqdm==4.23.4 werkzeug==0.14.1",Error,deployment
2330,https://github.com/streamlit/streamlit/issues/2330,[Layouts] Padding does not work on safari,padding is 0 on safari,deployment,other
2052,https://github.com/streamlit/streamlit/issues/2052,[Layouts] Wrap columns on mobile,"proposal 1: once screen width drops below a certain threshold (600px?), we just stack every column - kinda brute force but simple to reason about, anyways. - might make things like grid layout weirdproposal 2: when a specific column drops below a certain threshold (200px?), wrap that one - problem: how to handle variable-width columns? - problem: 3 same-width columns, but two in line 1 and one in line 2? the second will fill the entire space unless we bound it somehow",Error,question
4117,https://github.com/streamlit/streamlit/issues/4117,Starmix is not tracking en.shopify.dk,"summary tried to pull data from gsc and the host is not showing.steps to reproduce create a report with gsc and try to look for the host en.shopify.dk code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....' ** explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,other
2544,https://github.com/streamlit/streamlit/issues/2544,ModuleNotFoundError: No module named 'streamlit.components',i was trying to do but it doesn't seem to work. its warning is shown in the title. i guess it should be a matter of my version. the current version i am using is 0.73.1. which version should i use?,question,question
255,https://github.com/streamlit/streamlit/issues/255,Assorted bugs when closing Streamlit still running throws error,"summary closing streamlit throws errors.steps to reproduce situation 1: 1 run an example `streamlit run examples/images.py` 2 close browser tab after it opens. 3 error on shell: `attributeerror: 'value` of 17 and the \`maxfoldereventhandler' object has no attribute 'watch'`expected behavior: should not throw errors, should stop when hit `ctrl+c` key. actual behavior: exceptions appear in the terminal.is this a regression? nodebug info - streamlit version: 0.45 - python version: python 3.7/python 2.7.15 - using: conda - os version: ubuntu 18.04 lts - browser version: not specifiedadditional information the environment does not have ** available.",Error,Error
640,https://github.com/iperov/DeepFaceLab/issues/640,Preview window shows only 2 columns,with the version 27.02.2020 the preview window shows only 2 columns (src) tested res: 400 and 512. with 256 it works fine :) (shows all 5 columns),other,other
388,https://github.com/deezer/spleeter/issues/388,[Discussion] Output in FLAC or other formats,"hi, i was wondering if it is possible to to export the output files in other formats than wma. i would be interested in having the files as flac, is it already possible to do so ? i thank you in advance for your time and comprehension towards my request.",question,other
598,https://github.com/deepfakes/faceswap/issues/598,is it possible to change the training size to larger one,"for example, larger than 128x128 like 188x188 or 256x256, thanks for answering",other,other
379,https://github.com/iperov/DeepFaceLab/issues/379,SAE issues in Linux,"i am trying to use the sae model in order to do the conversion. i have trained the model for hours and the training results seem very good. however, i have a problem during the conversion process. this was what happened when i tried to do raw-rgb conversion using the learned mask approach. i didn't see any error in the terminal so i couldn't figure out what the problem was. i have used the command line given in the scripts folder (7sae). i am using python 3.6.6 on linux with exact same environment as mentioned in the installation guide.",other,question
271,https://github.com/mozilla/TTS/issues/271,How much it costs to train the model?,"hi @erogol, do you mind share how much it costs to train this model? like how much $? or how much time on what gpu? e.g. 50k $ on the aws, 100 hours on the v100? thanks in advance!",question,question
2275,https://github.com/streamlit/streamlit/issues/2275,Cannot hash SwigPyObject (or any binded objects),"summary streamlit doesn't knows how to hash swigpyobjects. no problem, you can use `hashfuncs={elasticsearch.elasticsearch: id}` but i cannot do the same for faiss.steps to reproduce what are the steps we should take to reproduce the bug: 1. create a 2. create some streamlit basic app 3. try to put the faiss index in the cache of the function (`@st.cache()`) 4. watch it failexpected behavior: allow to ignore or deal with non python types (for me ignoring them it's okay)actual behavior: traceback is this a regression? nodebug info - streamlit version: 0.69.2 - python version: 3.8.6 - using python venv native module (same problem with no virtual env) - os version: arhclinux kernel zen 5.9.1 - browser version: firefox 82.0additional information thanks in advance, streamlit is really a wondefull tool",question,Error
2561,https://github.com/streamlit/streamlit/issues/2561,Single file upload replacements returning old data,looks like data returned is off by 1. works fine in 0.73.0. caused by #2535. should evaluate the impact on other components given the cause.,Error,other
131,https://github.com/iperov/DeepFaceLab/issues/131,Deepfacelab problem tensorflow problem (NoneType),expected behavior i try to convert h128 model but it said actual behavior i swear that i don't delete anything in the workspace include any png and dst or src file but it failed other relevant information - ** 3.6.5,other,question
511,https://github.com/mozilla/TTS/issues/511,Re-saving released models to solve the module name issue as loading checkpoint.,"due to the recent module name relapse, you might encounter pickling issue as you load model checkpoints. i tried to provide a temp solution with a recent commit but it is better to resave release tts models to solve the issue completely. here is the part with a temp fix :",Error,other
1106,https://github.com/streamlit/streamlit/issues/1106,Session states,"we deployed a streamlit app to azure via docker with web app on top. we had about 20 users accessing the app at the same time but we noticed weird behaviour with streamlit (images repeating etc), a few performance issues too. my question is does streamlit create individual sessions per user, it seems as if every connection from our 20 users was probably coming in as a single session. i may have answered my question by seeing this gist: but just wanted some certainty.",Error,question
579,https://github.com/deezer/spleeter/issues/579,Spleeter on PythonAnywhere,"hello, i’ve tried to install spleeter on a server of pythonanywhere using a pro account with 10 gb available. however after the installation when trying to use it to extract the stems of an input audio it gives this error: tensorflow/core/platform/env.cc:351] check failed: -1 != path_length (-1 vs. -1) is there a way to fix this?",question,question
398,https://github.com/iperov/DeepFaceLab/issues/398,"one few shot implementation, face swapping video from single image",recently one/few shot approaches look like they are getting good success with face swapping in video. a implementation like can do face swapping in video from a single image without training face pairs for any face. can this approach be used to improve face swap videos?,question,other
1016,https://github.com/microsoft/recommenders/issues/1016,[BUG] Not able to restore xDeepFM model,description not able to restore trained xdeepfm models from tensorflow save files. ******** any help with this is much appreciated - thank you! in which platform does it happen? all platforms how do we replicate the issue? try restoring a saved xdeepfm model expected behavior (i.e. solution) the model should be restored from files; making it usable in the run_eval & predict functions.,Error,other
356,https://github.com/deepfakes/faceswap/issues/356,"While converting, default option 'None' for sharpen image raises error","** - ... (for example, installed packages that you can see with `pip freeze`)",Error,other
1523,https://github.com/microsoft/recommenders/issues/1523,[ASK] Questions about Building and Running with Docker,"description after i built and ran with docker according to this instruction, i open the jupyter notebook at to run an example ""dkndive"" but receive the following error: .",question,question
3598,https://github.com/streamlit/streamlit/issues/3598,Initializing default session_state value for a selectbox raises ValueError if options change,"summary if i create a with a key that stores the selected option in sessionstate.item` is initialized, in this case to `e`, when the list changes (from the first selectbox), a valueerror is raised. the report is then non-operational. case two if `st.sessionstate.item = liststate variable is initialized or not. ** when the selectbox's session_state variable is intiialized, the selectbox raises a valueerror when the options change and the selection is not in the new options.is this a regression? not that i'm aware of. session state is new in streamlit 0.84.debug info - streamlit version: streamlit, version 0.84.2 - python version: python 3.8.11 - using conda? pipenv? pyenv? pex? just pip - os version: debian buster (in a python:3.8 container) - browser version: safari 14.1 on macos mojave 10.14.6additional information none.",Error,Error
1072,https://github.com/streamlit/streamlit/issues/1072,Button widget Loses its state upon re-rendering,summary there's a serious issue with **expected behavior: btn1 should keep its state.actual behavior: everything re-renders and button loses its state.is this a regression? no is this an expected behaviour?,other,Error
477,https://github.com/mozilla/TTS/issues/477,Runtime exception due to scipy.io.wavfile import  problem,"in both the master and dev branches (as of a few days ago) i see the following failure running the test_audio.py nosetests: i also saw this exception being thrown running train.py in master when it gets to the eval stage and tried to write out the wav files. i'm running on ubuntu 18.04 and python 3.8.3. i'm guessing this is a recent change in behavior of python imports, if this hasn't been a problem till now. i have a couple candidate prs to fix this in master and dev; will submit.",deployment,other
873,https://github.com/deepfakes/faceswap/issues/873,install error,when i click on launcher =>,question,other
359,https://github.com/iperov/DeepFaceLab/issues/359,DeepfakeLab.  Crash    2) extract images from video data_src,"downloaded from rutreker.org. in the “workspace” folder are the video “tony stark” and “just do it”. i run the file “2) extract images from video 'and nothing happens. when you press any button, the console closes sorry my english",Error,question
1665,https://github.com/streamlit/streamlit/issues/1665,"streamlit on WSL in windows, running script always shows the demo on browser","good day, i am using streamlist on wsl ubuntu on windows, installation went perfect. then i create a hello.py with some code following your tutorial, then do streamlit run hello.py, and instead of getting the result of the code, i get the streamlit demo page again on localhost:8501, it always shows the demo, regardless of what script i run, thank you",question,question
541,https://github.com/mozilla/TTS/issues/541,[Help needed] TTS Community Survey - Let us know your feedback,"i've created this short survey to have a better understanding of the tts community, and use cases of mozilla tts. this way we can plan the future of the project in more clarity and prioritize the work in an order that would serve the community in a more effective way. if you use mozilla tts please spare your 5 mins and let us know what you think.",other,other
1804,https://github.com/streamlit/streamlit/issues/1804,st.video start_time seem not work,"summary i set starttime==18,but it start from 0debug info - streamlit version: (get it with `$ streamlit version`) streamlit, version 0.64.0 - python version: (get it with `$ python --version`) python 3.6.5 :: anaconda, inc. - using conda? pipenv? pyenv? pex? anaconda - os version: win10 - browser version: google chrome €",Error,Error
26,https://github.com/streamlit/streamlit/issues/26,Frontend: do something reasonable with handleMessage errors,"`websocketconnection.handlemessage` can throw errors for a variety of reasons. we're currently swallowing those errors, which leaves a running report in a broken state. (we may want to just move to the ""disconnected forever"" state and tell the user that everything blew up.)",Error,other
37,https://github.com/mozilla/TTS/issues/37,NameError: name 'mode' is not defined,"hi, when i am running the benchmark with checkpointsize, config.nummels, config.r) file ""/media/hamza/local disk/projects/untitled folder/tts/models/tacotron.py"", line 20, in _dim, r) file ""/media/hamza/local disk/projects/untitled folder/tts/layers/tacotron.py"", line 203, in __ self.mode = mode nameerror: name 'mode' is not defined thanks",Error,Error
1936,https://github.com/streamlit/streamlit/issues/1936,Components static files / encoding on windows,"summary (windows only) when serving component static files, the encoding is not specified. it falls back to the system's default, which can fail to decode the file.steps to reproduce what are the steps we should take to reproduce the bug: 1. set windows system encoding to japanese 2. install hiplot with `pip install hiplot` 3. run the hiplot demo example ( behavior: bundle js file served.actual behavior: fails with an exception about encoding. is this a regression? nodebug info this is where the file read happens - encoding should be specified. original issue:",Error,Error
296,https://github.com/microsoft/recommenders/issues/296,SAR+ Requires Crossjoin,"it seem's sar+ requires cross join be performed. this operation scales very poorly in spark, which is why you have to turn it on specifically. can this be optimized out? (it's been awhile, but is this a spot where you don't really need the whole product matrix, but really just an upper triangular section? if so i think that operation would perform much better...)",other,question
428,https://github.com/iperov/DeepFaceLab/issues/428,what's the difference between DEV_FUNIT and TrueFace model?,what's the difference between dev_funit and trueface model? both model use the funit network.,question,question
569,https://github.com/mozilla/TTS/issues/569,multiband-melgan train time too long,may i ask why the `multiband-melgan` train epoch takes so long ?,question,question
1046,https://github.com/streamlit/streamlit/issues/1046,Alpha Numeric Slider,problem using alpha numeric values instead of numerics example: freezing --- cold --- warm --- hot --- boilingadditional context,other,other
1864,https://github.com/streamlit/streamlit/issues/1864,Create and Replace Table Component,"we know of one locations for the table component (and subcomponents). * instead, we will use baseweb's goal is to mimic the same look and feel as before and announce if there are any changes that are unattainable. please display before and after images in pull requests.",other,other
121,https://github.com/deezer/spleeter/issues/121,[Bug] segmentation fault on Ubuntu,"description i m trying to split music which i am downloading and converting from youtube via youtube-dl library , but while using such file spleeter gives me segmentation fault. when i m converting audio_example given in project it works fine step to reproduce 1. run 'youtube-dl --extract-audio --audio-format mp3 2. run it into ' spleeter separate -i /home/tornikeka/resources/io.mp3 -o output' output environment ----------------- ------------------------------- os linux installation type conda / ram available xgo hardware spec cpu / etc ... additional context ffmpeg version 4.2",Error,question
1287,https://github.com/streamlit/streamlit/issues/1287,Refactor flaky e2e test locations,"flaky tests currently live alongside non flaky ones as siblings in the root folder. move them under a parent folder using one of the following approaches - e2e/flaky/specs, e2e/flaky/scripts - e2e/specs/flaky/foo.spec.ts, e2e/scripts/flaky/foo.py - e2e/specs/flakyfoo.py questions: non flaky tests go where in the first two options?",other,other
528,https://github.com/iperov/DeepFaceLab/issues/528,Can't train after Dec 23 changes,"i can't train or convert even with a new model after some commits on dec 23. the error is this: ... time to end session: 12 hours traceback (most recent call last): file ""deepfacelab/main.py"", line 19, in file ""/usr/lib/python3.6/multiprocessing/context.py"", line 242, in setmethod runtimeerror: context has already been set",other,question
2189,https://github.com/streamlit/streamlit/issues/2189,Int64 column with NA Can Not Be Displayed in Streamlit,"i have a column in pandas dataframe with type of int64, and there are some rows with na values. however, such dataframe cannot be displayed ` a = pd.series([1,2,3,4,pd.na]) a = a.astype('int64') st.table(a) ` the error message is like:",Error,Error
473,https://github.com/mozilla/TTS/issues/473,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","> using cuda: true > number of gpus: 1 > git hash: 599c2db > experiment folder: /home/ngthuong/desktop/ljspeech/ljspeech-july-31-2020rate:22050 > numlevelshiftlengthlevelfreq:513 > power:1.5 > preemphasis:0.0 > griffiniters:60 > signalnorm:true > melfmax:8000.0 > maxnorm:true > dosilence:true > trimsoundpath:none > hoplength:1024 > navgdict, globaldict = criterion(postnetoutput, melprojects/ai/tts/venv/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 722, in impl result = self.forward(input, * (sigma ** 2))) runtimeerror: expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! how to fix it?",deployment,question
484,https://github.com/iperov/DeepFaceLab/issues/484,Hi some help with this error?,"hi! i'am trying to train h128 but after setting everything this showes up: traceback (most recent call last): file ""c:\users\dexxt\desktop\deepfake\deepfacelab10.1internal\deepfacelab\samplelib\samplegeneratorface.py"", line 139, in batchcudaavx\cudaavx\warpwarpbootstrap file ""multiprocessing\process.py"", line 93, in run file ""c:\users\dexxt\desktop\deepfake\deepfacelab10.1internal\deepfacelab\utils\iterfunc file ""c:\users\dexxt\desktop\deepfake\deepfacelab10.1internal\deepfacelab\samplelib\samplegeneratorface.py"", line 141, in batchcudaavx\workspace\data0.jpg. error: traceback (most recent call last): file ""c:\users\dexxt\desktop\deepfake\deepfacelab10.1internal\deepfacelab\samplelib\samplegeneratorface.py"", line 139, in batchcudaavx\cudaavx\warpwarp_params accepts only square images.",question,question
660,https://github.com/streamlit/streamlit/issues/660,Objects we know about should be hashed properly,"there are some types which we檙e not hashing properly, which should be hashed differently. - dicts: we should recursively hash their .items() aka key value pairs. - iterables: we should just iterate into them and hash whatever we find. right now, only lists and tuples work. - look at the list and see if any other obvious types are missing",other,other
560,https://github.com/streamlit/streamlit/issues/560,Material UI,can we expect to have ui designing elements in streamlit. just saw this flask based application on github and started wondering.... i want stream lit to have everything 槄 follow the link,other,other
3305,https://github.com/streamlit/streamlit/issues/3305,st.selectbox not gives expected output,summary if the index of a pandas data frame is in random order and when we pass one of the columns of the data frame then it does not give expected output and if we pass column values then it gives correct output. screenshot of webpage: screenshot of code to produce this behavior:,other,Error
605,https://github.com/deezer/spleeter/issues/605,[Bug] Unable to process multiple tracks,"- [x] i didn't find a similar issue already open. - [x] i read the documentation (readme and wiki) - [x] i have installed ffmpeg - [x] my problem is related to spleeter only, not a derivative product (such as webapplication, or gui provided by others) description it only exports one track (the first track) step to reproduce 1. installed using `pip` 2. run as `spleeter separate -p ""c:\users\windo\documents\basemwf"" -f {filename}-{instrument}.{codec} -d 7200 -c mp3 -b 320k --mwf -b tensorflow ""c:\users\windo\music\track1.mp3"" ""c:\users\windo\music\track2.mp3"" ""c:\users\windo\music\track3.mp3""` 3. got `one track` processed . output environment ----------------- ------------------------------- os windows installation type mini conda & pip ram available 6go",question,other
944,https://github.com/streamlit/streamlit/issues/944,Enable support for facets,problem some libraries rely on to visualize data in juypiter: streamlit does not support it. examples are tensorflow tfdv and tfma.solution investigate how we can support facets.additional context,other,other
513,https://github.com/streamlit/streamlit/issues/513,Proposal: Embedding Obseverable Notebooks in Streamlit,"this is a proposal for adding `st.observable` to streamlit. you would not only be able to embed into a streamlit app, but also inject your own data from python into the observable notebook, and use the observable notebook as a widget in the streamlit app. problem - as a user, i want to use custom elements that i could only currently do with `unsafehtml`. - as a user, i want the option to have complete control of the charts and graphics in my streamlit app, outside of the current builtin charts and integrations. - (stretch) as a user, i want to be able to create my own custom widgets (forms, sliders, pickers, etc.) with my own interactions and user interface.solution (quick observable notebook review - every notebook is composed of cells, and each cell either has a unique name or is anonymous. the cells are and executed only when needed) with this proposal, there will be 3 main use cases for embedding observable notebooks in a streamlit app: 1. embedding plain observable notebooks with no modifications ** - allow users to and use it in python - with any inputs and custom css - - for color pickers, map coordinates pickers, local file chooser, passwords, etc.potential code samples `st.observable` definition things to consider (security, required setup, etc.) - since observable notebooks are basically just running arbitrary javascript, each notebook embedding should be in an iframe. this won't solve all security problems, but it'll at least sandbox the notebook away from the streamlit window/dom. - the iframe should ideally have a different origin, for security purposes. maybe streamlit could invest in a domain like `` (that could be configured to a self hosted option, like `st.observable(""@/b"", iframechart` where it can be later re-defined to another streamlit element, like this: but, if we want to be able to observe values of an embed notebook, then it also makes sense for it to act like a widget like `slider` or `textchart` or `text`, but can also be a streamlit widget like `slider` or `text_input`. resources - from observablehq - from me - from me - defining how a host can create and interact with an observable notebook in an iframe (wip) - from me, implementation of the above protocol (also a wip). something like this would have to be built in `/frontend` --- note: this is something i'd love to work on! i've made a lot of tooling/packages around observable notebooks, and i'd love better integration with then in streamlit (and i need some prs for hacktoberfest).",other,other
378,https://github.com/microsoft/recommenders/issues/378,Nonnegative = false in quick start ALS,"is affected by this bug? quick start als notebook. expected behavior (i.e. solution) for recommenders nmf is less common, so `nonnegative = false` is typical.",other,other
1391,https://github.com/microsoft/recommenders/issues/1391,conda env create Access Denied,"hi all, thanks for your work. i'm getting exception while executing the conda env create -f reco9p36c\\black.exe' consider using the `--user` option or check the permissions.**",question,question
2404,https://github.com/streamlit/streamlit/issues/2404,[File Uploader] Progress bar does not work in safari,this works in chrome but not safari. initial debugging shows that the our progress bar component is being called. need to investigate more to see if there is an issue with rendering the baseweb progress bar too frequently. for some reason it works when deployed on s4a in safari (probably due to iframing),Error,other
193,https://github.com/iperov/DeepFaceLab/issues/193,CUDA9.2SSE  cudnn has crash,"deepfacelabcuda9.2sse032019 has carsh，error log say: could not create cudnn handle: cudnninternal9.2.148size : 4 //== == sortyaw : false //== == randomae : false //== == pixelexecutor/cuda/cudastatuserror 2019-03-15 19:42:35.943660: e tensorflow/streamdnn.cc:373] could not create cudnn handle: cudnninternalexecutor/cuda/cudastatuserror 2019-03-15 19:42:35.955916: e tensorflow/streamdnn.cc:373] could not create cudnn handle: cudnninternalinternal\deepfacelab\mainscripts\trainer.py"", line 75, in trainerthread file ""c:\temp\deepfacelabcuda9.2sse\oneinternal\deepfacelab\models\modelinternal\python-3.6.8\lib\site-packages\keras\engine\training.py"", line 1217, in trainbatch file ""c:\temp\deepfacelabcuda9.2sse\backend.py"", line 2715, in _internal\python-3.6.8\lib\site-packages\keras\backend\tensorflowcall file ""c:\temp\deepfacelabcuda9.2sse\internal\python-3.6.8\lib\site-packages\tensorflow\python\framework\errorsimpl.unknownerror: failed to get convolution algorithm. this is probably because cudnn failed to initialize, so try looking to see if a warning log message was printed above. done. 请按任意键继续. . .",question,question
488,https://github.com/iperov/DeepFaceLab/issues/488,How can I donate the money by ALIPay,"dear iperov: i am grateful for your work. it brings a lot of fun to us. as a chinese, i have to say donate money by paypal is not convinent. can i use alipay to donate money to your account? i notice there is a alipay account in your manual before oct. 08. best wishes,",other,question
401,https://github.com/iperov/DeepFaceLab/issues/401,AVATAR training: dramatically slow speed,"please look into my log, as i think it's not working properly. the iteration speed ( in ms ) looks ok, but the time between 2 iterations is ** than it should be. my configurations is:",Performance,Performance
2281,https://github.com/streamlit/streamlit/issues/2281,Streamlit crashes/reset; connection to server lost,"from: both on my local machine, and when my script is deployed on streamlit via my github, when the very last checkbox is selected, it crashes. on the deployed app, the app refreshes. on my local machine, my terminal closes out of streamlit. i not sure what going on. the only thing the check box does is display a selectbox that access keys within a dictionary. any thoughts/insights would be appreciated! thanks! here my github link: my deployed app: 5",question,other
1180,https://github.com/microsoft/recommenders/issues/1180,[FEATURE] add output zip file for MIND competition,description add code in newsrec jupyter script to output zip file requested for mind competition. expected behavior with the suggested feature add code in naml nrms lstur npa jupyter code to output mind prediction.zip,other,other
329,https://github.com/deepfakes/faceswap/issues/329,Unexpected process termination = corrupted model,"i thought that since the weights are saved fairly regularly an unexpected power loss wouldn't be catastrophic however i lost power the other day and open reopening the training the model was completely messed up, had to start from scratch. i don't know if i was just extremely unlucky with the timing of the power loss or if this is regular behavior but either way it'd be nice if the model file was more secure.",other,other
786,https://github.com/deepfakes/faceswap/issues/786,issue converting with filter,"hi, i'm trying to run convert with a face filter (to process/not process certain person) and it shows the following error. (it runs no problem if i don't include the face filter) face"").lower(), attributeerror: 'namespace' object has no attribute 'detector'",Error,question
76,https://github.com/deezer/spleeter/issues/76,[Discussion] A Fool On The Hill : Flute and Voice indistinguishable ,"spleeter is great. try to split voice and music from ""a fool on the hill"" from the beatles. you'll notice that spleeter is not able to separate the flute from the voice.",other,other
60,https://github.com/iperov/DeepFaceLab/issues/60,What is the full name of LIAEF and MIAEF,"hi, i'm an ai researcher, too. very curious about the full name of ** stands for? thanks for reply~",question,question
1238,https://github.com/streamlit/streamlit/issues/1238,How to put multiple apps together,such as `streamlit hello`,question,question
628,https://github.com/iperov/DeepFaceLab/issues/628,face reenactment feature,"do you plan to implement ""few shot face reenactment"" feature? in github there are several projects",other,other
2001,https://github.com/streamlit/streamlit/issues/2001,Formally support Python 3.9,"python 3.9 will be released approximately 2020-10-05. if we want to maintain a standard of only supporting the last 3 python minor versions, we should begin testing that streamlit works with python 3.9. python 3.9 rc1",other,other
2028,https://github.com/streamlit/streamlit/issues/2028,"add new item in selectbox, but it doesn't show right away","i have a selectbox which shows all objects in a directory, and add new file in that directory, but it won't show it until click the one of option in selectbox. import streamlit as st import pathlib dict1 = {pathlib.purepath(d).name: d for d in pathlib.path(_input('please input new item') if newitem: with open(newitem, 'w') as f: f.write(newitem)",question,other
611,https://github.com/deezer/spleeter/issues/611,[Bug] The INFO won't show up until I press ctrl + c,how can i get information to be displayed as it is executed？,question,question
176,https://github.com/deepfakes/faceswap/issues/176,"CONVERT super slow, is not using the GPU ""1080TI"" when it should!","> python faceswap.py convert -i c:/faceswap/a -o c:/faceswap/a/merged -m c:/faceswap/models -t gan -c gan -d cnn i'm very far away from being a python expert, but if you look at the anaconda window, it say **",question,question
289,https://github.com/streamlit/streamlit/issues/289,Dark theme option,it would be great if you could add a dark theme option.,other,other
130,https://github.com/streamlit/streamlit/issues/130,st.cache hashing fails when script is run from certain folders,"i don't have the user's actual script, but here is what he's seeing:when cwd is ~ 1. cd ~ 2. streamlit run ~/foo.py what happens: streamlit crashes with the error below.when cwd is /home 1. cd /home 2. streamlit run ~/foo.py what happens: same error.when cwd is ~/anaconda3 1. cd ~/anaconda3 2. streamlit run ~/foo.py what happens: no error!when cwd is ~/anaconda3/envs 1. cd ~/anaconda3/envs 2. streamlit run ~/foo.py what happens: no error!the actual error",Error,question
194,https://github.com/iperov/DeepFaceLab/issues/194,[Linux] Fail to run on Linux without reverting linux fix,"if i try run after straight checkout i get 'typeerror: can't pickle weakref objects' i saw there was commit for fixin linux, but it seems to be opposite, if i remove 'multiprocessing.setmethod(""spawn"")' again from main.py it works. so unless 296e8a111ad2cdd59e95de468c1220738eb907ba is necessary for windows i think it should be reverted to make it work on windows",deployment,deployment
139,https://github.com/deepfakes/faceswap/issues/139,'WindowsPath' object has no attribute 'encode' when running train,"i just updated latest version. but failed to use any models on windows10. i am sure all things work well before i update. after updated, even replace model still can't fix this problem. actual behavior output as follow： model a directory: d:\project5\datab\aligned training data directory: d:\face\project5\models loading data, this may take a while... using live preview loading model from modela/aligned -b d:/project5/data_b/aligned -m d:/face/project5/models -p other relevant information - ** latest version",Error,question
1199,https://github.com/microsoft/recommenders/issues/1199,[BUG] negative_feedback_sampler does not allow user defined col_user/col_item,"description there is a bug in the negativesampler function although the function includes parameters colitem, these are not passed through to userpairs in line 324. this causes an error if non-default values for colitem are passed to negativesampler ('userid' and 'itemid' respectively). in which platform does it happen? azure linux dsvm how do we replicate the issue? call negativesampler with any values other than colitem=""itemid"" expected behavior (i.e. solution) user should be able to pass user-defined column names into negativesampler",Error,Error
53,https://github.com/deezer/spleeter/issues/53,[Discussion] Quality Affected,"i really liked the result of this tool, but there is something that bothers me and i hope the developers may be fixing it in the future. i noticed that in the final file, the frequencies are cut in the spectrogram, which means a loss of quality in the audio, making it more muffled. i hope if possible this will be fixed and it managed to maintain the lossless quality of the file .. this would make the tool excellent!! original file:",other,other
197,https://github.com/iperov/DeepFaceLab/issues/197,"If I change the resolution in an old SAE model, what will happen?(eg. changing from 128 to 256)","your design does not allow to change the resolution once starts training. i wonder if i edit the model.py and make it work, what will happen to the training model?",question,question
550,https://github.com/mozilla/TTS/issues/550,Ground Truth alignments using ExtractTTSSspectrogram are not working,"hi, i tried to extract ground truth spectrograms to train pwgan and see if i can hopefully get better performance, so i used the extractttsspectrograms notebook to grab them, using my tacotron2 model; however, when i try to start training i get an assertionerror `meloutputs, alignments, stopoutputsbackward = model.forward(textlengths, mel_input)`",question,question
547,https://github.com/mozilla/TTS/issues/547,TFlite and Double Decoder Consistency,"hi, i think there might be a bug with tflite: it doesn't seem to have double decoder consistency, the results at the bottom of this (tflite) differ greatly from this (tf+ddc). it is using the same models and configs from the ddc notebook. is it a limitation in tflite, or is it unintended? what i have tried: 1: running this with the provided tflite models. 2: generate the tflite models myself using this . it crashes at because of a version mismatch, but i export the models to the first notebook. any suggestions/ideas?",question,other
472,https://github.com/streamlit/streamlit/issues/472,Altair grouped bar chart demo doesn't display,"summary the altair grouped bar chart demo ( doesn't work on streamlit. instead of displaying a graphic, nothing is displayed when it is passed into `st.write`.steps to reproduce debug info - streamlit version: streamlit, version 0.48.1 - python version: python 3.7.4 - using conda? pipenv? pyenv? pex? conda - os version: macos 10.14.6 - browser version: chrome 77.0.3865.120, no ad blockersadditional information thanks for looking into it!",other,Error
1732,https://github.com/streamlit/streamlit/issues/1732,File uploader error (maybe?),summary using st.filerunfile = st.filemethod,question,other
829,https://github.com/microsoft/recommenders/issues/829,[BUG] Update readme of o16n,"description in the o16n readme we only have the als notebook. two new notebooks that we did are missing. also, we added some content in the readme, which is not common to all the notebooks. should we put this content in the als notebook? was it @yueguoguo who did this workflow right? not sure if we should move it to the readme of notebooks.",question,Error
300,https://github.com/deepfakes/faceswap/issues/300,Suggestion: 3D Face Reconstruction via Direct Volumetric CNN Regression,"ala: this achieves good results from a single image, imagine what could be done with larger datasets. could be used for missing angles.",other,other
491,https://github.com/deepfakes/faceswap/issues/491,Feature request: Name the images based on position in the image (Option) / or sorting tool feature,"example you have two people in the frame. left one would always be marked as 1.png etc.. now it seems random on how 1.png are marked. or sorting could be done taking advantage of the alignments file, where the positions are?",other,other
564,https://github.com/microsoft/recommenders/issues/564,[FEATURE] SETUP for Azure Databricks uses 'zip' command not available on Windows,"description the setup instructions here, say to run 'zip' on your local computer. if you're running windows, this won't work. expected behavior with the suggested feature for those users running windows, describe the steps they should use to create the archived file. my workaround will probably be to use a linux dsvm that i have already created (and downloaded the recommendations repo) and run the 'zip' command there. other comments",other,other
511,https://github.com/deezer/spleeter/issues/511,[Discussion] How to use/export model after finished training?,"hi, i just finished training on the musdb18hq dataset, and having those files as result",question,question
2014,https://github.com/streamlit/streamlit/issues/2014,Bokeh Column Layout not displayed,"summary bokeh column layout are not being displayedsteps to reproduce execute the code belowexpected behavior: column layout should workactual behavior: other layouts are working finedebug info - streamlit version: streamlit, version 0.67.0 - python version: 3.7.4 - using pyenv - os version: linux mint 19 - browser version: google chrome 84.0.4147.125 additional information",Error,Error
1045,https://github.com/microsoft/recommenders/issues/1045,[BUG] Test error windows gpu with asyncio,description link to full logs: in which platform does it happen?,deployment,question
1067,https://github.com/microsoft/recommenders/issues/1067,[ASK] [HELP] Using NCF how to get top 10 recommendations given a user_id and item_id?,description i built a recommender system using ncf. my question: is there a way to have a function takes in both userid and gives recommendations based on the userid? example: let say userid 2345. how to give top 10 item recommendations considering the userid 2345? any idea how to implement this using implicit ncf model? other comments,question,question
1135,https://github.com/microsoft/recommenders/issues/1135,[FEATURE] Add news scenario,description add news scenario md expected behavior with the suggested feature add news scenario description in scenario/news/,other,other
296,https://github.com/deepfakes/faceswap/issues/296,"Closed Eyes, Open Mouths and Teeth","more of a discussion thread rather than an issue ... but has anyone looked into how we can improve the below mentioned features. ** teeth is often rendered as a series of white pixels with no black pixels in between to make for the divisions or give texture. this is not a problem generally when the subject is further away from the camera but for closeup shots, it ends up looking really bad. -------- these are some of the glaring issues in terms of output that we face right now. i am thinking of working on some methods to improve these conditions. just wanted to know if any of you guys did any work in these aspects and if anyone had any idea that might be good to check out to address these issues and improve the model.",question,other
1543,https://github.com/streamlit/streamlit/issues/1543,Create a config option to toggle websocket compression in Streamlit server,"problem pr #1506 enabled websocket compression in streamlit server by default. it's a great feature and while it works well when connecting to the server directly, some reverse proxies may not support the websocket compression, making the app unaccessible.solution ** create a new boolean config option `server.enablewebsocketcompression` to toggle websocket compression. it should be enabled by default, and optionally users can override and disable it.",Error,other
377,https://github.com/deepfakes/faceswap/issues/377,"After latest clone, convert shows images being generated but target dir is empty","i cloned the latest version of faceswap yesterday and it won't convert frames at all. or rather, they don't end up in the specified folder. i've tried the gui as well and cannot seem to get any of my frames converted no matter the model i use.",question,question
467,https://github.com/deezer/spleeter/issues/467,error,"(base) c:\users\desktop\desktop\spleeter-master>spleeter separate -i exemple.mp3 -p spleeter:2stems -o output traceback (most recent call last): file ""c:\users\desktop\miniconda3\scripts\spleeter-script.py"", line 10, in file ""c:\users\desktop\miniconda3\lib\site-packages\spleeter\_toseparategetcore\python\training\saver.py"", line 1277, in restore valueerror: can't load save_path when it is none.",other,other
335,https://github.com/streamlit/streamlit/issues/335,"Improve streamlit.linechart to have functionalities similar to Tensorboard's ""AddScalar"" plots","tensorboard allows us to very easily create dynamically updating 2d plots by just passing 3 values to a method. this results in a highly interactive plot with various useful tools to analyze the data (filtering , comparing to past runs , smoothing values etc) considering during training almost everyone plots their loss and accuracy values, adding a similar feature would be very-very helpful since it would save us from dealing with figures and plotting libraries. streamlit's linechart is already very similar to this since it already supports dynamic row insertion and updating, what i am suggesting is basically adding additional options to already existing linechart such as ""smoothing"" and ""selecting columns to be displayed"" etc from the browser. this could work by passing an additional argument to line chart constructor, such as `chart = st.linevisualization_options=true)`",other,other
469,https://github.com/mozilla/TTS/issues/469,"Hi, everyone! From where i can get the  checkpoint_420000 checkpoint_420000.pth.tar  like files. Is there any tutorial to setup up these all models?","questions will not be answered here!! please consider posting on page if your issue is not directly related to tts development (bugs, code updates etc.). you can also check for common questions and answers. happy posting!",other,other
3991,https://github.com/streamlit/streamlit/issues/3991,ModuleNotFoundError: No module named 'streamlit.hashing',"summary streamlit, version 1.1.0 can not find streamlit.hashingsteps to reproduce code snippet: modulenotfounderror: no module named 'streamlit.hashing'",other,other
234,https://github.com/deepfakes/faceswap/issues/234,out of memory when extract after today update,"i have extracted faces from 2000+ images after today update and it went great. then i tried to extract another set of 100 images and program failing saying ` reason: error while calling cudamalloc(&data, n) in file d:\fapp\dlib-master\dlib\dnn\cudaptr.cpp:28. code: 2, reason: out of memory` (images are same size) any idea why this happening?",deployment,question
284,https://github.com/mozilla/TTS/issues/284,Which Released Models is for branch Master and Errors for Tacotron2-iter-260K with Master code,"hi, i would like to know what is the released models for branch master. i used tacotron2-iter-260k but got torch.size mismatch errors like: best regards, paul",question,question
268,https://github.com/iperov/DeepFaceLab/issues/268,S3FD inaccuracy of easy shots.,"it appears that some issue was introduced in some of the later iterations of the s3fd extractor. namely, some seemingly easy, unobstructed, clear shots are not being detected correctly. this is happening almost invariably in profile shots and in particular yaws, orientations: about 45 degrees looking left or 45 looking right and about 5 to 30 degrees looking down. here are some examples: in the above one the left eyebrow and eye and the nose are incorrectly detected as if the face is facing more to the left than what it actually does. here is another example: and another: and another:",Performance,Performance
621,https://github.com/deezer/spleeter/issues/621,httpcore.ReadTimeout and httpx.ReadTimeout When trying to download pretrained model,"- [x] i didn't find a similar issue already open. - [x] i read the documentation (readme and wiki) - [x] i have installed ffmpeg - [x] my problem is related to spleeter only, not a derivative product (such as webapplication, or gui provided by others) description 1. ran `spleeter separate -o audio_output -p spleeter:5stems-16khz rand.mp3` environment ----------------- ------------------------------- os windows installation type pip ram available 8 hardware spec integrated graphics/ intel i5 - 2430m",question,question
287,https://github.com/streamlit/streamlit/issues/287,Mapping and DataFrame demo apps through errors,"summary while going through the demos provided in `hello` app i have encountered problems with two out of four (mapping and dataframe demo apps) through errors.steps to reproduce 1. mkvirtualenv streamllit 2. pip install streamlit 3. streamlit hello 4. click on `mapping demo` or `dataframe demo`expected behavior: i would expect demos would be shown in the right pain. it works as expected for the first two apps (animation and plotting demo).actual behavior: i get notifications, first and then 6 of these and at the end there is a traceback: is this a regression? tried to launch it for the first time.debug info - streamlit version: 0.47.4 - python version: python 3.6.8 - using virtualenv with virtualenvwrapper - os version: ubuntu 18.04 - browser version: chrome version 77.0.3865.90",Error,question
687,https://github.com/mozilla/TTS/issues/687,How about the RTF of glowTTS  + mul-melgan?,"questions will not be answered here!! help is much more valuable if it's shared publicly, so that more people can benefit from it. please consider posting on page or matrix if your issue is not directly related to tts development (bugs, code updates etc.). you can also check for common questions and answers. happy posting!",other,other
651,https://github.com/streamlit/streamlit/issues/651,Recontact users reporting caching bugs and tell them about the new fix,message all 渦nderspecified usersdependency - #597 merged and released - #551 merged and released,other,other
856,https://github.com/streamlit/streamlit/issues/856,Could we customize the window title?,"hello, could we customize the window title in streamlit?",question,question
252,https://github.com/iperov/DeepFaceLab/issues/252,Batch Size more than 8 causes an error,"hello, first of all, thank you so much for your work and sharing! i've been writing a bunch describing everything, but it was getting too long, so please allow me to just summarize: intel(r) core(tm) i7-9700k cpu @ 3.60ghz (8 cpus), ~3.6ghz ram: 16gb gc: gtx 1660 ti vram: 6gb os: windows 10 as soon as i installed windows 10, i installed deepfacelab by deepfacelabcuda10.1sse042019.exe file. everything works fine, but when i try to have a batch size like 16, 32, 64, 128 it doesn't work and only the batch size 8 and lower works. i'll attach the error message at the end. i believe my computer is good enough to handle a batch size much higher, but i can't seem to figure out the problem. i've googled and googled and read and tried stuff but i can't seem to fix the issue. i'm sorry if this happens to be a dumb question. thank you! -error message when tried with batch size 128 at h128- starting. press ""enter"" to stop training and save model. error: oom when allocating tensor with shape[128,1024,32,32] and type float on /job:localhost/replica:0/task:0/device:gpu:0 by allocator gpubfc hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. traceback (most recent call last): file ""d:\deepfakes\deepfacelabcuda10.1sse\internal\deepfacelab\models\modelbase.py"", line 362, in trainiter file ""d:\deepfakes\deepfacelabcuda10.1sse\df\model.py"", line 73, in ontrainoneiter file ""d:\deepfakes\deepfacelabcuda10.1sse\oninternal\python-3.6.8\lib\site-packages\keras\backend\tensorflowinternal\python-3.6.8\lib\site-packages\keras\backend\tensorflowcall file ""d:\deepfakes\deepfacelabcuda10.1sse\internal\python-3.6.8\lib\site-packages\tensorflow\python\framework\errorsimpl.resourceexhaustederror: oom when allocating tensor with shape[128,1024,32,32] and type float on /job:localhost/replica:0/task:0/device:gpu:0 by allocator gpubfc hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. done.",question,question
212,https://github.com/deepfakes/faceswap/issues/212,why the face swap area color different from original image?,"i had done data extraction, model training, and image convert followed the instruction, and finally the result like this:",question,question
454,https://github.com/deezer/spleeter/issues/454,How to get model to train on GPU instead of CPU?,possibly an easy to explain guide? i'm new to python,question,question
317,https://github.com/streamlit/streamlit/issues/317,benchmark with large dataset,problem check if large dataset is supported in streamlit (how big it can go). its kind of useful if we try to put in production setup and prevent crash if users use with bigger dataset.solution take some dataset like the taxi nyc and run it as regression tests or unit tests.additional context,other,other
517,https://github.com/deepfakes/faceswap/issues/517,Dlib alignment still available?,i figured out how to switch the detector between mtcnn and dlib but i notices that the biggest performance bottleneck is the alignment. i used an older version of the repo and the alignment was done by dlib. now the fan / deep learning aligner is used - which is indeed much better - but also much slower. browsing the code i could not figure out if the old aligner is still available / or if there is a way to switch to that. any hints?,Performance,question
3473,https://github.com/streamlit/streamlit/issues/3473,Placement of the Zoom Button in Sidebar,summary the zoom button of the ** overlaps with the setting button. the setting button lays over the zoom button. so the image can not be zoomed out. i need to resstart the app to fix this error. debug info - streamlit version: 0.83.0 - python version: 3.8.10 - anaconda - os version: windows 10 21h1 - browser version: chrome 91.0.4472.114,other,Error
3124,https://github.com/streamlit/streamlit/issues/3124,javascript is not getting executed while using the st.write(html_content. unsafe_allow_html=True),summary javascript is not getting executed while using the st.write(htmlallowfile.py sample.js ** when the span is clicked the hidden span is not appearing. but the same thing is working when using the command streamlit.components.v1.html. debug info - streamlit version: 0.80.0 - python version: 3.8.5 - using conda - os version: ubuntu 20.4 - browser version: google chrome 89.0.4389.114 please let me know what mistake i have done. thanks in advance!!!!.,other,Error
341,https://github.com/streamlit/streamlit/issues/341,Visualize Keras Model With st.graphviz_chart Not Working,"summary keras lets you create a graph visualization of a model using pydot. when converted to a string, the object returned by the keras function should be able to be used by streamlit to make a graphviz visualization.steps to reproduce 1. create a keras model and do `modeltograph = str(modelchart()` should render a visualization from the input string.actual behavior: there is no visualization. an error is not raised, either.is this a regression? don't knowdebug info - streamlit version: 0.47.4 - using anaconda - os version: windows 10 - browser version: chrome",other,other
245,https://github.com/streamlit/streamlit/issues/245,Image Picker Widget,allow uploading images from machine or from remote urls,other,other
1011,https://github.com/deepfakes/faceswap/issues/1011,"""Dockerfile.cpu"" can't find ""matplotlib==3.1.1"" when build","** update the base image of the `dockerfile.cpu` to `tensorflow/tensorflow:1.13.2-py3` the dockerfile above successfully builds the image, but there are 2 errors while running the tests. test results ```shellsession $ docker build --no-cache --file dockerfile.cpu -t test:local . ... successfully built 2e26d6218712 successfully tagged test:local $ docker run --rm -it -v $(pwd):/app test:local /bin/bash _____________________________________________________________ _ / _ / / / / / / / / / / / / /_/ /_____travis/simplednn plugin... 04/19/2020 07:57:14 info loading align from cv2addr = 0, storedreport.2020.04.19.075746756183.log'. you must provide this file if seeking assistance. please verify you are running the latest version of faceswap before reporting [-] test failed with command '['/usr/local/bin/python', 'tools.py', 'sort', '-i', '/root/cache/tests/vid/faces', '-o', '/root/cache/tests/vid/facesalignments.fsa -fc /root/cache/tests/vid/facesalignments.fsa' 04/19/2020 07:57:51 info [faces data] 04/19/2020 07:57:51 error error: the faces location /root/cache/tests/vid/facesalignments.fsa', '-fc', '/root/cache/tests/vid/facesalignments.fsa -b /root/cache/tests/vid/faces -alb /root/cache/tests/vid/testalignments.fsa' 04/19/2020 07:58:00 info reading alignments from: '/root/cache/tests/vid/testlosslossalignments.fsa -b /root/cache/tests/vid/faces -alb /root/cache/tests/vid/testlosslosslosslossalignments.fsa' 04/19/2020 08:01:16 info loading writer from ffmpeg plugin... 04/19/2020 08:01:16 warning there was an error reading from the nvidia machine learning library. either you do not have an nvidia gpu (in which case this warning can be ignored) or the most likely cause is incorrectly installed drivers. if this is the case, please remove and reinstall your nvidia drivers before reporting.original error: nvml shared library not found 04/19/2020 08:01:16 warning no gpu detected. switching to cpu mode 04/19/2020 08:01:16 info loading model from lightweight plugin... 04/19/2020 08:01:17 info using configuration saved in state file 04/19/2020 08:01:21 info loaded model from disk: '/root/cache/tests/vid/model' 04/19/2020 08:01:21 warning there was an error reading from the nvidia machine learning library. either you do not have an nvidia gpu (in which case this warning can be ignored) or the most likely cause is incorrectly installed drivers. if this is the case, please remove and reinstall your nvidia drivers before reporting.original error: nvml shared library not found 04/19/2020 08:01:21 warning no gpu detected. switching to cpu mode 04/19/2020 08:01:21 info loading mask from boxblend plugin... 04/19/2020 08:01:22 info loading color from avgconverted.mp4' 04/19/2020 08:01:22 warning imageio ffmpegblockblockblock_size to 1 (risking incompatibility). converting: 100%███████████████████████████████████████████████████████████████████████████ 91/91 [00:22",Error,question
264,https://github.com/deezer/spleeter/issues/264,wav output length is way smaller to mp3 input,is there any way for the input mp3 length gets the same output length on wav format?,question,question
796,https://github.com/iperov/DeepFaceLab/issues/796,Unpredictable shutdown when I use 2 GPUs to train one model,"expected behavior training my model with both gpus at the same time. actual behavior my computer shutdown when i train with 2 gpus. it is totally unexpected; it could happen after 5 minutes or after 1 hour. longest time training is 2 hours. yes, i know this seem clearly hardware related, but i have tested different case scenarios and i think the answer is not so simple (if i am wrong please don’t mind to tell me i am an idiot). first strange thing: all works fine in build 042020, but in the later builds i have the issue (in build 042020 longest training time is 30 hours and i stopped the training, the pc didn’t shutdown suddenly). steps to reproduce train saehd with my two gpus (i dont think it could be reproduced) other relevant information i am using prebuilt windows binary - windows 10 - amd threadripper 1950x 3.4 ghz 16-core processor - msi x399 gaming pro carbon atx - 64 gb ram - ssd (500 gb free) - 1 gtx 1080ti 11gb - 1 titan xp 12gb - seasonic prime titanium 1000 w (pc part picker says my pc estimated wattage is 838w) as i said all works ok in build 042020 (longest trainning session using both gpus in my model is 30 hours). my model is 256px, 296-80-80-22, batch=8. all in gpu. - if i use just the gtx 1080ti with same model and settings but batch=4, all works fine (training during 24 hours with no issues and using the computer to watch youtube videos, surf the web, etc). - if i use just the titan xp with same model and settings but batch=4, all works fine (training during 24 hours with no issues and using the computer to watch youtube videos, surf the web, etc). - if i use both gpus at the same time, each one working in a different folder with a model with the same settings, but batch=4, all works fine (training during 24 hours with no issues and using the computer to watch youtube videos, surf the web, etc). - if i use both gpus, both training the same model with batch=8, the issue happens. (i also tried with batch=6, 4 and even 2 and it is the same. my pc shutsdown suddenly after a couple of minutes even without using the pc for anything else besides the training). thank you very much.",deployment,question
475,https://github.com/streamlit/streamlit/issues/475,Ability to update Deck.GL viewport dynamically,"steps to repro: 1) run this file: ** need to figure out what the desired behavior is for when the user recenters the map via the ui, and then a script rerun is triggered. my gut feeling is that if the rerun has the same `viewport` as from when before the user interacted (or, may if the proto being sent is the same), the user's interaction should take precedence. otherwise, the new `viewport` setting should take precedence. this is similar to how we handle widgets.",other,other
430,https://github.com/streamlit/streamlit/issues/430,Allow filling widget values with parameters passed by the URL,"problem it is currently not possible to share an app with a selected set of values for the widgets without changing the source code. solution allow parameters to be passed in the url. for example, the url `myapp.com/age=25&profession=engineer` would map 25 to the possible api: ** if widgets have ids/names (see #40 ) automatically map the value on the url to the widget with that name. also when changing the sliders on a page change the url for easy copy and paste",other,other
592,https://github.com/deezer/spleeter/issues/592,[Discussion] Could not find a version that satisfies the requirement tensorflow==2.3.0 (from spleeter),"i've been trying to install spleeter obviously the native version and when i try to install with cmd ""pip3 install spleeter"" i get this error message. error: could not find a version that satisfies the requirement tensorflow==2.3.0 (from spleeter) error: no matching distribution found for tensorflow==2.3.0 not sure what to do to fix this",question,question
3160,https://github.com/streamlit/streamlit/issues/3160,Print pylint differences,"currently when pylint fails, it not shows the diffs. this can be help to not install the whole environment when want to make a modification. pylint use `black` which has a `--diff` argument. this can be prints the reformat information",other,other
657,https://github.com/iperov/DeepFaceLab/issues/657,labelme error when running 5. XSeg) data_dst edit.bat,"expected behavior trying to run the new xseg editor actual behavior xseg editor won't run. executable file labelme.exe appears to be missing from the pre-built windows 10 release. the command-line error is: ""'labelme' is not recognized as an internal or external command, operable program or batch file."" steps to reproduce run 5. xseg) data_dst edit.bat running latest windows 10 build.",Error,Error
1030,https://github.com/microsoft/recommenders/issues/1030,[BUG] issue with library of fastai,description,Error,other
1059,https://github.com/streamlit/streamlit/issues/1059,Multiselect box is slow on large datasets,"summary we are getting feedback from multiple users that the multiselect box is slow. this is especially true when the number of items is very large. for example, in 100,000 items are added to a multiselect box. we should i) make clear what the limits of a multiselect box are ii) think whether we want to have a speed optimized search box.is this a regression? noadditional information",Performance,other
236,https://github.com/streamlit/streamlit/issues/236,"Duplicated issue, to be deleted ?",the same error is here :,Error,other
3274,https://github.com/streamlit/streamlit/issues/3274,FYI: ad blocker blocking streamlit generated UI elements,"summary at least one widely distributed ad blocker filtering list is stripping streamlit buttons out of web pages. note: i don't think this is a bug in streamlit, but it does affect people who use streamlit, and maybe there's something that you can do to prevent it from happening. either way you should know. see: to reproduce code snippet: using in safari with adguard with the ""web annoyances ultralist"" filter. possibly other ad blockers that use the same filtering will have the same effect. this is testable on ** does not show a button because the button is blocked.debug info - streamlit version: 0.82.0 - python version: python 3.9.5 - using conda? pipenv? pyenv? pex? no - os version: macos 10.15.7 - browser version: safari 14.0.3 (15610.4.3.1.7, 15610)additional information adguard for safari with web annoyances ultralist enabled",other,other
191,https://github.com/iperov/DeepFaceLab/issues/191,Getting error after Windows Update: gpufmkmgr.pynvml.NVMLError_Unknown: Unknown Error,"hello, i have gtx1060 gpu with 6gb ram, and prebuilt windows app was working perfectly... but after the latest windows 10 update, something seems to be broken and im getting this error while extracting faces: gpufmkmgr.pynvml.nvmlerror_unknown: unknown error i have tried the latest windows build: deepfacelabcuda9.2sse and while extracting it goes through only 1st pass, and nothing happens afterwards: performing 1st pass... running on generic geforce gpu #0. can anybody please help ?",question,question
954,https://github.com/deepfakes/faceswap/issues/954,Return code 3221225620 when start training a new model,"** ``` loading... setting faceswap backend to nvidia 12/12/2019 21:46:24 info log level set to: trace using tensorflow backend. 12/12/2019 21:46:26 info model a directory: c:\users\wujia\faceswap\datas\imgs\lyl64 12/12/2019 21:46:26 info model b directory: c:\users\wujia\faceswap\datas\imgs\cxk64 12/12/2019 21:46:26 info training data directory: c:\users\wujia\faceswap\datas\train\lyl-cxk3 12/12/2019 21:46:26 info =================================================== 12/12/2019 21:46:26 info starting 12/12/2019 21:46:26 info press 'terminate' to save and quit 12/12/2019 21:46:26 info =================================================== 12/12/2019 21:46:27 info loading data, this may take a while... 12/12/2019 21:46:27 info loading model from realface plugin... 12/12/2019 21:46:27 verbose loading config: 'c:\users\wujia\faceswap\config\train.ini' 12/12/2019 21:46:27 info using optimizer savings 12/12/2019 21:46:27 verbose loading config: 'c:\users\wujia\faceswap\config\train.ini' 12/12/2019 21:46:27 info no existing state file found. generating. 12/12/2019 21:46:27 info using convolutional aware initialization. model generation will take a few minutes... 12/12/2019 21:46:27 warning from c:\users\wujia\miniconda3\envs\faceswap\lib\site-packages\keras\backend\tensorflowdefaultdefaultbackend.py:517: the name tf.placeholder is deprecated. please use tf.compat.v1.placeholder instead.\n 12/12/2019 21:46:27 warning from c:\users\wujia\miniconda3\envs\faceswap\lib\site-packages\keras\backend\tensorflowuniform is deprecated. please use tf.random.uniform instead.\n 12/12/2019 21:46:27 info calculating convolution aware initializer for shape: (3, 3, 682, 2728) 12/12/2019 21:46:33 warning from c:\users\wujia\miniconda3\envs\faceswap\lib\site-packages\tensorflow\python\ops\variables.py:2618: variable.initializedvalue. variables in 2.x are initialized automatically both in eager and graph (inside tf.defun) contexts. 12/12/2019 21:46:33 info calculating convolution aware initializer for shape: (3, 3, 682, 682) 12/12/2019 21:46:34 info calculating convolution aware initializer for shape: (3, 3, 682, 682) 12/12/2019 21:46:36 info calculating convolution aware initializer for shape: (3, 3, 682, 1364) 12/12/2019 21:46:39 info calculating convolution aware initializer for shape: (3, 3, 341, 680) 12/12/2019 21:46:39 info calculating convolution aware initializer for shape: (3, 3, 170, 340) 12/12/2019 21:46:40 info calculating convolution aware initializer for shape: (3, 3, 85, 168) 12/12/2019 21:46:40 info calculating convolution aware initializer for shape: (5, 5, 42, 3) 12/12/2019 21:46:40 info calculating convolution aware initializer for shape: (3, 3, 682, 1536) 12/12/2019 21:46:43 info calculating convolution aware initializer for shape: (3, 3, 384, 768) 12/12/2019 21:46:44 info calculating convolution aware initializer for shape: (3, 3, 192, 384) 12/12/2019 21:46:44 info calculating convolution aware initializer for shape: (3, 3, 96, 192) 12/12/2019 21:46:44 info calculating convolution aware initializer for shape: (5, 5, 48, 1) 12/12/2019 21:46:44 warning from c:\users\wujia\miniconda3\envs\faceswap\lib\site-packages\keras\backend\tensorflowdefaultdefaultfeaturebackend.py:181: the name tf.configproto is deprecated. please use tf.compat.v1.configproto instead.\n 2019-12-12 21:46:44.653250: i tensorflow/streamloader.cc:42] successfully opened dynamic library nvcuda.dll 2019-12-12 21:46:44.679650: i tensorflow/core/commondevice.cc:1640] found device 0 with properties: name: geforce rtx 2080 ti major: 7 minor: 5 memoryclockrate(ghz): 1.65 pcibusid: 0000:30:00.0 2019-12-12 21:46:44.679824: i tensorflow/streamcheckerruntime/gpu/gpuruntime/gpu/gpuruntime/gpu/gpuruntime/gpu/gpuruntime/gpu/gpu____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________",other,other
292,https://github.com/iperov/DeepFaceLab/issues/292,result.mp4  not showing up anywhere,"i got it all up and running and got to the final stage where i render the .mp4 file. it took a while but after it was done it said ""done"" press any key to continue . which i did and then i look for the result.mp4 file and it is not in workspace , its not anywhere on my computer. could the app be rendering it as another file name somewhere else because i can not find it",question,question
194,https://github.com/deezer/spleeter/issues/194,[Discussion] Separation of live audio from microphone,"hi, i'd like to separate audio coming from the microphone, how do you suggest i should proceed. is it possible? i'm currently creating small duration files and using them, but it doesn't seem like a clean approach.",other,question
3196,https://github.com/streamlit/streamlit/issues/3196,Change websocket ping time to 1s to improve connection resilience?,"summary the websocket connection drops consistently for . in `sever/server.py`, when we change this: ...to this: ...the problem goes away. **steps to reproduce we couldn't find a toy scenario to repro this :(is this a regression? don't think sodebug info - streamlit version: 0.80.0 - python version: (get it with `$ python --version`) - using conda - os version: - browser version:",Performance,Error
289,https://github.com/mozilla/TTS/issues/289,Unexpected bus error encountered in worker,"hello everyone. when training tacotron2 from dev branch on v100 16gb single gpu, near the end of the second epoch i've got error: > step:598/770 globalstep:1370 postnetloss:0.03293 decoderloss:0.01078 stoploss:0.17731 alignscore:0.0933 gradnorm:0.32636 gradnormst:0.20265 avgtextlen:140.4 avgspeclen:646.5 steptime:2.75 loadertime:0.02 lr:0.000100 error: unexpected bus error encountered in worker. this might be caused by insufficient shared memory (shm). ! run is kept in /ssd/ya/outputs/v100-september-26-2019step, epoch) file ""train.py"", line 165, in train textlengths, melids=speaker64.egg/torch/nn/modules/module.py"", line 547, in _outputs, mel64.egg/torch/nn/modules/module.py"", line 547, in _output, stopweights = self.decode(memory) file ""/opt/tts/lib/python3.6/site-packages/tts-0.0.1+53d658f-py3.6.egg/tts/layers/tacotron2.py"", line 232, in decode stopinput.detach()) file ""/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x8664.egg/torch/nn/modules/container.py"", line 92, in forward input = module(input) file ""/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x8664.egg/torch/nn/modules/dropout.py"", line 54, in forward return f.dropout(input, self.p, self.training, self.inplace) file ""/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86vf.dropout(input, p, training)) file ""/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86utils/signalerroranyfails() runtimeerror: dataloader worker (pid 32284) is killed by signal: bus error. (tts) root@server____________10+02am-53d658f traceback (most recent call last): file ""train.py"", line 682, in main(args) file ""train.py"", line 587, in main ap, globaliter, data in enumerate(data64.egg/torch/utils/data/dataloader.py"", line 819, in _process64.egg/torch/utils/data/dataloader.py"", line 846, in data data.reraise() file ""/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86utils.py"", line 369, in reraise raise self.exc64.egg/torch/utils/data/worker64.egg/torch/utils/data/fn(data) file ""/opt/tts/lib/python3.6/site-packages/tts-0.0.1+53d658f-py3.6.egg/tts/datasets/ttsdataset.py"", line 221, in collate_fn linear = torch.floattensor(linear).contiguous() runtimeerror: [enforce fail at cpuallocator.cpp:64] . defaultcpuallocator: can't allocate memory: you tried to allocate 99187200 bytes. error code 12 (cannot allocate memory) what can be the reason? the files train.py and others, except for some of utils/text python scripts are unchanged. batch size is 32. number of workers = 4.",question,question
962,https://github.com/microsoft/recommenders/issues/962,[FEATURE] Migrate AzureML service 'ContainerImage' usage to 'Environments',"description creating and deploying a containerimage is soon-to-be deprecated functionality inside of azureml service. the new recommended flow is instead to prepare an environment object that contains your pip/conda requirements and simply include this in your inferenceconfiguration to be supplied in a model.deploy() call. this speeds up image build and service deployment time, and allows subsequent service deployments to leverage existing prepared environment images. documentation on environments can be found here: in which platform does it happen? azure machine learning service how do we replicate the issue? register a model with azureml service deploy that containerimage as an aks webservice expected behavior (i.e. solution) train your model locally or on a remote compute in the given environment deploy that model as an aks webservice other comments",other,other
2887,https://github.com/streamlit/streamlit/issues/2887,"Set empty query params should only reset the params portion, not the entire URL","summary when set an empty params to experimentalqueryparams=foo actual result: link.com expected result: link.com/subfolder/snippet code i'm not sure how to run a streamlit app with a link with a sub directory though. `streamlit run myparam is empty, it's used '/', which will reset the url.",Error,Error
2533,https://github.com/streamlit/streamlit/issues/2533,Regression: slider label only shows on hover,steps to reproduce 1. go to 2. look at the slider! expected: slider thumb always has a label above it actual: slider thumb only has label when you hover over it see screencast: this a regression? yes debug info - streamlit version: 0.73.1 - browser version: chrome version 87.0.4280.88 (official build) (64-bit),Error,other
552,https://github.com/deezer/spleeter/issues/552,[Discussion] Public Release Version 2.0.2,"hello here, and thanks for all that stuff ! really usefull ! i'm going to update the archlinux aur package of spleeter and i need a public release at 2.0.2 version. is it possible to provide such a release ? thanks a lot in advance.",other,other
987,https://github.com/microsoft/recommenders/issues/987,[BUG] in dsvm_notebook_win_pyspark,description,Error,other
773,https://github.com/streamlit/streamlit/issues/773,Update the Cached Object Mutated warning,"summary if we detect a cached object mutation (and `allowmutation` is `false`) then we should: 1. issue this warning below 2. return the mutated cached result ** > > by default, streamlit cache is immutable. you received this warning because streamlit thinks you modified a cached object. > > . of course, the link should be to our , not the google drive design doc.background this is part of our .",other,other
1501,https://github.com/microsoft/recommenders/issues/1501,Add nightly build,description add nightly-build to our new github action ci infrastructures. expected behavior with the suggested feature there is an existing nightly build running the full test suite in azure devops. we should replicate the same build as we migrate to github action other comments,other,other
2225,https://github.com/streamlit/streamlit/issues/2225,Separate Modules inside a page with its own state in-depended on others,"first of all, this is my first comments anywhere in github but this communities tolerance level or acceptance level :) encouraged me to write this so please bear with me here. also, can't tell you how awesome streamlit is; i started learning about its from last few days and cant get over it.problem i have a task where i am suppose to use 3-4 tools/separate internal site to get one use case complete. my solution is a python program to make this possible from single place. so i am trying to create a web app which communicates with multiple outside web apps via python request api. and step by step it's making request and getting the output, which is used by another step in my web app and so on. as i understood having separate state in same streamlit web app, by design, is an issue, i was hoping if we can have separate sections which have its own state and execute on their on without getting affected by other modules in same web app.solution have a class like implementation of the streamlit web app where by default all webpage executes in single class/module. but user has option to split the functionality and box them in different independent modules. ** 1- further, modules can use the output from other modules and make changes dependent on their state. 2- also, each module will only execute, if ""triggered"" by changes in widget, input box or other ui items inside it, else it will be in its previous state executed or non-executed. 3- can have executed or non-executed ui block in webpage as disabled or in different color depending on its state which can be selected by user. like green for executed and so on. 4- modules can either be in same webpage or on tabs on same web app as well.",other,other
37,https://github.com/iperov/DeepFaceLab/issues/37,File Exists Error,"d:\deepfacelabtorrent>""6) train miaef128 best gpu.bat"" running trainer. loading model... loading: 100%███████████████████████████████████████████████████████████████████ 2828/2828 [00:01 'd:\\deepfacelabtorrent\\workspace\\model\\miaef128internal\bin\deepfacelab\mainscripts\trainer.py"", line 84, in trainerthread file ""d:\deepfacelabtorrent\save file ""d:\deepfacelabtorrent\internal\bin\deepfacelab\models\modelinternal\bin\deepfacelab\models\modelbase.py"", line 245, in savesafe file ""pathlib.py"", line 1307, in rename file ""pathlib.py"", line 393, in wrapped fileexistserror: [winerror 183] cannot create a file when that file already exists: 'd:\\deepfacelabtorrent\\workspace\\model\\miaef128decodercommonb.h5' press any key to continue . . .",question,question
3115,https://github.com/streamlit/streamlit/issues/3115,HTML is not working in streamlit.markdown in version 0.80.0,"summary html code in `st.markdown` was working okay in streamlit version 0.79.0. but with version 0.80.0, it's not giving the desired output. steps to reproduce code snippet: () ** the title line `唳椸δ唳is this a regression? this problem didn't appear in version 0.79.0. debug info - streamlit version: 0.80.0 - python version: 3.8.3 - using conda - os version: windows 10 pro - browser version: google chrome version 89.0.4389.114 (official build) (64-bit)additional information",Error,Error
622,https://github.com/microsoft/recommenders/issues/622,refactor movielens dataloader for single responsibility,description movielens dataloader needs refactor for single responsibility and better code-readability expected behavior (i.e. solution) change `datafile` to `maybeandmaybeextractanddata`,other,other
187,https://github.com/deezer/spleeter/issues/187,[Discussion] your question: confuse about GPU when Itry to train the model .,the command is :cudadevices='1' python _master/configs/4stems/base_?,question,question
599,https://github.com/iperov/DeepFaceLab/issues/599,TypeError: object of type 'PosixPath' has no len(),"os: debian linux gpu: nvidia t4 cuda: 10.0.130-1 python: 3.6 command line: python3.6 deepfacelab/main.py merge --input-dir workspace/datadst/merged --output-mask-dir workspace/datamask --aligned-dir workspace/datadst/00096.jpg]: traceback (most recent call last): file ""/content/deepfacelab/mainscripts/merger.py"", line 151, in processimagemask file ""/content/deepfacelab/facelib/landmarksprocessor.py"", line 324, in expandsubprocessdata exception: error while merging file [/content/workspace/datadata file ""/content/deepfacelab/merger/mergemasked.py"", line 327, in mergemasked file ""/content/deepfacelab/merger/mergemasked.py"", line 13, in mergemaskedface file ""/content/deepfacelab/facelib/landmarksprocessor.py"", line 355, in gethulleyebrows typeerror: object of type 'posixpath' has no len() len(cache)) exception while process data [/content/workspace/datadata file ""/content/deepfacelab/merger/mergemasked.py"", line 327, in mergemasked file ""/content/deepfacelab/merger/mergemasked.py"", line 13, in mergemaskedface file ""/content/deepfacelab/facelib/landmarksprocessor.py"", line 355, in gethulleyebrows typeerror: object of type 'posixpath' has no len() during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/content/deepfacelab/core/joblib/subprocessorbase.py"", line 71, in run file ""/content/deepfacelab/mainscripts/merger.py"", line 157, in processdst/00097.jpg]: traceback (most recent call last): file ""/content/deepfacelab/mainscripts/merger.py"", line 151, in processimagemask file ""/content/deepfacelab/facelib/landmarksprocessor.py"", line 324, in expand_eyebrows typeerror: object of type 'posixpath' has no len() len(cache))",Error,Error
1215,https://github.com/microsoft/recommenders/issues/1215,[FEATURE] Set up test machine linux,"description from make sure all tests pass: unit: - [x] pytest tests/unit -m ""not notebooks and not spark and not gpu"" --durations 0 - [x] pytest tests/unit -m ""notebooks and not spark and not gpu"" - [x] pytest tests/unit -m ""not notebooks and not spark and gpu"" - [x] pytest tests/unit -m ""notebooks and not spark and gpu"" - [x] pytest tests/unit -m ""not notebooks and spark and not gpu"" - [x] pytest tests/unit -m ""notebooks and spark and not gpu"" smoke: - [x] pytest tests/smoke -m ""smoke and not spark and not gpu"" --durations 0 - [x] pytest tests/smoke -m ""smoke and not spark and gpu"" --durations 0 - [x] pytest tests/smoke -m ""smoke and spark and not gpu"" --durations 0 integration: - [x] pytest tests/integration -m ""integration and not spark and not gpu"" --durations 0 - [x] pytest tests/integration -m ""integration and not spark and gpu"" --durations 0 - [x] pytest tests/integration -m ""integration and spark and not gpu"" --durations 0 expected behavior with the suggested feature other comments",other,question
1555,https://github.com/streamlit/streamlit/issues/1555,Streamlit is stuck after redirecting port with Apache and Nginx,"summary i want to direct to the output of port 8040 (where i have a streamlit application running) to the port 80 that is associated to the web server, i have installed both nginx and apache web servers and with both the application gets stuck after trying to access it via myproxy modules for apache): proxypreservehost on proxyrequests off servername serveralias mywebsite.ai . trying to follow the suggestion in a previous i configured my config.toml as follows: [server] port=8040 enablecors=falseexpected behavior: when accessing the url mywebsite.ai:8040 the application works fine. also works fine when changing the url of my server for the ip of my server i.e. my_ip:8040. i have used also a nginx server and the same problem appeared. i was not able to redirect traffic from port 8040 to port 80 using iptables. i am using a droplet from digitalocean as my server.",question,deployment
1684,https://github.com/streamlit/streamlit/issues/1684,Application crashed,"summary application crashes: *args, **kwargs) file ""/app/.heroku/python/lib/python3.6/site-packages/matplotlib/figure.py"", line 2496, in tightpad=hpad=wlayout.py"", line 360, in getlayoutpad=hpad=wlayout.py"", line 109, in autosubplotpars tightraw = union([ax.getlayout.py"", line 110, in if ax.getbase.py"", line 4323, in getxaxis = self.xaxis.gettightbbox self.labelupdateposition bboxes, bboxes2 = self.ticksiblings(renderer=renderer) file ""/app/.heroku/python/lib/python3.6/site-packages/matplotlib/axis.py"", line 2006, in ticksiblings tlb, tlb2 = axx.xaxis.ticktogetbboxes for tick in ticks if tick.label1.getvisible()], file ""/app/.heroku/python/lib/python3.6/site-packages/matplotlib/text.py"", line 905, in getextent bbox, info, descent = self.layout(self.getcached[key] = ret file ""/app/.heroku/python/lib/python3.6/site-packages/matplotlib/cbook/_killkeys[0]]",other,Error
2970,https://github.com/streamlit/streamlit/issues/2970,Make it possible to click/drag in the color picker,"our color picker widget is going to get a whole lot more usage in the near future since it's used internally in the theme selector. the widget works, but clicking and dragging currently isn't something you can do in it, which makes the experience suboptimal (ideally i'd like to be able to click and drag to be able to quickly ""scroll"" through a bunch of possible colors until i find one that looks good).",other,other
622,https://github.com/streamlit/streamlit/issues/622,Server-side state,"problem i would like to have a server-side state. my use-case is : i want to be able to share ""notifications"" between users. if one user write a notification, i want other users to be able to see this notification.solution i have no idea how to do that... i saw in #165 how to do a client-side state, but it's precisely client side, not server side. any input on how to do (even if it's a dirty workaround for now) would be helpful.",other,other
290,https://github.com/iperov/DeepFaceLab/issues/290,"CONVERT not using GPU, only CPU.",using last ** 397.64,deployment,other
704,https://github.com/mozilla/TTS/issues/704,Input from file,great project! but it would be much more useful if it could read a text file instead of just the argument from the command line! to harmonize with --outpath.,other,other
111,https://github.com/streamlit/streamlit/issues/111,Lists inside of DataFrames are unhashable and break st.cache,"summary a `dataframe` which contains a list is unhashable and therefore breaks `st.cache`.steps to reproduce run this code behavioractual behavior we get an error with the following stack trace expected behavior this should work, just as if `returnlist` weren't decorated with `@st.cache`.is this a regression? not sure.debug info",Error,Error
70,https://github.com/mozilla/TTS/issues/70,Train TTS with phonemes,"** one can train phoneme model using dev-phoneme branch. due to some hard words like ""echo, focus, epitomize"", it is worthy to try phonemes over raw characters.",other,other
613,https://github.com/streamlit/streamlit/issues/613,Specify max number of characters in text area,"problem right now, there is no limit to specify number of characters in text area. this is very risky and an overkill if users use it to process too large textual data.solution it would be great to give users the option to specify the max number of characters allowed in text area. it should be up to the user to specify number of characters though.",other,other
59,https://github.com/iperov/DeepFaceLab/issues/59,Color Transfer by Neural Networking,i found this paper looks very interesting and we may use this for better color transferring. reference:,other,other
124,https://github.com/deepfakes/faceswap/issues/124,"Small error in lib/cli.py get_faces(self, image) ","@line 76 if facescount is greater than 1 instead? currently, converting an image sequence continuously bombards the shell with 'note: found more than one face in an image!' when in actuality, there is only one face.",Error,question
1317,https://github.com/streamlit/streamlit/issues/1317,"an object is used when the item refreshed, itself do not refresh and it's data is added new data","an object is used when the item refreshed, itself do not refresh and it's data is added new data",other,other
2743,https://github.com/streamlit/streamlit/issues/2743,'Copy to Clipboard' icon location is off,to repro select st_code.py,Error,Error
760,https://github.com/iperov/DeepFaceLab/issues/760,Perhaps the face detector is due for an upgrade,"seeing as s3fd is beginning to show its age - that being three years old - due to many users' difficulty using it on more complex scenes, such as ones with poor lighting and/or an abundance of occlusions, i believe has come time to retire s3fd and replace it with one of the multitude of face alignment papers that are far more recent and robust; these newer face alignment algorithms can be found , and seem to be substantially more resistant to occlusions and poor lighting. the only concerns i would have with this upgrade would primarily be how ingrained s3fd is into this repository, i.e. if it can merely be swapped with a pretrained model and some variable changes, or if it would require a more fundamental restructuring of the codebase. i am opening this as an issue to promote discussion relating to the current face extraction algorithm and whether it is indeed sufficient. i, however, am still planning to upgrade the detector on my fork of dfl . please leave your thoughts in this issue thread. i would especially love to hear @iperov 's take on this. ** upon further reading into the code, it does indeed appear that head extraction makes use of a tensorflow implementation of . however, strangely enough, it still states that it uses s3fd even when extracting via head. regardless, s3fd is still used on all other face extraction modes. in spite of this, however, i still believe that even the fan-based head extractor could be upgraded as well. one thing brought up by ctrl+shift+face was the possibility of a trainable extractor, like an xseg for alignment. this idea is intriguing, and i believe that it warrants further exploration.",other,other
522,https://github.com/iperov/DeepFaceLab/issues/522,Request - Can we have the model summary file which we already generate in DFL; act as changelog –,"expected behavior i am trying different models on my end; with different values for different iterations as training and testing the results to see how they turnout as video at end – and how that can be used in different circumstances …… a screenshot is attached to show – how they are saved at the moment: actual behavior as probably visible from the screenshot; it’s hard to keep track of them as time goes by … - also, after a month or so, if i were to come as see these naming scheme again, would be even hard for me to remember what was implied at the time of running them or saving them. - lastly, if shared with someone else to work with; if nothing else it's real cumbersome to explain to them; how & what that naming scheme means etc …. request ** – wherein if a model is run with different settings from before: - it reflects the older summary and - then reflects below the new summary (with new values etc…) advantages - easy to share with people with transparency - can always count back what actions were taken so the model ended up looking as result; anytime. - faster testing time of dfl settings at user end; maybe even debugging",other,question
323,https://github.com/mozilla/TTS/issues/323,Finding appropriate checkpoint,"hi, i am trying to run the notebook for testing, as mentioned in the notebook i have to load the right commit and related model, but none of the checkpoints that are provided runs on the latest commit. they all give me size mismatch errors. can you help and provide me with the latest best checkpoint that works with the last commit of the master branch?",question,question
934,https://github.com/deepfakes/faceswap/issues/934,"IndexError: Tuple index out of range, Training, Ubuntu 18.04 & Windows 10","expected behavior went to start training, attempted in both gui and command line. expected tensorboard to load and training to begin. actual behavior steps to reproduce extracted faces from images, cleaned up the folder if images were not good. in training, added folder of original person in a, person to swap in on b. added image alignment folder, added directory for model output. set up timelapse folder. other relevant information - ** gpu, cuda 10.0 link to crashfile:",question,Error
376,https://github.com/streamlit/streamlit/issues/376,add cli config flags to `streamlit hello`,based on the comment:,other,other
496,https://github.com/iperov/DeepFaceLab/issues/496,Google Colab - Stopped working ,"expected behavior trying to train a model and it doesn't run and give this error. steps to reproduce also, tried after some time with a different account and browser.. same error .. other relevant information was working fine till a couple of hours ago ...",deployment,question
654,https://github.com/deepfakes/faceswap/issues/654,Is it possible to swap a whole movie?,"i watched captain marvel today, the moment i saw stan lee(rip), i wonder if it's possible to swap the whole movie. maybe even let the audience choose characters for the movie.",question,question
1172,https://github.com/deepfakes/faceswap/issues/1172,Realtime solution?,** add any other context or screenshots about the feature request here.,other,other
49,https://github.com/iperov/DeepFaceLab/issues/49,NVML shared directory not found,"hello, i downloaded the prebuilt torrent and tried to run it. i was successfully able to create frames, but im stuck at extracting faces. it throws an error saying ""nvml shared library not found"" i have nvidia 1060 and installed cuda 9. i have tried openfaceswap, it worked with half face... but i need to model using dfaker full face. please help. thanks.",question,question
3538,https://github.com/streamlit/streamlit/issues/3538,Error when caching matplotlib plots,"summary ""internalhasherror: 'spines' object does not contain a 'name' spine"" error when trying to cache matplotlib figsteps to reproduce 1. load iris data from sci-kit learn. 2. split the data into train and test sets. 3. load knn model from sci-kit learn. 4. pass train, test sets and the model class object `kneighborsclassifier(nk)` as defined in the code below. code snippet: ** when trying to cache a matplotlib figure, streamlit returns an error. kbesttrain, ytest, yk=15) st.pyplot(k_fig) is this a regression? that is, did this use to work the way you expected in the past? nodebug info - streamlit version: 0.84.0 - python version: 3.9.5 - using conda - os version: macos catalina 10.15.7 - browser version: edge version 87.0.664.66",other,Error
455,https://github.com/iperov/DeepFaceLab/issues/455,Quick question,"guys, quick question. i have favorite musican, i wanna enchace old music video, there is a shitty vob quality. can i improve face quality on the original video via faceswap if i just train it with the same person, but with hq images?",question,question
1496,https://github.com/streamlit/streamlit/issues/1496,Error during exception handling in st.write - TypeError: exception() takes 3 positional arguments but 4 were given,"summary i'm trying to display a dataframe with `st.write` and it fails with a streamlit error while trying to handle an error from rendering the object. steps to reproduce it happens when you raise an exception during type conversion in `st.write`. here's the simplest example i could think of that throws the same error. expected behavior: it should display the `valueerror`actual behavior: instead it throws a streamlit internal `typeerror` error (see above traceback). it means that i can't actually debug the exception that is throwing that code.debug info - streamlit version: `streamlit, version 0.60.0` (get it with `$ streamlit version`) - python version: `python 3.6.10 :: anaconda, inc.` (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - conda - os version: - browser version:additional information from a quick spin following the traceback, i believe i can see the issue. the traceback says that the error originates here: the wrapper provides 2 args and it says it's receiving 4 while expecting 3, so that means that the wrapped method is being called with 2 instead of an expected 1 argument. earlier in the traceback, it says that it's being raised in `st.write` by `exception` (notice it's being called with 2 arguments): looking at its definition, `exception` is wrapped with `element` and takes 3 arguments, 2 of which are provided by the wrapper. tl;dr - you probably shouldn't be passing the traceback into the exception function (or perhaps you meant to and you haven't finished implementing it on the other side.) either way, it's broken rn. possible solution? from a cursory glance, it looks like you should just simplify to this and this issue will go away:",Error,Error
843,https://github.com/streamlit/streamlit/issues/843,Bug when loading keras model under keras2.3.0 and TensorFlow2.0,"summary bug when loading keras model under keras2.3.0 and tensorflow2.0 if using st.button.steps to reproduce what are the steps we should take to reproduce the bug: 1. load a keras model under keras2.3.0 and tensorflow2.0. 2. streamlit run app.py 3. input some characters. 4. click the extract button. 5. error occurs. any ideas? thanks a lot. attributeerror: 'local' object has no attribute 'value' traceback: file ""c:\users\lxy\appdata\roaming\python\python36\site-packages\streamlit\scriptrunner.py"", line 311, in script exec(code, module.__)debug info - streamlit version: (get it with `$ streamlit version`) => 0.51.0 - python version: (get it with `$ python --version`) => 3.6 - using conda? pipenv? pyenv? pex? => conda - os version: => win10 - browser version: => chrome",question,Error
908,https://github.com/microsoft/recommenders/issues/908,[FEATURE] xlearn best practice,description is one of the popular packages that support building fm model in an efficient way. it would be great to have best practices shared in the repo expected behavior with the suggested feature other comments,other,other
309,https://github.com/streamlit/streamlit/issues/309,Grid layout,users sometimes want to lay items in a grid. for example: the difference between a grid an a horizontal layout (issue #241) is in a grid both the rows and the columns must line up. and here's a straw-man api for this feature: ...but this is just one of multiple possible apis we could come up with here. everyone: what are other possible apis?,other,other
194,https://github.com/mozilla/TTS/issues/194,Note on CPU inference performance,"i have a solution for slow inference on cpu. you should try setting environment variable ompthreads=1 before running a python script. when pytorch is allowed to set the thread count to be equal to the number of cpu cores, it takes 10x longer to synthesize text. it's really a problem with pytorch and blas libraries, not tts. however, it leads to the perception that tts inference is slow. i would suggest documenting it in the readme file.",other,other
325,https://github.com/iperov/DeepFaceLab/issues/325,Is there similar software to fake voice?,subj.,question,question
1153,https://github.com/deepfakes/faceswap/issues/1153,Debug landmarks feature resulting in OpenCV error,"problem description when running extraction with ""debug landmarks"" checked under ""output"", the task fails with an opencv error: the actual bug `cv.circle` expects a tuple of integers as second argument but `posy` are floats. proposed fix changing line 506 of `scripts/fsmedia.py` from to",Error,Error
550,https://github.com/iperov/DeepFaceLab/issues/550,Training simply doesn't work on some AMD cards,"windows 10 64bit graphics card is a radeon vii downloaded latest opencl version and when i try to train it hangs at this: `info:plaidml:opening device ""openclgfx906.0""` edit 1: i downgraded to older version of dfl (11/14/19) and and it works fine now. so it seems the latest version of dfl opencl is still the culprit...",other,deployment
767,https://github.com/deepfakes/faceswap/issues/767,FFMPEG image sequence with longer durations after generated an output video,** - os: high sierra 10.13.6,question,other
812,https://github.com/iperov/DeepFaceLab/issues/812,Exception: Unable to start subprocesses.,"expected behavior i'm trying to extract the faces from datadebug?: n my specs are amd radeon r3 for gpu, and amd a4-9120 radeon r3, 4 compute cores 2c+2g for cpu. other relevant information - windows - python version 2.7",other,question
25,https://github.com/deezer/spleeter/issues/25,'FFMPEGProcessAudioAdapter' object is not callable,"description report a type error using python api step to reproduce i'm now with the following code: 1. installed using `pip`, and ffmpeg library has also been installed 2. got `typeerror: 'ffmpegprocessaudioadapter' object is not callable` environment ----------------- ------------------------------- os macos 10.14 python 3.6, in jupyter notebook",question,Error
279,https://github.com/deepfakes/faceswap/issues/279,Sharpening the face before applying it,"sharpen by multiplying every pixel by 2, and then subtracting the average value of the neighborhood from it. i modified convert_masked.py and i find the face less blurry on closeups on hi-res pics, though it's a bit too sharp on normal/low res compared to the rest of the image. ymmv.",other,Performance
980,https://github.com/streamlit/streamlit/issues/980,file_uploader: use PUT/POST rather than websockets,`file_uploader` currently sends files over our (single) websocket pipe. let's use put or post instead.,other,other
1568,https://github.com/microsoft/recommenders/issues/1568,[FEATURE]  Getting rid of the looping and filtering of Pandas DF in TF-IDF based Rec-Sys Snippet.,"wrt the code blocks in the description so, here we are looping over the whole dataframe and then for every iteration we are also trying to filter and get the id cols which are ranged higher as per cosine-sim. this is making it very slow and pandas df's looping and selection is a red flag. the code can be boosted a lot w.r.t runtime if we can get rid of this. expected behaviour with the suggested feature - the runtime of the code can be improved amazingly! other comments apologies if selecting the feature tag is incorrect. let me know if there's interest, i can try to work on it in the upcoming weekends. should be an easy fix! looking forward to your thoughts, best, aditya.",other,other
3985,https://github.com/streamlit/streamlit/issues/3985,Plots are not displayed despite of the source code present,"summary hi. i have used one of your templates to produce my own visuals, but i cannot get to a solution why the two plots (for now they are duplications just for testing purpose) do not show once i deployed the app. please let me know if i'm making any coding mistakes or if there is some bugs.steps to reproduce code snippet: (please provide a code snippet! this will help expedite us finding and solving the problem.) if applicable, please provide the steps we should take to reproduce the bug: code attached above. ** i expect to see two plots in one row. instead i cannot see any plots.is this a regression? nodebug info - streamlit version: (get it with `$ streamlit version`) 1.1.0 - python version: (get it with `$ python --version`) 3.8.5 - using conda? pipenv? pyenv? pex? no - os version: macos catalina - browser version: chromeadditional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,other
600,https://github.com/iperov/DeepFaceLab/issues/600,merge SAE HD ERROR,"file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 157, in processnvidia\workspace\datanvidia\data file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 331, in mergemasked file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 13, in mergemaskedface file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 355, in gethullnvidia\eyebrows typeerror: object of type 'windowspath' has no len() file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandnvidia\subprocessnvidia\data exception: error while merging file [c:\users\guita\downloads\deepfacelabdst\00001.png]: traceback (most recent call last): file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandnvidia\data file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 331, in mergemasked file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 13, in mergemaskedface file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 355, in gethullnvidia\eyebrows typeerror: object of type 'windowspath' has no len() during handling of the above exception, another exception occurred: traceback (most recent call last): file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\core\joblib\subprocessorbase.py"", line 71, in run file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 157, in processnvidia\workspace\datanvidia\data file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 331, in mergemasked file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 13, in mergemaskedface file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 355, in gethullnvidia\eyebrows typeerror: object of type 'windowspath' has no len() file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandnvidia\subprocessnvidia\data exception: error while merging file [c:\users\guita\downloads\deepfacelabdst\00001.png]: traceback (most recent call last): file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandnvidia\data file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 331, in mergemasked file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 13, in mergemaskedface file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 355, in gethullnvidia\eyebrows typeerror: object of type 'windowspath' has no len() during handling of the above exception, another exception occurred: traceback (most recent call last): file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\core\joblib\subprocessorbase.py"", line 71, in run file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 157, in processnvidia\workspace\datanvidia\data file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 331, in mergemasked file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\merger\mergemasked.py"", line 13, in mergemaskedface file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 355, in gethullnvidia\eyebrows typeerror: object of type 'windowspath' has no len() file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandnvidia\subprocessnvidia\data exception: error while merging file [c:\users\guita\downloads\deepfacelabdst\00001.png]: traceback (most recent call last): file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\mainscripts\merger.py"", line 151, in processnvidia\nvidia\nvidia\imagemask file ""c:\users\guita\downloads\deepfacelabinternal\deepfacelab\facelib\landmarksprocessor.py"", line 324, in expandquick96session.dat done.",Error,question
406,https://github.com/streamlit/streamlit/issues/406,Change Background color of Button widget,"any simple way to work around changing color of button widget? thanks,",other,other
1541,https://github.com/streamlit/streamlit/issues/1541,Move 'Run Streamlit remotely' tutorial to wiki,"this page in the tutorial is an overly specific development environment issue that arguably has nothing to do with streamlit: proposal is to move this page to the streamlit wiki, where a more developer-focused crowd might expect to see it.",other,other
542,https://github.com/microsoft/recommenders/issues/542,Test spark hypertune notebook,description there are no tests for the notebook for spark tuning expected behavior with the suggested feature,other,other
963,https://github.com/iperov/DeepFaceLab/issues/963,Colab commit that works with FANSEG LAST BUILD version.,"technically, not an issue with the main pc program. have suddenly started getting this error on colab: [cpu] : cpu [0] : tesla v100-sxm2-16gb [0] which gpu indexes to choose? : 0 error: module 'tensorflow' has no attribute 'configproto' traceback (most recent call last): file ""/content/deepfacelab/mainscripts/trainer.py"", line 55, in trainerthread debug=debug, file ""/content/deepfacelab/models/modelbase.py"", line 151, in init nn.initialize(self.device**_ using any other commit uses: ""uniform yaw distribution of samples"" as one of the training options which the fanseg last build version does not, thus creating incompatibility between the 2. commit 0eb7e06ac1b059746f373e1932321a7593c29515 is the first/earliest commit that doesn't throw the above error on colab, but the model trained on it won't work with the fanseg build.",other,Error
734,https://github.com/mozilla/TTS/issues/734,> Decoder stopped with `max_decoder_steps` 500,"steps to reproduce: 1. install tts with `python -m pip install tts` 2. run in console: `tts --text ""hello my name is johanna, and today i want to talk a bit about autoplug. in short, autoplug is a feature-rich, modularized server manager, that automates the most tedious parts of your servers or networks maintenance."" --outabsolutepath_here\output.wav` result: the output.wav file is around 10 seconds long and the reader stops talking around the middle of the text (""... server manager, that...""). system: windows 10 x64 bit looks like its related to the length of a sentence...",question,question
427,https://github.com/iperov/DeepFaceLab/issues/427, colab chooses random faces for preview,how can i choose which faces to see in saesae.jpg? how to choose which frames from datapreview_sae.jpg?,question,question
5249,https://github.com/iperov/DeepFaceLab/issues/5249,train Quick96 does not work?,"extracting images and facesets worked correctly. so i'm wondering what is the issue with the training part... actual behavior error code: `running trainer. [new] no saved models found. enter a name of a new model : new model first run. choose one or several gpu idxs (separated by comma). [cpu] : cpu [0] : geforce rtx 3060 ti [0] which gpu indexes to choose? : 0 initializing models: 100%#################################################################################################################################################################################### 5/5 [00:02"", line 1, in file ""multiprocessing\spawn.py"", line 105, in spawnmain file ""d:\bittorrent\deepfacelab\deepfacelabinternal\deepfacelab\samplelib\_main file ""d:\bittorrent\deepfacelab\deepfacelabinternal\deepfacelab\samplelib\sample.py"", line 7, in file ""multiprocessing\spawn.py"", line 115, in nvidia\nvidia\nvidia\nvidia\bynvidia\main file ""d:\bittorrent\deepfacelab\deepfacelabinternal\deepfacelab\core\imagelib\_nvidia\nvidia\nvidia\nvidia\main file ""d:\bittorrent\deepfacelab\deepfacelabinternal\python-3.6.8\lib\site-packages\scipy\special\_nvidia\nvidia\basic import cv2 from ..special import relmain traceback (most recent call last): file """", line 1, in file ""d:\bittorrent\deepfacelab\deepfacelabinternal\python-3.6.8\lib\site-packages\scipy\special\nvidia\nvidia\main importerror: numpy.core.multiarray failed to import from . import specfun file ""d:\bittorrent\deepfacelab\deepfacelabinternal\deepfacelab\samplelib\sample.py"", line 4, in from . import nvidia\nvidia\nvidia\basic.py"", line 18, in file ""d:\bittorrent\deepfacelab\deepfacelabinternal\deepfacelab\samplelib\sample.py"", line 4, in : file """", line 1, in dll load failed: the paging file is too small for this operation to complete. file ""multiprocessing\spawn.py"", line 105, in spawnnvidia\_ importerrorimporterror: numpy.core.multiarray failed to import oserror: : dll load failed: the paging file is too small for this operation to complete.[winerror 1455] the paging file is too small for this operation to complete ` steps to reproduce i tried to see if different videos would behave differently, unfortunately no change. other relevant information -windows 10 home os 64-bit -gpu: nvidia geforce rtx 3060 ti -cpu: amd ryzen 5 5600x 6core",question,question
1242,https://github.com/streamlit/streamlit/issues/1242,import error,"summary type here a clear and concise description of the bug. aim for 2-3 sentences.steps to reproduce what are the steps we should take to reproduce the bug: 1. go to '...' 2. click on '....' 3. scroll down to '....'expected behavior: explain what you expect to happen when you go through the steps above, assuming there were no bugs.actual behavior: explain the buggy behavior you experience when you go through the steps above. if applicable, add screenshots to help explain your problem.is this a regression? that is, did this use to work the way you expected in the past? yes / nodebug info - streamlit version: (get it with `$ streamlit version`) - python version: (get it with `$ python --version`) - using conda? pipenv? pyenv? pex? - os version: - browser version:additional information if needed, add any other context about the problem here. for example, did this bug come from or another site? link the original source here!",other,other
1274,https://github.com/microsoft/recommenders/issues/1274,[ASK] Rename master branch -> main,description rename master branch -> main other comments,question,other
1124,https://github.com/deepfakes/faceswap/issues/1124,"I apologize in advance but I have no idea where to ask, so I'm opening a post.",**,question,other
1716,https://github.com/streamlit/streamlit/issues/1716,Streamlit behind reverse proxy doesn’t work when you change port,"summary i am trying to run streamlit behind revese proxy doesn't work when we change port in streamlit. type here a clear and concise description of the bug. aim for 2-3 sentences. when change port in streamlit, and put it behind reverse proxy, front end try to use default port 8501 rather than changed port.steps to reproduce first run streamlit in reverse proxy, if you point to this url. i am seeing almost all calls fails. but let me show you one example request. when i look network, i see following request fail. healthz. issue is, healthz is still ping at original port (8501) which is seen as following as error: when i copy request as curl from browser for that request i see following: as you can see here, it is pointing to original port (8501) not change port 2000.expected behavior: expected behavior should be that it request should use port that is provided in cmd not original actual behavior: however, it is pointing to default (original port) 8501.debug info - streamlit version: 0.63.0 - python version: python 3.6.9 - using pipenv - os version: ubuntu 18.04.4 lts - browser version: brave",Error,question
1731,https://github.com/streamlit/streamlit/issues/1731,"Typo at ""Tutorial: Create a data explorer app"" page",*you檒l notice that loaddata is a plain old function that downloads some ***,other,other
721,https://github.com/deepfakes/faceswap/issues/721,Can you make faceswap an integration package?,i hope that faceswap can be compressed like deepfacelab. this is a good thing for us beginners. now faceswap is too complicated our chinese players are basically using deepfacelab. because its use is very simple we also donated money to the deepfacelab author. i think you can also make faceswap easy to use.,other,other
709,https://github.com/iperov/DeepFaceLab/issues/709,Xseg images not fully loading on Mac,"expected behavior i am attempting to open the xseg editor for my ""head"" extracted images. i do this by entering the following command (my name excluded): `python main.py xseg editor --input-dir /users/[my name]/desktop/workspace/datadst/aligned` do note that this is taking place in a local anaconda environment, in which all requirements have been installed at the correct versions via conda (or pip if unavailable there). steps to reproduce to fully reproduce this error, use macos and attempt to run the first command given (adjusted for your system, of course) on a directory filled with images extracted as ""head"". the steps to reproduce the curious result are also detailed above. it is entirely possible that this issue merely has something to do with the fact that i am executing the commands directly from the command line, as i am required to do in lieu of the ability to run .bat files. or, it could be a macos compatibility issue, seeing as this issue has not appeared for my friends on windows. however, i cannot be certain of any of this due to the lack of any error messages or verbose dumping (i have attempted to force it to dump verbose, but those attempts have been unsuccessful; this can be tried again if required). other relevant information - ** 3.7.4",deployment,other
165,https://github.com/deezer/spleeter/issues/165,"[Bug] Critical issue with ""Serving"" temp dir","#82 description it seems like the ""serving"" temp dir doesn't get cleared causing the whole hard drive to fill up. step to reproduce reproduce the behavior: --> 1. use spleeter many times 2. done output environment ----------------- ------------------------------- os windows 10 installation type conda ram available 32gb hardware spec any",Error,Error
638,https://github.com/mozilla/TTS/issues/638,train_vocoder_wavernn.py have no model.generate function,"in the process of wavernn training, on line 293 of the trainwavernn.py file，use the model.generate function to create wav. but there is no such method in the wavernn.py file . it's a bug?!",Error,other
83,https://github.com/deezer/spleeter/issues/83,"[Discussion] I made model, so I wanna use it",`spleeter train -p configs/jazzexample.mp3 -o audio_output -p spleeter:4stems`,question,other
3210,https://github.com/streamlit/streamlit/issues/3210,Add support for Card component,please add support to render a single stat value in a component.,other,other
2179,https://github.com/streamlit/streamlit/issues/2179,Allow for Center of text etc.,"problem when using st.write, my text is aligned to the left. i would like to position it center. i read that you can use html today, but it will be deprecated. so looking for a more permanent solution. using columns is not always a solution imho.solution not an expert here, but could be a tag etc, that said positon=center, left, right etc.",other,other
78,https://github.com/streamlit/streamlit/issues/78,Allow st.selectbox to take a one column dataframe,low priority issueproblem st.selectbox throws an error when you pass it a dataframe even if that df only has one column. solution it should detect if you are passing a dataframe with only one column and accept that.,other,other
1516,https://github.com/streamlit/streamlit/issues/1516,Altair example of Bar Chart with rounded edges don't work in streamlit,"summary this code example in altair doc does not work with streamlitsteps to reproduce what are the steps we should take to reproduce the bug: my full code: expected behavior: to show the graphicactual behavior: give me this exception: is this a regression? no, the first time i try this graphicdebug info - streamlit version: streamlit, version 0.60.0 - python version: python 3.8.2 - using conda? nope, i'm using os python - os version: arch linux x86_64 - browser version: chromium 81.0.4044.122 arch linux",question,Error
263,https://github.com/iperov/DeepFaceLab/issues/263,error when running training script,"whenvever i tried to run the training script i gets these errors loading model... model first run. enter model options as default for each run. write preview history? (y/n ?:help skip:n) : target iteration (skip:unlimited/default) : 0 batchsize : 64 == == sortyaw : true == == randomloss : false == running on: == == [0 : geforce gtx 1060 6gb] ========================= starting. press ""enter"" to stop training and save model. error: oom when allocating tensor with shape[64,512,64,64] and type float on /job:localhost/replica:0/task:0/device:gpu:0 by allocator gpubfc hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. traceback (most recent call last): file ""c:\users\tranb\downloads\deepfacelabcuda10.1avx\internal\deepfacelab\models\modelbase.py"", line 404, in trainiter file ""c:\users\tranb\downloads\deepfacelabcuda10.1avx\liaef128\model.py"", line 80, in ontrainoneiter file ""c:\users\tranb\downloads\deepfacelabcuda10.1avx\oninternal\python-3.6.8\lib\site-packages\keras\backend\tensorflowinternal\python-3.6.8\lib\site-packages\keras\backend\tensorflowcall file ""c:\users\tranb\downloads\deepfacelabcuda10.1avx\internal\python-3.6.8\lib\site-packages\tensorflow\python\framework\errorsimpl.resourceexhaustederror: oom when allocating tensor with shape[64,512,64,64] and type float on /job:localhost/replica:0/task:0/device:gpu:0 by allocator gpubfc hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. hint: if you want to see a list of allocated tensors when oom happens, add reportallocationsoom to runoptions for current allocation info. done. press any key to continue . . .",question,question
2979,https://github.com/streamlit/streamlit/issues/2979,AttributeError: ‘JpegImageFile’ object has no attribute ‘apply_tfms’,please provide me a solution for the issue given in the link below or i getting this error when i trying to predict an image. please do provide me a solution. here is my code - %%writefile app.py import streamlit as st from fastai.vision import 100),Error,other
605,https://github.com/dusty-nv/jetson-inference/issues/605,Make the inference window size same as the video size?,when i run inference with a live webcam the window that contains the webcam feed with the bounding boxes takes the full screen while my webcam video is 640x480. so i end up with a huge black window and a tiny video in the top-left corner of the screen. is there a way to make that window be same size as the video?,question,question
643,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/643,sudo bash 3rdparty/osx/install_deps.sh fails,"issue summary i am running on a mac. `sudo bash 3rdparty/osx/installdeps.sh` type of issue you might select multiple topics, delete the rest: - compilation/installation error your system configuration 1. **: latest github code",deployment,question
369,https://github.com/dusty-nv/jetson-inference/issues/369,issue running imagenet-console.py ,"greetings, i am having an error when running imagenet-console.py. please see error below. jetson-inference/build/aarch64/bin$ ./imagenet-console.py --network=googlenet orange0.jpg jetson.inference.__.py"", line 4, in importerror: libjetson-utils.so: cannot open shared object file: no such file or directory also, all the python scripts are giving errors but the all the c++ example are working as expected. any help will greatly be appreciated.",question,question
698,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/698,compile error on Ubuntu 16.04,"i have attempted to build the openpose source code on aws p3.2xlarge instance with aws deep learning ami hardware: vcpu 8 memory 61gb nvidia tesla v100 the os was ubuntu 16.04 with almost all the prerequisites preinstalled: cuda 9 cudnn - not sure which ver. opencv - latest on github caffe - not sure coz both are preinstalled cmake 3.5.1 before the make command to build openpose, i configured the build and lib dir of opencv and caffe in cmake as follows opencv build dir : /usr/local/include/opencv/ opencv lib dir: /usr/local/include/ caffe build dir: /usr/local/include/ caffe lib dir : /usr/local/include/caffe2lib70'* > [src/openpose/cmakefiles/openpose.dir/hand/openposerenderhand.cu.o] > error 1 make[2]: *** > [src/openpose/cmakefiles/openpose.dir/face/openposerenderface.cu.o] > error 1 src/openpose/cmakefiles/openpose.dir/build.make:112: recipe > for target > 'src/openpose/cmakefiles/openpose.dir/net/openposeresizeandmergebase.cu.o' > failed make[2]: > [src/openpose/cmakefiles/openpose.dir/tracking/openposepyramidallk.cu.o] > error 1 cmakefiles/makefile2:179: recipe for target > 'src/openpose/cmakefiles/openpose.dir/all' failed make[1]: * > [src/openpose/hand/cmakefiles/openposehandrenderhand.cu.o] > error 1 cmakefiles/makefile2:344: recipe for target > 'src/openpose/hand/cmakefiles/openposehand.dir/all] error 2 > > src/openpose/face/cmakefiles/openposeface.dir/openposegeneratedface.dir/openposegeneratedface.dir/all' failed make[1]: > [src/openpose/face/cmakefiles/openposefilestream.dir/filesaver.cpp.o > [ 12%] building cxx object > src/openpose/filestream/cmakefiles/openposegpu.dir/cuda.cpp.o in file > included from > /home/ubuntu/openpose/include/openpose/filestream/headers.hpp:9:0, > from > /home/ubuntu/openpose/src/openpose/filestream/definetemplates.cpp:1: > /home/ubuntu/openpose/include/openpose/filestream/filestream.hpp:54:32: > error: ‘cvjpegimwritequality, 100, cvpngfilestream.dir/build.make:110: > recipe for target > 'src/openpose/filestream/cmakefiles/openposefilestream.dir/definetemplates.cpp.o] > error 1 make[2]: waiting for unfinished jobs.... [ 13%] building cxx > object src/openpose/gpu/cmakefiles/openposeimwritequality’ was not declared in this scope = > {cvjpegimwritecompression, 9}); ^ > compilation terminated due to -wfatal-errors. > src/openpose/filestream/cmakefiles/openposefilestream.dir/filestream.cpp.o' > failed make[2]: > [src/openpose/filestream/cmakefiles/openposegpu.dir/opencl.cpp.o > cmakefiles/makefile2:234: recipe for target > 'src/openpose/filestream/cmakefiles/openposefilestream.dir/all] error > 2 in file included from > /home/ubuntu/openpose/src/openpose/calibration/cameraparameterestimation.cpp:11:0: > /home/ubuntu/openpose/include/openpose/filestream/filestream.hpp:54:32: > error: ‘cvjpegimwritequality, 100, cvpngcalibration.dir/build.make:62: > recipe for target > 'src/openpose/calibration/cmakefiles/openposecalibration.dir/cameraparameterestimation.cpp.o] > error 1 make[2]: waiting for unfinished jobs.... > /home/ubuntu/openpose/src/openpose/calibration/gridpatternfunctions.cpp: > in function ‘void > op::improvecornerspositionsatsubpixellevel(std::vector >&, const > cv::mat&)’: > /home/ubuntu/openpose/src/openpose/calibration/gridpatternfunctions.cpp:22:52: > error: ‘cveps’ was not declared in this scope > cv::termcriteria{ cveps+cviter, 1000, 1e-9 }); ^ > compilation terminated due to -wfatal-errors. > src/openpose/calibration/cmakefiles/openposecalibration.dir/gridpatternfunctions.cpp.o' > failed make[2]: > [src/openpose/calibration/cmakefiles/openposecalibration.dir/all' > failed make[1]: ** [all] error 2",question,deployment
1747,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1747,[Win10] [Release v1.6.0] OpenPose does not show anything when starting pre-built binaries v1.6.0,"issue summary i want to try the pre-built binaries . when i execute the command from to test it with video, a window opens up but it stays gray/black. openposedemo continues to consume about 10% cpu load but nothing happens. after a time, the usual windows ""not responding"" appears. q: how can i run openposedemo? executed command (if any) from powershell: openpose output (if any) errors (if any) program stops responding (i can stop it with ctrl+c, though) type of issue - execution error - help wanted your system configuration 1. * release version v1.6.0 * system:",deployment,question
1684,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1684,"cudnn find in cmake,but not find in make","cmake .. find cudnn, but when i run make, it out puts cudnn not found. here is cmake out put . found cuda: /usr (found version ""10.2"") -- building with cuda. -- cuda detected: 10.2 -- added cuda nvcc flags for: sm61 -- cudnn : not found",deployment,question
1900,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1900,Windows error OpenPoseDemo.exe not found,"posting rules 1. ** with no further clarification. issue summary i was trying to get openpose working and i ran the code and got an error saying the program is unable to start. unable to start program 'openposedemo.exe', the system cannot find the file specified executed command (if any) note: add `--loggingmultigeneratedgeneratedgeneratedgenerated_bodypartconnectorbase.cu.obj 1>bodypartconnectorbase.cu 1>c:\users\ravin\openpose\include\openpose/core/matrix.hpp(204): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/string.hpp(39): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(31): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(56): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(64): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(85): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(94): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(104): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(118): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(129): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(136): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(143): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(150): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(157): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(165): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(172): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(179): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(186): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(194): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(217): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(223): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(229): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(239): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(250): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/matrix.hpp(204): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/string.hpp(39): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(31): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(56): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(64): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(85): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(94): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(104): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(118): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(129): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(136): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(143): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(150): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(157): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(165): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(172): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(179): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(186): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(194): warning : field of class type without a dll interface used in a class with a dll interface 1> 1>c:\users\ravin\openpose\include\openpose/core/datum.hpp(217): warning : field of class type without a dll interface used in a class with a dll int",question,question
1431,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1431,How to rebuild the third libaray caffe?,hello! i add new layers to caffe on window10. how to rebuild the third libaray caffe? i'm confused now,other,question
835,https://github.com/dusty-nv/jetson-inference/issues/835,[TRT]    Could not register plugin creator -  ::FlattenConcat_TRT version 1,"i am using the video tutorial on youtube to develop my first real time object detection. when i try to run the script, the following error show up `[trt] could not register plugin creator - ::flattenconcat_trt version 1 ` after the error, the camera never turns on and the terminal keeps on spitting values like this: after concat removal: 66 layers [trt] graph construction and optimization completed in 0.045886 seconds. [trt] constructing optimization profile number 0 [1/1]. [trt] --------------- timing runner: (reformat) [trt] tactic: 1002 time 0.747864 [trt] tactic: 0 time 1.23659 [trt] fastest tactic: 1002 time: 0.747864 [trt] --------------- timing runner: (reformat) [trt] tactic: 1002 time 20.0891 [trt] tactic: 0 time 1.31471 [trt] fastest tactic: 0 time: 1.31471 has anyone come up with the same issue and knows how to fix it? i'm running the script on the jetson nano 4gb development kit with jetpack 4.4 installed and using the raspberry pi camera v2. this is the actual script:",question,question
465,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/465,可以通过IP调用其它监控摄像头吗？命令是怎么样的？,"posting rules 1. **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? compiler (`gcc --version` in ubuntu):",other,other
208,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/208,Recreating heat-maps of pose model via Caffe - help wanted,"hello! i am trying to recreate heat-maps (network output only) using caffe, but i am getting a blurry image that is nowhere near the heat-maps created by the demo. what post processing do you do to get them? (for heat maps only, not skeletons). also, i cannot find the code that uses the caffemodel... i am using `models/pose/coco/poselinevec.prototxt` + `models/pose/coco/pose440000.caffemodel` running with pycaffe thanks!",question,question
130,https://github.com/dusty-nv/jetson-inference/issues/130,coco2kitty.py is missing,"the coco2kitty.py file linked to in the readme.md is not existing. the file can be found in the which is linked in the same section. i think the section should be edited to imply that or a link to the raw file should be there. i've already the coco dataset, so i was only interested in downloading the coco2kitty.py instead of the dataset once again.",Error,Error
833,https://github.com/dusty-nv/jetson-inference/issues/833,Is it possible to delete or overwrite the mounted directory?,"i'm sorry if my translation is wrong. i downloaded the fuit data from the open image dataset. i thought there were too many images downloaded. so i used the same code again to specify 2500 sheets. i specified the fruit folder, so i thought i'd overwrite it, but the number of images didn't decrease. is there a way to overwrite it? it was also mounted on the container and could not be removed. please tell me how to delete or overwrite the mounted directory. thank you",question,question
237,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/237,how to get keypoints in wrapper.hpp,i want to use keypoint value in wrapper.hpp now i'm try to use tdatums.... but it seems like impossible any one help me ... or any recommend not using datum....,question,question
556,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/556,"Feasibility query: Windows 10, QT, No GPU or Graphics Card","issue summary i am trying to develop a software for scientific study of movement. the intended use is on most basic desktop machines and so a slow performance is ok if it works at all. (even if it takes 1 sec for one image i am ok as far as proof of concept is required) my questions are related to all the components/ dependencies, necessary to build the library before i start developing the application. a bit fine grained questions are as follows: 1) is cuda and cudnn necessary for gpu only or it should be installed anyways, and is gpu necessary for openpose ? 2) can i develop an application which is selective to work on gpu if it is available and work anyways without it? does it depends upon how the library was built ? 3) in any case for a non gpu version what are all the components and in what order i need to build. i ask this because the page i was referring to (link below) , did say the process for ubuntu only if i want to use the api in my own project but there is no such thing for windows. i certainly want to use the api. i completed the steps until generate from cmake gui tool and getting error in compiling via vs which i am still trying to solve. one more thing; at the end i would like to use the api with qt as teh rest of application is written in that. executed command (if any) none openpose output (if any) none type of issue - help wanted - question your system configuration **: openpose default (only windows)",question,question
524,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/524,CUDNN_STATUS_SUCCESS (6 vs. 0)  CUDNN_STATUS_ARCH_MISMATCH,"i followed all the steps that the ""installation"" said, but the following error occured: auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. starting thread(s) f0406 20:11:35.007386 6912 cudnnlayer.cpp:53] check failed: status == cudnnsuccess (6 vs. 0) cudnnarchworld310.dll anyone can help me?",question,question
1807,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1807,Error when make -j`nproc`,[ 3%] built target openposex8632 against `.bss' can not be used when making a shared object; recompile with -fpic /usr/local/lib/libavformat.a: error adding symbols: bad value collect2: error: ld returned 1 exit status src/openpose/cmakefiles/openpose.dir/build.make:6827: recipe for target 'src/openpose/libopenpose.so.1.6.0' failed make[2]: ***** [all] error 2,question,question
264,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/264,CMAKE Link Library for openpose missing,"hello all, i am currently playing around with openpose on ubuntu 16.04. i need to build it with cmake which works fine, but i also need to access openpose as library. unfortunately i am not able to locate the openpose ""lib"" dir and also not the ""libopenpose.so"" after building with cmake . any one else with this issue? simon",deployment,question
862,https://github.com/dusty-nv/jetson-inference/issues/862,The examples flash the image so the image can not be seen,"i would add some more information to the example. first, when the example code is first ran, it appears to train the network. what is the input and output to the training; and how long does the training take? second, on the first, and subsequent runs, an image is flashed, but is not visible, and then the classification is printed with a percentage. is it possible to display the image long enough to see it? and how similar is the image to the training data? and, what does the output mean? here's the commands i'm referring to #: ./imagenet images/jellyfish.jpg images/test/jellyfish.jpg #: ./detectnet images/peds0.jpg",question,question
1034,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1034,Jetsen TX2 Usage  command dir error,"jetsen tx2 issue summary type of issue - other a dir error, missed ""examples"" command should be: ./build/*480 ...",Error,question
231,https://github.com/dusty-nv/jetson-inference/issues/231,Upgraded onboard camera; now problems with libjetson-inference.so,"hello @dusty-nv, using a jetson tx2 with the original onboard camera, i was able to perform the live camera detection demo in the tutorial! however, i tried upgrading the onboard camera to an e-cam31_tx2 by e-con systems. (in doing so, i had to re-flash the jetson tx2 with the appropriate camera drivers). now, when i perform a command such as the one below, i get an error about the libjetson-inference.so library: the requested library seems to be located in the `lib` folder, all by itself:",question,question
648,https://github.com/dusty-nv/jetson-inference/issues/648,How to resize captured RTSP stream to smaller resolution using videoSource,"hi, can we use an argument with rtsp stream to resize the captured stream to smaller resolution size? is there an ""args"" list to see what can we use? thanks.",question,question
535,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/535,   Questions about keypoints.json files ,i have successfully run openpose.cpp. i want to know what the numbers in parentheses in posestring (writejson). is there a manual to explain keypoint_json's data file?,question,question
1612,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1612,json to bvh?,can we transfer json to bvh?,other,other
377,https://github.com/dusty-nv/jetson-inference/issues/377,"""GST_ARGUS: Available Sensor modes : Segmentation fault (core dumped)"" on both python and C examples","hi i tried the imagenet-camera according to all the instructions . i am using a logitech c310 webcam with a jetson nano. i already looked at the similar issue which was solved by changing defaultcamera index and it worked. but the new version throws errors. i tried running the following commands: ""./imagenet-camera"" ""./imagenet-camera --camera 0"" also with python: ""python3 imagenet-camera.py"" ""python3 imagenet-camera.py -- camera 0"" ""./imagenet-camera"" output: here is the python output:",question,question
1553,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1553,How we can run the batch (batch size >=2 ) inference in python?,posting rules 1. ** issue:,question,question
1753,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1753,OpenPose Build/Compilation Error,"i have an issue building openpose in build directory with command: make -j`nproc`, even after successful configuration and generation in cmake and cloning of caffe in 3rd party directory. the following output appears in the terminal: [ 12%] performing build step for 'openposecompilecompilegeneratedlayer.cu.o [ 2%] building nvcc (device) object src/caffe/cmakefiles/cuda1.dir/layers/cuda1accuracycompilecompilegenerateddatacompilecompilegeneratednormcompilecompilegeneratedfunctions.cu.o [ 2%] building nvcc (device) object src/caffe/cmakefiles/cuda1.dir/layers/cuda1bnllcompilecompilegeneratedlayer.cu.o [ 4%] building nvcc (device) object src/caffe/cmakefiles/cuda1.dir/layers/cuda1batchlayer.cu.o nvcc fatal : unsupported gpu architecture 'compute80' cmake error at cuda1baselayer.cu.o.release.cmake:220 (message): error generating /home/hisham246/openpose/build/caffe/src/openposecompilecompilegenerateddatacompilegeneratedlayer.cu.o.release.cmake:220 (message): error generating /home/hisham246/openpose/build/caffe/src/openposecompilecompilegeneratedlayer.cu.o error generating /home/hisham246/openpose/build/caffe/src/openposecompilecompilegeneratedfunctions.cu.o make[5]: **************** nvidia geforce mx150 cuda 10.1 cudnn 7.5.1 cmake 3.18.4 opencv 3.2",question,question
190,https://github.com/dusty-nv/jetson-inference/issues/190,TX2/JetPack 3.2 - CMakePreBuild.sh - broken 'glew-utils' package is breaking the cmake build,i'm taking the udacity robotics nd and this is the sample project for one of the labs. i'm using the tx2 with jetpack 3.2. the following line in the cmakeprebuild.sh is breaking b/c the 'glew-utils' package is not being found:,question,question
940,https://github.com/dusty-nv/jetson-inference/issues/940,unable to build within the ROS noetic docker container,i am having an issue building jetson-inference within a ros noetic docker container. i can build it on the native system (i am using an xavier nx) but when i try and build it within the the ros noetic docker container i am getting this error: [ 69%] linking cxx shared library aarch64/lib/libjetson-inference.so /usr/bin/ld: cannot find -lnvcaffe_parser collect2: error: ld returned 1 exit status cmakefiles/jetson-inference.dir/build.make:257: recipe for target 'aarch64/lib/libjetson-inference.so' failed make[2]: ***** [all] error 2 any idea on what i can do to fix this? thank you,question,question
706,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/706,Getting join information ,is it possible to get information about the joins in the pose i.e. which lines connect from a certain body parts to an other. such as from shoulder to elbow etc..,other,question
1751,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1751,"Openpose-python_api, how do you run this for videos? ","how exactly would you go about running the python_api for videos, the tutorials are good for images but there is not much documentation i can find for implementing it for videos, any help would be much appreciated!!",question,question
36,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/36,How can i use thess Body parts for pose estimation?,"how can i use thess body parts for pose estimation?like dace,runing,fitghting or walk etc thanks",question,question
999,https://github.com/dusty-nv/jetson-inference/issues/999,Saving the coordinates of bounding boxes of a self-trained model,"hi @dusty-nv! i have trained my own model to detect baseball using transfer learning on jetson nano by following your tutorials! thanks a lot for that! now, i want to save the coordinates of bounding boxes while detecting. basically, i need them to find the center of the ball. kindly help me as soon as possible. i will be highly obliged!",question,other
1233,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1233,Understanding the op::Wrapper and its workers,"dear people, i need some clearance about how the `op::wrapper` and its workers are scheduled. my setup is the following: i got already a dedicated thread for openpose in my application. therefore i decided to use the `op::wrapper` and call it's `exec()` function. i implemented a custom `wuserinput` class derived from `op::workerproducer` and a custom `wuseroutput` class derived from `op::workerconsumer`. in `wuserinput::workproducer` everytime a frame is set from the outside a `op::datum` ptr is returned, like in examples/tutorialuser_synchronous.cpp. in `wuseroutput::workproducer()` keypoints are only send to the standard output for now. i'm adding these two workers via `setworkerinput()` / `setworkeroutput()` to the `op::wrapper`. second argument `workeronnewthread` is set to false. so i expected that the inputworker, the actual openpose worker, and the output worker are executed in the same thread one after another. but actually they are executed in different threads and i dont understand why. even the `wuseroutput::workconsumer` function is executed in an infinite loop concurrently with the `wuserinput::workproducer`. can someone shed some light on this? i'm working with openpose v1.3. thanks!!",question,question
119,https://github.com/dusty-nv/jetson-inference/issues/119,use the middle layer as output?,"i'm trying detect object by use ./detectnet-console and ./detectnet-camera can i use the middle layer as output? for example, ""conv2/3x3[34]"" ( i want to output 34th in conv2/3x3 layers.) $ net=/home/nvidia/jetson-inference/build/aarch64/bin/detect $ ./detectnet-console test.bmp output.bmp --prototxt=$net/deploy.prototxt --model=$net/test.caffemodel --inputcvg = conv2/3x3[34] --output_bbox=bboxes --threshold=200 detectnet -- loading detection network model from: [gie] attempting to open cache file /home/nvidia/jetson-inference/build/aarch64/bin/detect/test.caffemodel.2.tensorcache [gie] cache file not found, profiling network model [gie] platform has fp16 support. [gie] loading /home/nvidia/jetson-inference/build/aarch64/bin/detect/deploy.prototxt /home/nvidia/jetson-inference/build/aarch64/bin/detect/test.caffemodel [gie] failed to retrieve tensor for output 'conv2/3x3[34]' [gie] retrieved output tensor 'bboxes' [gie] configuring cuda engine [gie] building cuda engine segmentation fault (core dumped) thanks.",question,question
1898,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1898,can't open file for writing: permission denied,"please help me. according to 01_demo.md, i am trying to save the visual output of openpose. i have ensured the write permission remains activated, but this error happened any idea? thanks!",question,question
421,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/421,"Unresolved external symbol ""void __cdecl op::wrapperConfigureSecurityChecks .......","issue summary i am unable to build after generating the solution with cmake. i have run all the batch files found in 3rdparty/windows/. cmake generates the files without error. when i try to build openpose.sln, it throws me the following error. other setups i have tried (with no success); 1. visual studio 2017 community with cuda 9.1 2. building openpose with caffe/bvlc instead of the libraries from the included downloaders openpose output (if any) type of issue - compilation/installation error your system configuration **: default from openpose compiler (`gcc --version` in ubuntu): cl.exe 2015 x64",Error,question
735,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/735,Pretrained Model,"hello, i want to use the open pose model as a pre-trained model, can you help me with that? i mean where i can find the pretrained model? thanks,",other,other
427,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/427,question about the Neural Net ouput format at the last layer,"posting rules 1. ** with no further clarification. issue summary after evaluating the framework, i'm trying to use the neural nets to do a transfer learning for my specific problem. i'm a little confused about the output heatmap format at the end of the network. the shape of the output in my case is (1, 57, 46, 82). my question is on the number, 57. since i was using the coco model, so i should have 18 joints + 1 background, so 19. and because it is rgb, so it is 19x3 = 57. my question is is the heatmap ordered as r: [0]. g:[19], b[38], or r[0], g:[1], b[2]? i tried the do the scatter plot for both cases, but neither of them overlapping, so i dont know which assumption is correct. please advise thank you",question,question
486,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/486,Cmake openpose problem (cuDNN not found),"hi, “ sudo ubuntu/installcmake.sh“ - gcc detected, adding compile flags -- building with cuda. -- cuda detected: 8.0 -- added cuda nvcc flags for: sm64-linux-gnu/libgflags.so) -- found glog (include: /usr/include, library: /usr/lib/x86version=""$(lsburl="" wget -c ${cudnncudnn to off. see also ""/home/ubuntu/openpose/cmakefiles/cmakeoutput.log"". see also ""/home/ubuntu/openpose/cmakefiles/cmakeerror.log"". but the cudnn is already there, which is under the /usr/local/cuda path. i also added the ""export cudnndir=/usr/local/cuda/include export cudnn_library=/usr/local/cuda/lib64/libcudnn.so"" to the .bashsrc file and source it. but still doesn't work. regards, cn",question,question
86,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/86,Unable to use heatmaps + heatmaps questions,"issue summary when i ran openpose on a full hd(1920x1080) frame it dumps a heatmap of size 24928x368. in this frame it has detected just one person and i have used --heatmapspafs flag with which i expect it dumps 2x19 pafs wrto all the limbs. can you help me make sense out of this heatmap output? i understand that it is the unit vector for each limb but in what form it is stored that i am unable to figure out. executed command (if any) ./build/examples/openpose/openpose.bin --imageheatmaps output/heatmaps/gopr0114/ --heatmapspafs ""true"" --noparts"":[ ] } ] } type of issue - help wanted - question",Performance,question
1341,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1341,Skeleton thickness don't stay homogeneous at all resolutions.,"this affects every openpose version regardless of the os, machine or configuration. if i output a skeleton from a 1920 x 1080 photo ( ) and resize it to 1280 x 720, then output a skeleton from the same exact photo resized at 1280 x 720 ( ), the resulting bone thickness between the two skeletons will appear very different when compared together ( ), this happens especially on distant people ( if you want to reproduce this issue: picture at 1920 x 1080 same picture at 1280 x 720 ), how can i make sure the bone thickness always stay consistently the same when using the same footage but at different resolutions? i believe data consistency is very important for any scientific scenario, in fact this can be a pure nightmare for deep learning, imagine having several low resolution pictures which you cannot detect the skeleton from, but the high resolution ones will, however, since the model has been trained on the low resolution pictures, the resized pictures will output an erroneous result due to the data inconsistency, i've seen stuff like this happen for not noticing a 1px border completely invisible to the naked eye, imagine how many things could go wrong when the thickness difference is so noticeable. i'd really appreciate any ideas or solutions to solve this, thanks!",Performance,question
392,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/392,OpenPose in Unity,"i would run openpose in unity and i tried to use ""1fromchandler 0x00007ffb61712d03 (vcruntime140) invokejitinvoke 0x00007ffb233a8a19 (mono) [c:\buildslave\mono\build\mono\metadata\object.c:2623] monoinvoke 0x0000000140a2eaec (unity) scriptinginvoke 0x0000000140a2243a (unity) scriptinginvocation::invoke 0x00000001409eb7df (unity) monobehaviour::invokemethodorcoroutinechecked 0x00000001409ed356 (unity) monobehaviour::invokemethodorcoroutinechecked 0x00000001409eeba8 (unity) monobehaviour::start 0x00000001409ef199 (unity) monobehaviour::delayedstartcall 0x00000001405031b5 (unity) delayedcallmanager::update 0x000000014071f87c (unity) `initplayerloopcallbacks'::`34'::earlyupdatescriptrundelayedstartupframeregistrator::forward 0x000000014071cb4e (unity) playerloop 0x00000001411de245 (unity) playerloopcontroller::updatescene 0x00000001411e85fa (unity) playerloopcontroller::enterplaymode 0x00000001411e8e76 (unity) playerloopcontroller::setisplaying 0x00000001411e9cc6 (unity) application::ticktimer 0x000000014140eb0f (unity) mainmessageloop 0x00000001414103b5 (unity) winmain 0x0000000141e643a8 (unity) __tmaincrtstartup 0x00007ffb66942774 (kernel32) basethreadinitthunk 0x00007ffb67190d51 (ntdll) rtluserthreadstart ========== end of stacktrace =========== thanks! george",question,other
1117,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1117,"netInputSize VS netOutputSize  . The resolution of the training picture can be different: 1, 2, 3 pictures in the same batchsize are not the same size.","the resolution of the training picture can be different: 1, 2, 3 pictures in the same batchsize are not the same size？ 2.训练图片 分辨率 可以 不一样：同一个batchsize 里的 1，2，3 张图片 每张尺寸都不一样",question,question
1652,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1652,Is it possible to train a single openpose network for pose detection on both human and animals?,posting rules 1. ** issue:,other,question
919,https://github.com/dusty-nv/jetson-inference/issues/919,How to mark the bounding box location of my own images without using camera-capture?,"dear dusty this is stephen lee. thank you for the youtube(jetson ai fundamentals - s3e5). ** so, the information would be xml data within bounding box location information from my own images.(like the yolo marker) references: [1] jetson ai fundamentals - s3e5: [2]",question,question
509,https://github.com/dusty-nv/jetson-inference/issues/509,"jetson.utils.cudaToNumpy() fails with ""failed to get array pointer from PyCapsule container""","prints ""exception: jetson.utils -- cudatonumpy() failed to get array pointer from pycapsule container""",Error,question
352,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/352,Please add freeglut download to get* scripts,"freeglut library is used on openpose3dreconstruction sample, but the libs downloading scripts do not include it. if i had write permissions on the repo (e,g: i would upload it myself and send a pull request. freeglut build for windows (not by me) can be found here:",deployment,other
980,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/980,int should be replace with float in poseTriangulation.cpp,"following line in posetriangulation.cpp is using 1310720 in denominator. `const auto imageratio = std::sqrt(imagesizes[0].x 1024 dimension, imageratio will be 0 and it will always throw following message without generating 3d points `unusual high re-projection error (averaged over #keypoints) of value 4.392997 pixels, while the average for a good openpose detection from 4 cameras is about 2-3 pixels. it might be simply a wrong openpose detection. if this message appears very frequently, your calibration parameters might be wrong.` if the denominator is changed to float value i.e. 1310720.0 it works. but, i think this should be fix with a better logic instead of providing constant value in denominator.",Error,Performance
1819,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1819,Compilation fails in Ubuntu 20.04,i installed boost and still get an error:,question,deployment
657,https://github.com/dusty-nv/jetson-inference/issues/657,Where to change dataset type on camera-capture?,"i read the document to make a dataset of mobilenetv2-ssd. when i tried to do this following ""jetson-inference/docs/pytorch-collect-detection.md"", i couldn't find an option to change dataset type on ""data capture control"" after executing ""camera-capture --camera /dev/video0"". it seems the appearance is different. what i tried first is to update this repo, since i cloned this for long time ago, but this didn't work. do i miss something?",question,question
1397,https://github.com/dusty-nv/jetson-inference/issues/1397,"opencv, CSI camera, gstreamer crashes","i am trying to use the videocapture functio from opencv2 with a imx219 csi camera onboard, but i have been unable to open the camera after many tries. i have followed code from here: trying to use a gstreamer like: `“nvarguscamerasrc ! video/x-raw(memory:nvmm), width=(int)1920, height=(int)1080,format=(string)nv12, framerate=(fraction)30/1 ! nvvidconv ! video/x-raw, format=(string)bgrx ! videoconvert ! appsink”` and also from here: but in both cases the camera fails to open. even trying to launch camera from command line using: gst-launch-1.0 nvarguscamerasrc ! nvoverlaysink returns an error of:",question,question
1134,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1134,[CPU version] Examples do not work as-is,"issue summary as discussed in the examples (e.g. 01fromdefault.cpp) do not work in cpu mode without adding this snippet: errors (if snippet is omitted) - by default, the wrapper's structs have `rendermode = gpu`: - thus, `renderoutputgpu` is `true`: - as a result, adds `posegpurenderer`s - eventually, `renderpose` is called on these `posegpurenderer`s: - `posegpurenderer::renderpose` finally prints the aforementioned error at type of issue - execution error your system configuration 1. ** system: self compiled version",Error,Error
1919,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1919,--tracking parameter doesn't work,"what --tracking parameter is used for? when i use this parameter, i got the following error: starting openpose demo... configuring openpose... starting thread(s)... person tracking (`tracking` flag) is in experimental phase. please, let us know if you find any bug on this alpha version. opencv error: assertion failed (prevpyr[level lvlstep2].size()) in calc, file /build/opencv-l2vumj/opencv-3.2.0+dfsg/modules/video/src/lkpyramid.cpp, line 1365 error: /build/opencv-l2vumj/opencv-3.2.0+dfsg/modules/video/src/lkpyramid.cpp:1365: error: (-215) prevpyr[level lvlstep2].size() in function calc - /openpose/src/openpose/tracking/pyramidallk.cpp:pyramidallkocv():458 - /openpose/src/openpose/tracking/persontracker.cpp:updatelk():125 - /openpose/src/openpose/tracking/persontracker.cpp:track():531 - /openpose/src/openpose/tracking/persontracker.cpp:tracklockthread():550 - /openpose/src/openpose/pose/poseextractor.cpp:tracklockthread():229 - /openpose/include/openpose/pose/wposeextractor.hpp:work():107 - /openpose/include/openpose/thread/worker.hpp:checkandwork():93 error occurred on a thread. openpose closed all its threads and then propagated the error to the main thread. error description: /build/opencv-l2vumj/opencv-3.2.0+dfsg/modules/video/src/lkpyramid.cpp:1365: error: (-215) prevpyr[level lvlstep2].size() in function calc - /openpose/src/openpose/tracking/pyramidallk.cpp:pyramidallkocv():458 - /openpose/src/openpose/tracking/persontracker.cpp:updatelk():125 - /openpose/src/openpose/tracking/persontracker.cpp:track():531 - /openpose/src/openpose/tracking/persontracker.cpp:tracklockthread():550 - /openpose/src/openpose/pose/poseextractor.cpp:tracklockthread():229 - /openpose/include/openpose/pose/wposeextractor.hpp:work():107 - /openpose/include/openpose/thread/worker.hpp:checkandwork():93 - [all threads closed and control returned to main thread] - /openpose/src/openpose/utilities/errorandlog.cpp:checkworkererrors():280 - /openpose/include/openpose/thread/threadmanager.hpp:stop():243 - /openpose/include/openpose/thread/threadmanager.hpp:exec():202 - /openpose/include/openpose/wrapper/wrapper.hpp:exec():424 when i remove the --tracking, it works fine. but what --tracking is working for? any impact? many thanks in advance!",question,question
1002,https://github.com/dusty-nv/jetson-inference/issues/1002,"Viewing RTP Remotely with GStreamer  - WARNING: erroneous pipeline: no element ""udpsrc""","thanks for all your excellent work! i run jetson nano csi camera with latest jetpack 4.5.1. i would like to ask for an advice on streaming with gstreamer. i can stream rtp from jetson to pc with vlc, however latency is very high.high. when i run on ubuntu pc: `gst-launch-1.0 -v udpsrc port=1234 \ caps = ""application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)h264, payload=(int)96"" ! \ rtph264depay ! decodebin ! videoconvert ! autovideosink` i get : `warning: erroneous pipeline: no element ""udpsrc""` when i run command with `sudo` i get message below, however streaming window doesn't appear: ` sudo gst-launch-1.0 -v udpsrc port=1234 caps = ""application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)h264, payload=(int)96"" ! rtph264depay ! decodebin ! videoconvert ! autovideosink setting pipeline to paused ... error: xdgdir not set in the environment. pipeline is live and does not need preroll ... got context from element 'autovideosink0-actual-sink-vaapi': gst.vaapi.display=context, gst.vaapi.display=(gstvaapidisplay)""\(gstvaapidisplaydrm\)\ vaapidisplaydrm2""; /gstpipeline:pipeline0/gstudpsrc:udpsrc0.gstpad:src: caps = application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)h264, payload=(int)96 /gstpipeline:pipeline0/gstrtph264depay:rtph264depay0.gstpad:sink: caps = application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)h264, payload=(int)96 setting pipeline to playing ... new clock: gstsystemclock /gstpipeline:pipeline0/gstrtph264depay:rtph264depay0.gstpad:src: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline /gstpipeline:pipeline0/gstdecodebin:decodebin0/gsttypefindelement:typefind.gstpad:src: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstqueue:vaapi-queue: max-size-time = 0 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstqueue:vaapi-queue: max-size-buffers = 0 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstqueue:vaapi-queue: max-size-bytes = 0 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0.gstghostpad:sink.gstproxypad:proxypad3: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0.gstghostpad:sink: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstcapsfilter:capsfilter0.gstpad:sink: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline /gstpipeline:pipeline0/gstdecodebin:decodebin0/gsttypefindelement:typefind.gstpad:sink: caps = video/x-h264, stream-format=(string)avc, alignment=(string)au, codecdata=(buffer)01424028ffe1000a6742402895a014016e4001000468ce3c80, level=(string)4, profile=(string)constrained-baseline redistribute latency... /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstvaapidecode:vaapidecode0.gstpad:src: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstqueue:vaapi-queue.gstpad:sink: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstqueue:vaapi-queue.gstpad:src: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstcapsfilter:capsfilter1.gstpad:src: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0/gstvaapipostproc:vaapipostproc0.gstpad:src: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0.gstghostpad:src: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstvideoconvert:videoconvert0.gstpad:src: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstautovideosink:autovideosink0.gstghostpad:sink.gstproxypad:proxypad1: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstautovideosink:autovideosink0/gstvaapisink:autovideosink0-actual-sink-vaapi.gstpad:sink: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstautovideosink:autovideosink0.gstghostpad:sink: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstvideoconvert:videoconvert0.gstpad:sink: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstdecodebin:decodebin0.gstdecodepad:src0: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstvideoconvert:videoconvert0.gstpad:sink: caps = video/x-raw, format=(string)nv12, width=(int)1280, height=(int)720, framerate=(fraction)0/1, interlace-mode=(string)progressive, pixel-aspect-ratio=(fraction)1/1, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, chroma-site=(string)mpeg2, colorimetry=(string)bt709 /gstpipeline:pipeline0/gstdecodebin:decodebin0.gstdecodepad:src0: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstvideoconvert:videoconvert0.gstpad:src: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstautovideosink:autovideosink0.gstghostpad:sink.gstproxypad:proxypad1: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstautovideosink:autovideosink0/gstvaapisink:autovideosink0-actual-sink-vaapi.gstpad:sink: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstautovideosink:autovideosink0.gstghostpad:sink: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstvideoconvert:videoconvert0.gstpad:sink: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0.gstdecodepad:src_0.gstproxypad:proxypad2: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 /gstpipeline:pipeline0/gstdecodebin:decodebin0/gstvaapidecodebin:vaapidecodebin0.gstghostpad:src.gstproxypad:proxypad4: caps = video/x-raw(memory:vasurface), format=(string)nv12, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(gstvideomultiviewflagsset)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction)0/1 ` i tried this advice however it didn't work any idea how to fix the issue?",question,question
778,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/778,Running OpenPose GPU version on server cannot infer,"issue summary i have built successfully on my server(ubuntu 16.04, cuda9.0, cudnn 7.1.4). however, i ran the video example and do not have any lines on example video. executed command (if any) note: add --loggingmultigpu 4 --numstart 0 --video examples/media/video.avi --write_video output.avi openpose output (if any) openpose output video do not have any difference between original video. your system configuration openpose version: latest github code general configuration: installation mode: sh script operating system : ubuntu release or debug mode? (by default: release): release compiler (gcc --version in ubuntu or vs version in windows): 5.4.0, ... (ubuntu)",other,question
1713,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1713,2D keypoints from openpose is in different place when viewed using Realsense SDK [Windows],"posting rules 1. ** issue: i am trying to get 2d human keypoints from openpose so i can get 3d keypoints after passing it through realsense sdk. however, the points i am getting is either too low or too high when using realsense. coded using python. the openpose part is coded with '--writescale2/ --keypointcandidates --netjson outputcamres/ --keypointcandidates --camera_resolution 640x480' the realsense part is left the same besides changing the coordinates. any help is appreciated, thank you!",question,question
288,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/288,"""Starting thread(s)"" -> ""Killed"" error on Jetson TX1 after running test on images at reduced net_resolution","issue summary 1. installed jetpack 3.1 according to this: 2. ran scripts in ubuntu folder to install cuda and cudnn. 2. ran ""./ubuntu/installandjetsontx2resolution, i get a ""killed"" error message about a minute later. executed command (if any) `./build/examples/openpose/openpose.bin --imageresolution 80x32 --logging_level 0` openpose output (if any) type of issue - execution error - help wanted your system configuration **: default",question,question
582,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/582,The optimal way to get depth value(z) of the body key points with Realsense Camera,issue summary there are lot of people asking questions about the depth info of the nodes. i am sorry for adding another one on top of the pile. i use openpose with realsense sr300 camera. rviz can display the point cloud or camera itself is capable of delivering depth information. what would be the way to obtain the depth value or openpose has already become capable of it? type of issue - help wanted - question your system configuration **: 3.2 compiler (`gcc --version` in ubuntu): ubuntu 5.4.0-6ubuntu1~16.04.9 5.4.0 20160609,question,question
496,https://github.com/dusty-nv/jetson-inference/issues/496,How can i use a external camera? (basler),"hello, i would like to use this repository for make inferences in a jetson tx2 with a blaster camera. in fact, this is my first time working with a jetson so i have a little bit of confusion. actually, i decided to install pypylon, which is the recommended software but i dont know really how to do it work with the examples of ""create your own object detection program"" (using python).",question,question
1775,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1775," Detected 1 GPU(s), using 1 of them starting at GPU 0.","auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. e1128 18:28:22.757053 9960 common.cpp:123] cannot create cublas handle. cublas won't be available. e1128 18:28:22.768019 9960 common.cpp:130] cannot create curand generator. curand won't be available. f1128 18:28:22.772009 9960 common.cpp:161] check failed: error == cudasuccess (3 vs. 0) initialization error ****",question,deployment
1598,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1598,Error: Cuda check failed (30 vs. 0): unknown error,"i am getting when running openpose. installation on ec2 instance running ubuntu 16.04. cuda version 10.0 and cudnn 5.1. any help with this would be much appreciaited! thanks, amrita",question,question
97,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/97,Stability issues on Ubuntu 16.04,"issue summary occasional segmentation fault when running openpose.bin demo - about every other time. when it works, all functionality seem to work well and processing continues till the end of the video. when it crashes - it happens immediately when the demo starts (nothing else is shown). executed command (if any) ./build/examples/openpose/openpose.bin --video examples/media/video.avi -numrelease -a` on ubuntu): ubuntu 16.04.2 lts **: 3.2.0 compiler (`gcc --version` on ubuntu): gcc (ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609",deployment,other
1267,https://github.com/dusty-nv/jetson-inference/issues/1267,Extending repository with other models,"hello dustin, i was looking at the possibility of running an onnx model containing a yolov3 network using this repository. as expected, simply loading in the onnx file did not give the expected results. looking at the post-processing for onnx (), it seems that onnx comes with the ssd post-processing algorithm. this algorithm differs from the yolov3 post-processing algorithm. am i correct in saying that it is assumed that, for detectnet, an onnx file always contains a model with an ssd head? best regards.",other,other
4,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/4,No output displayed or it gets stuck - OpenCV issue,"my terminal isn't responding after executed the command below: `./build/examples/openpose/rtpose.bin --imagelevel 1 --netcaffeopenpose.sh` , which was provided by official",question,other
954,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/954,C++ Tutorial 7_synchronous_custom_input.bin Seg Faults due to producer nullptr dereference,"issue summary i just pulled the latest version of openpose and i started getting a seg fault on the c++ api tutorial 7. i have also implemented my own version which also seg faults. i did a bit of debugging myself and it appears there is no check for null around `const auto numberviews = (intround(producersharedptr->get(producerproperty::numberviews)));` at line 711 of wrapperauxilary.cpp. i am running the default example so i presume this is a legal use case. this only started happening since i pulled 5295f2f38609a6ff24739b37cf348b9c9324faef. previously i had not pulled since 26 nov 2018, and this worked for me. executed command (if any) `./build/examples/tutorialcpp/7* issue:",Error,Error
481,https://github.com/dusty-nv/jetson-inference/issues/481,How to use gige camera?,"i wanna use external gige camera instead of mipi. i can connect gige camere with jetson tx2, but can't make segnet recognize the gige. how to do it?",question,question
719,https://github.com/dusty-nv/jetson-inference/issues/719,Quick Question regarding detectnet.py and segnet.py,"hi i am extremely new to using the jetson tx2, and for this project i am working on, i think the best approach to reach my team's goals is to modify the python examples, really our only issue is we're not exactly sure what we are supposed to edit to change the camera from the onboard tx2 camera to our zedd usb. any suggestions to understanding these two programs line by line?",question,other
720,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/720,Feet,"help wanted hi - the inclusion of foot detection is massively important for us, but it's very unclear from the doc how i would go about getting the extra keypoints to correspond with feet. do i need to load a different model, and if so which? does openposedemo in the v1.3 build include appropriate options or could openpose.cpp be modified to do so? all help welcome thanks mike pelton",question,other
911,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/911,integrate openpose in Qt with VS2017 Compiler,"issue summary hey, i try to integrate openpose in qt and integrated the libs and dlls in the .pro project file. apparently i get some errors and i hope you can help me. i'm not able to compile my project because i get error ""reference to unresolved external symbol"" for 30 files. i tried to build it with the cmakelist.txt before and it was successfull with msvc2017 compiler in qt. but i want to use qt creator and that's why i want to include it via the .pro file into qt. i copied the code of the poseextractimage example. i also tried the poseextractorfrom_image and got the same errors. errors (if any) reference to unresolved external symbol... image: [deleted] type of issue - compilation/installation error - execution error - help wanted your system configuration 1. ** issue: my qt .pro file: thanks for your help :-)",question,question
1155,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1155,"Can't use ""--body_disable""","hi! i have the following errors when running the example: 07fromapihandimage.bin --bodydetector 2"", it showed me ""error: unknown command line flag 'bodyapihandimage.bin --hand --hand_detector 2"", it worked. what should i do? best regards,",Error,question
552,https://github.com/dusty-nv/jetson-inference/issues/552,Save a video after detection,"hi @dusty-nv , i'm trying to use ssd-mobilenet-v2 with a pre-recorded video, and i want to save it with the detections. my problem is i don't know how to save it after the detections. i'm trying to use videowriter, but it accepts a specific variable. i start with your code from ""coding your own object detection program"", i know that videowriter accept a variable a the same type as ""frame"". but i don't know how to have this type after the detections applied. i saw that the variable transforms like this : frame -/cv2.cvtcolor/-> framedetectionpropwidth, width ) vid.set(cv2.capframepieton.avi',cv2.videowriterrgba = cv2.cvtcolor(frame, cv2.colorrgba) detections = net.detect(img, width, height) #affichage et detail display.renderonce(img, width, height) display.settitle(""object detection network {:.0f} fps"".format(net.getnetworkfps())) writerx.write(newf) ` sincerely,",question,question
354,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/354,How to set custom picture data(video frame) form memory to find body？,"issue summary how to set custom picture data(video frame) form memory to find body？ i see there is a image_dir to set path or camera to use opencv capture ,but i want set picture data(video frame) from my custom memory,how to do this? i use directshow to capture webcamare frame, and i want use this frame to find body,not use opencv capture.how can i do this? type of issue - question your system configuration **: openpose default compiler :vs2015",question,question
1094,https://github.com/dusty-nv/jetson-inference/issues/1094,Crash on running the train_ssd.py,"hi dustin, i followed your tutorial for ""training the ssd-mobilenet model"" i used the to annotate the images and provide the data. i followed the video tutorial for configuring directories and content for retraining the ssd model. when i execute the python3 trainimages_downloader.py and it also resulted in an illegal instruction (core dumped) error. i've built the source code of jetson-inference library on my jetson nano and installed it successfully as per procedure. can you please help? thanks ! kashyap",question,question
1179,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1179,symbol lookup error: ./build/src/openpose/libopenpose.so.1.4.0: undefined symbol,"sun@sun-thinkpad-edge-e431:~/openpose$ ./build/examples/openpose/openpose.bin starting openpose demo... configuring openpose... starting thread(s)... auto-detecting camera index... detected and opened camera 0. auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. ./build/examples/openpose/openpose.bin: symbol lookup error: /home/sun/openpose/build/src/openpose/libopenpose.so.1.4.0: undefined symbol: stringicst11char_traitsicesaiceee i have finished installation, but when i want to run the quickstart, there's some wrong. i can't solve it, could anyone help me?",question,question
107,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/107,File not found: models/pose/coco/pose_deploy_linevec.prototxt - Windows,"after build , i run project with video : openposedemo.exe --video video.avi examples/media/video.avi , program crash and has message erro : f0628 13:38:02.780624 4956 io.cpp:41] check failed: fd != -1 (-1 vs. -1) file not found: models/pose/coco/poselinevec.prototxt ****",question,deployment
18,https://github.com/dusty-nv/jetson-inference/issues/18,Compiling with -std=c++11 gives errors,"i am using 14.04, with tensorrt etc installed(custom jetson carrier boards) any suggestions to fix it ?",question,question
263,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/263,Small training batch?,"when i see the training code, it has smalll batch size of 10. is there any reason for this? if i raise this value to 128 for fast training, is it okay?",question,question
909,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/909,Compilation fails on Ubuntu 18.04 with undefined reference to symbol '_ZN2cv6String10deallocateEv',"issue summary i'm attempting to install openpose on ubuntu 18.04 with caffe and opencv built from source. build failing when compiling calibration example, with "" undefined reference to symbol '_zn2cv6string10deallocateev'"" executed command (if any) and then `make` produces no errors until: type of issue - compilation/installation error - help wanted your system configuration 1. **:",other,deployment
993,https://github.com/dusty-nv/jetson-inference/issues/993,OpenBLAS warning,"hi, during trainopenmp=1 option. it looks like it shows at each step i dont have information about steps but only at epoch done thanks for help",question,Error
1273,https://github.com/dusty-nv/jetson-inference/issues/1273,Detectnet - How remove the '%' ,"hi, i would like to know how could i remove the '**? thank you.",question,question
1808,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1808,Failed to lad Windows library. Trying to load Linux library,"my environment is win10, python3.6,vs2017, when i build openpose,c++ api can be used, but python have the error ,i change the code: sys.path.append('d:/project/mythesis/openpose/build/python/openpose/release'); os.environ['path'] = os.environ['path'] + ';' + 'd:/project/mythesis/openpose/build/x64/release;' + 'd:/project/mythesis/openpose/build/bin;' import pyopenpose as op there is the error : [winerror 126] 找不到指定的模块。 failed to lad windows library. trying to load linux library... failed to load library [winerror 126] 找不到指定的模块。",question,question
1809,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1809,Time to say Goodbye to Travis?,is closing and has removed unlimited minutes for open source projects. so i recommend to switch to github actions. it should not be too difficult,other,other
707,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/707,specific protobuf version  needed?,"executed command (if any) make -j4 openpose output (if any) /home/jeremy/pycharmprojects/openpose/build/caffe/src/openposecaffe-build/include/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your protocol buffer headers. please update type of issue - compilation/installation error your system configuration 1. **: cmake-gui , no gpu , no cudnn ubuntu 18.04 release gcc (ubuntu 7.3.0-16ubuntu3) 7.3.0 product: intel(r) core(tm) i5-6200u cpu @ 2.30ghz memtotal: 16356504 kb",question,question
489,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/489,Cuda check failed (9 vs. 0): invalid configuration argument,"issue summary coming from: - src/openpose/core/netcaffe.cpp:forwardpass():86 - src/openpose/core/netcaffe.cpp:forwardpass():90 - src/openpose/pose/poseextractorcaffe.cpp:forwardpass():123 - ./include/openpose/pose/wposeextractor.hpp:work():80 - ./include/openpose/thread/subthread.hpp:worktworkers():138 - ./include/openpose/thread/subthreadqueueinout.hpp:work():87 - ./include/openpose/thread/thread.hpp:threadfunction():206 executed command (if any) ./build/examples/openpose/openpose.bin --imageimages output/ --no_display type of issue - execution error your system configuration 14.04.1-ubuntu manual makefile installation, ... (ubuntu); cuda-8.0 cudnn 5.1.10 gpu: geforce gtx tit... 12g caffe： the same as your opencv: 3.3.1 gcc: g++ (ubuntu 5.4.1-2ubuntu1~14.04) 5.4.1",question,question
276,https://github.com/dusty-nv/jetson-inference/issues/276,libnvinfer.so linking to cuda9.0,"hi, i am trying to build the jetson-inference , i build cuda8.0 providing by jetpack3.1, i successfully built caffe,it is ok when testing vgg net ,but performance is far from benchmark ,then i found out i need to use to tensorrt to test the performance , i am trying to build the jetson-inference , so i downloaded the jetpack3.3 tensortrt4.0, building the cuda 9.0 without flashing the os, i succesfully build the jet-infererence bin and libs ,but when testing it tell me the cuda cuda initialization failure with error 35. i rebuild the caffe ,the same errors, i guess the driver cuda9 needed is higher, so i build back cuda8.0 providing by jetpack3.1, i successfully built caffe,it is works ,but when compling your jetson-inference ,some errors happendedthe errors are the following `/usr/bin/ld: warning: libcublas.so.9.0, needed by /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libcudart.so.9.0, needed by /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so, not found (try using -rpath or -rpath-link) /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublasgemmex@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudadevicereset@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublashgemm@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudalaunchkernel@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudafree@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudacreatechanneldesc@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `_v2@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudasetupargument@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublassgemmex@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublassetstreamv2@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublasgetversionv2@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudastreamdestroy@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudaeventelapsedtime@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudaruntimegetversion@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudamemgetinfo@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudastreamaddcallback@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudadestroytextureobject@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublassetmathmode@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublashgemmstridedbatched@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cublasgetmathmode@libcublas.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudaeventdestroy@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudamemcpy@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudagetdeviceproperties@libcudart.so.9.0' /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libnvinfer.so: undefined reference to `cudaconfigurecall@libcudart.so.9.0' collect2: error: ld returned 1 exit status imagenet-console/cmakefiles/imagenet-console.dir/build.make:101: recipe for target 'aarch64/bin/imagenet-console' failed make[2]: ***** [all] error 2 ` i dont know why , i try to remove cuda9 ,it doesnt work ,so how to solve this , thanks in advanced!",question,question
4,https://github.com/dusty-nv/jetson-inference/issues/4,tensorNet::LoadNetwork Seg Faults,"when running the imagenet-camera example, the program seg faults. it appears to be in the call: imagenet.cpp imagenet::init - tensornet::loadnetwork where the loadnetwork function is called with null. replacing the null with an empty string ("""") allows the program to run. configuration: l4t 24.2 cuda 8.0 gcc version 5.4.0 20160609 (ubuntu/linaro 5.4.0-6ubuntu1~16.04.2)",Error,other
172,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/172,Inquiring about researching,"hello there, my name is vrushabh, i know this not a place to ask this question and i apologize for that, but i couldn't reach u guys on the department website. i m currently a final year student studying b. tech in nit calicut, india. i m planning to go for higher studies for specializing in computer vision. i have read a lot of articles about the famous cmu robotic institute for computer perception. i really admire the work of this institute. so coming to my main question, i wanted to know what is the pre requisite for the course and also what are the things that will help me in getting admitted in this course. i m planning to write gre and toefl. also, i would like to work on porting this openpose repo for android. again sry for asking this question here.",other,other
1432,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1432,Add the option for pre-processing of poor-qality video.,"please add the option for pre-processing the poor quality videos. for neural networks, it is not a problem to convert low-quality video into video made on professional equipment. i can get involved with this option, but i'm new to openpose, so i'll need your help in quickly learning the source code and working with your library.",other,other
32,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/32,"Slow startup, get body key-points more quickly? + CUDNN_STATUS_BAD_PARAM","issue summary every time i start, gui has to keep the screen black for some time. i just want 18 body key-points on the picture. how can i get the body key-points more quickly? thanks. executed command (if any) ./build/examples/openpose/rtpose.bin --imageposegpu 2 openpose output (if any) sometimes there is only one person in the picture, but it outout 2 body data?? type of issue - help wanted - question your system configuration ** (`nvidia-smi`): wed may 17 10:43:18 2017 +-----------------------------------------------------------------------------+ nvidia-smi 367.48 driver version: 367.48 -------------------------------+----------------------+----------------------+ gpu name persistence-m bus-id disp.a volatile uncorr. ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m. ===============================+======================+====================== 0 geforce gtx 1080 off 0000:03:00.0 off n/a 33% 43c p0 39w / 180w 0mib / 8113mib 0% default +-------------------------------+----------------------+----------------------+ 1 geforce gtx 750 off 0000:84:00.0 off n/a 25% 38c p0 1w / 38w 0mib / 979mib 0% default +-------------------------------+----------------------+----------------------+ 2 geforce gtx 750 ti off 0000:85:00.0 off n/a 25% 37c p0 2w / 38w 0mib / 2000mib 0% default +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ processes: gpu memory gpu pid type process name usage ============================================================================= no running processes found +-----------------------------------------------------------------------------+ compiler (`gcc --version` on ubuntu): gcc (ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4",question,question
1350,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1350,How to get the poseIds?,"hello, i need to get all persons' poseids, but the return value is -1, how can i get it, thank you very much. datumsptr = opwrapper.emplaceandpop(img); if (datumsptr != nullptr && !datumsptr->empty()) { auto& poseids = datumsptr->at(0)->poseids; op::log(""body id: "" + poseids.tostring(), op::priority::high); }",question,question
164,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/164,File not found: models/pose/coco/pose_deploy_linevec.prototxt - Ubuntu,i'm using a local caffe installation and i'm getting this error when i'm running the openpose command from outside of the openpose directory. this works: `./build/examples/openpose/openpose.bin --video video.mp4 --writejson output/` this doesn't work (outside of the openpose directory): `/home/ubuntu/projects/openpose/build/examples/openpose/openpose.bin --video /home/ubuntu/projects/openpose/video.mp4 --writejson /home/ubuntu/projects/openpose/output/` any idea why?,question,question
564,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/564,fatal error C1083: Cannot open include file: 'opencv2/cudaimgproc.hpp',"i don't have this error anymore, now have some new errors",Error,question
1335,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1335,How to access keypoint name mapping via Python API?,"i'm using openpose via its python interface and i'm in the process of extracting poses (body+hand) from videos, which is working great so far. however, i'm wondering how i can make sure i label my csv-columns correctly, especially if i try out different models (coco, mpi). i understand that there is , but i'm looking for a programmatic way to ensure correct feature associations. i found containing the mapping i'm looking for - are these somehow accessible via the python api? type of issue - help wanted your system configuration 2. ** api:",other,question
1472,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1472,openpose_lib error 1,issue summary -- generating done cmake generate step failed. build files cannot be regenerated correctly. cmakefiles/openposelib' failed make[2]: ***** api:,question,question
838,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/838,"Empty frame detected, frame number 751 of 752","in the official example i was able to run the program properly. eg: ./build/examples/openpose/openpose.bin --video examples/media/gangqing.avi however, this error occurred while running with video, which i downloaded myself.the error is :empty frame detected, frame number 751 of 752. and the output video is the same as the original video,which confuse me a lot. i sincerely hope to get your guidance, thank you.",question,question
1909,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1909,CMake errors on new code under Windows VS2019,"issue summary on my first attempt to open the code from the latest zip file, i get two cmake errors in win10.: severity code description project file line suppression state error cmake error: running error cmake error: running note: add `--loggingmulti_thread` to get higher debug information. openpose output (if any) errors (if any) severity code description project file line suppression state error cmake error: running error cmake error: running select the topic(s) on your post, delete the rest: - compilation/installation error your system configuration 1. ** issue:",Error,question
141,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/141,Zero Persons Found  HDF5: infinite loop closing library ,"hi, i am not getting any error while compilation. when i run the video provided with the code, i see the video display but with a message 0 people found. no pose is displayed. the same happen in case i run webcam. display is there but no person detected. executed command ./build/examples/openpose/openpose.bin --video examples/media/video.avi ####openpose response: hdf5: infinite loop closing library d,t,f,fd,p,fd,p,fd,p,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e ubuntu 14.04 lts cuda 8.0 cudnn 5.1 please suggest some solution",question,question
1326,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1326,Compilation fails on CentOS 7 with undefined reference to symbol '_ZN2cv6String10deallocateEv',** `cmake -dopencvdirs=/usr/local/opencv-3.4.7/include -dopencvdir=/usr/local/opencv-3.4.7/lib64 -dcaffedirs=/usr/local/caffe/include -dcaffecaffe=off ..` and then `make` produces no errors until:,deployment,deployment
231,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/231,Hand/face joint heatmaps ,issue summary are there plans to expose the hand/face heatmaps? handextractor does not have a getheatmaps() as the poseextractor. there is a private member spheatmapsblob. are all the heatmaps stored in that member? type of issue - enhancement / offering possible extensions / pull request / etc,other,other
1856,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1856,Removing background from *video/webcam*,is there any way to remove the background and only have the skeleton showing up when detected? i am currently using openpose to implement a privacy preserving human detection. thanks < - i am looking to output something like this,question,question
820,https://github.com/dusty-nv/jetson-inference/issues/820,Unable to retrain an ssd-mobilenet model using voc format input,"i have successfully managed to re-train the fruit example in the tutorial but when i try to retrain the picture example with voc files the training runs ok but when i then try to convert the model using onnxdict for ssd: size mismatch for classificationheaders.0.bias: copying a param with shape torch.size([30]) from checkpoint, the shape in current model is torch.size([24]). this seems to indicate that the mobilenet model i have trained is not the same layer format as the mobilnet model the utility is trying to generate. i have run this example in a docker container and outside a docker container and get the same results. i am not sure what the problem is due to. when i do the training i am using the correct mobilenet base model so i would expect it to generate the new model in the same format, which i would then expect to be in the required format for the onnx utility. any help would be appreciated. - charles p.s i have asked this question in the nvidia developers forum as well but i thought this might be a better place to ask it.",question,question
2012,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/2012,DLL Not Found Error. openpose.dll is missing,"issue summary i've followed your installation instruction, but faced some issues here.. at first, this error occured when i ran a demo inside `openpose\examples\tutorialpython`: i've solved this by modifying path inside the code. after that this came up: so i've looked up the installation again whether there are things that i've missed, and found that i had neither openpose.dll nor exe file inside my `openpose\build\bin` folder - very important note: in order to use openpose outside visual studio, and assuming you have not unchecked the buildfolder flag in cmake, copy all dlls from `{builddirectory}x64/release for the 64-bit release version. somehow openpose.dll and demo.exe haven't showed up after pressing generating button from cmake. here is a list of dlls that were in `openpose/build/bin folder`: - boosttime-vc142-mt-gd-x64-1date74.dll - boost74.dll - boost74.dll - boost74.dll - boost74.dll - boost74.dll - boost74.dll - caffe-d.dll - caffe.dll - caffehdf5.dll - caffehdf5hl.dll - caffehdf5d.dll - caffezlib1.dll - caffezlibd1.dll - cublas6411.dll - cudart648.dll - curand64svideoio64.dll - opencvmsmf450videoio64d.dll - opencvworld450d.dll - vcruntime140.dll - vcruntime140_1.dll thanks in advance. errors (if any) type of issue select the topic(s) on your post, delete the rest: - compilation/installation error - help wanted your system configuration 1. ** system:",question,Error
1386,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1386,where the training code for foot keypoints mentioned in the link,where is the code mentioned in the link ?,other,question
1316,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1316,make[1]: *** [src/openpose/CMakeFiles/openpose.dir/all] Error 2 mac os,"zsh: command not found: nproc -- gcc detected, adding compile flags -- building cpu only. -- found gflags (include: /usr/local/include, library: /usr/local/lib/libgflags.dylib) -- found glog (include: /usr/local/include, library: /usr/local/lib/libglog.dylib) -- could not find openmpccnames) -- could not find openmpcxxcxxnames) -- could not find openmp (missing: openmpfound openmpfound) -- ${caffedirs} set by the user to /usr/local/include/caffe -- ${caffecustomprocessing.bin -- adding example 01fromdefault.bin -- adding example 02bodyimagekeypointsimage.bin -- adding example 04fromkeypointsimagesgpu.bin -- adding example 06fromhandimage.bin -- adding example 08fromkeypointsheatmaps.bin -- adding example 10customasynchronousoutput.bin -- adding example 12customoutputdatum.bin -- adding example 13customsynchronouspreprocessing.bin -- adding example 15customsynchronousoutput.bin -- adding example 17customandthreadprocessingthreadinputoutputdatum.bin -- adding example handfromjsontest.bin -- adding example resizetest.bin -- download the models. -- downloading bodydnn.4.1.1.dylib', needed by `src/openpose/libopenpose.1.5.0.dylib'. stop. make[2]: ***** [all] error 2 i can't solve it..... plz help me ... this is my cmake",deployment,question
304,https://github.com/dusty-nv/jetson-inference/issues/304,why do you loop 10 times in superres-console.cpp?,there is a loop in here. i removed it and cannot see any difference in the up-scaled output image. / upscale image with network */ for( int i=0; i < 10; i++ ),question,question
521,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/521,how about the recognition ?,"understand it can do the facial and body detection , how about facial recognition and body language recognition ?",other,question
739,https://github.com/dusty-nv/jetson-inference/issues/739,Failed to run imagenet example,"hi, i have followed a step by step guide to work on jetson-inference examples and when i am trying to execute any imagenet example i get error like below: ` jasvinder@jasvinder-desktop:~/jassi/jetson-inference/build/aarch64/bin$ ./imagenet-console --headless images/orange0.jpg ------------------------------------------------ imageloader video options: ------------------------------------------------ -- uri: file:///home/jasvinder/jassi/jetson-inference/build/aarch64/bin/images/orangetrt version 1 [trt] registered plugin creator - ::nmstrt version 1 [trt] registered plugin creator - ::regiontrt version 1 [trt] registered plugin creator - ::lrelutrt version 1 [trt] registered plugin creator - ::normalizetrt version 1 [trt] registered plugin creator - ::batchednmstrt version 1 [trt] registered plugin creator - ::cropandresize version 1 [trt] registered plugin creator - ::detectionlayertrt version 1 [trt] registered plugin creator - ::pyramidroialigntrt version 1 [trt] registered plugin creator - ::split version 1 [trt] registered plugin creator - ::specialslicetrt version 1 [trt] detected model format - caffe (extension '.caffemodel') [trt] desired precision specified for gpu: fastest [trt] requested fasted precision for device gpu without providing valid calibrator, disabling int8 [trt] native precisions detected for gpu: fp32, fp16 [trt] selecting fastest native precision for gpu: fp16 [trt] attempting to open engine cache file networks/bvlcgooglenet.caffemodel.1.1.7103.gpu.fp16.engine [trt] invalid engine cache file size (0 bytes) networks/bvlcgooglenet.caffemodel [trt] imagenet -- failed to initialize. imagenet: failed to initialize imagenet ` any idea what is wrong here?",question,question
907,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/907,Mac Installation Error: No makefile generated,posting rules 1. ** issue:,question,other
1665,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1665,Low FPS on 2x RTX 2080 Ti,"hello, i have 2x rtx 2080 ti, cudnn-7.5, cuda - 10, vs 2017 in my workstation. when i run the openpose i get ~13 fps. i need ~25 fps. would you please suggest me what should i do to get ~25 fps.?",question,question
542,https://github.com/dusty-nv/jetson-inference/issues/542,"File ""./detectnet-console.py"", line 67, <module>","file ""./detectnet-console.py"", line 67, jetson.utils.saveimagergba(opt.file_out, img, width, height) exception: jetson.utils --saveimagergba() failed to save the image getting the error above after running the python demo under ""detecting objects from the command line"" $./detectnet-console.py images/ped0.jpg out.jpg help please",question,question
1198,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1198,Build information update request for windows.,in case of **,other,other
50,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/50, fatal error C1083: Cannot open file included: “cudnn.h”: No such file or directory,issue summary: when i compiling the openposedemo，there is an error occurred:cannot open file included: “cudnn.h”: no such file or directory. your system configuration **: default compiler (`gcc --version` on ubuntu):,question,question
649,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/649,[Question] Real-time 3-D Performance/Latency for Experimental Usage **Windows**,"i am curious about the performance for real-time tracking in 3d. the hope is to use this software to track body position in real-time for an experimental setup, where body position influences the output from a projector, which then influences the body position (etc etc etc). is latency needed for this kind of setup achievable with the current release? if so, what kind of hardware would be necessary? i can answer further questions in case this is unclear.",question,question
72,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/72,how to use openpose in other dir,"issue summary i want to call openpose as one part in my project ,so i try in other dir(not in openpose dir ),it would not work. i know this is caused by the path problem，but how to change ""models"" dir to a absolute path or link it with ""/home/ubuntu/openpose/build/examples/openpose/openpose.bin"" executed command (if any) ""./build/examples/openpose/openpose.bin --video examples/media/video.avi --numdisplay --writejson output/vedio"" this command works in openpose dir but failed in other dir openpose output (if any) starting pose estimation demo. starting thread(s) f0612 11:48:41.891775 3775 io.cpp:36] check failed: fd != -1 (-1 vs. -1) file not found: models/pose/coco/poselinevec.prototxt ****: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. compiler (`gcc --version` on ubuntu):opencv 2.x",question,question
108,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/108,can openpose implement tracking? I want to get someone's poseKeyPoints from each frames in videos ,posting rules 1. **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. compiler (`gcc --version` on ubuntu):,question,other
660,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/660,python Error,posting rules 1. run sudo python ./build/examples/tutorialextractdeployerror' what(): error: prototxt file not found: /usr/local/python/openpose/../../../models/pose/coco/poselinevec.prototxt. possible causes: 1. not downloading the openpose trained models. 2. not running openpose from the same directory where the `model` folder is located. 3. using paths with spaces. coming from: - /usr/rep/openpose/src/openpose/net/netcaffe.cpp:implnetcaffe():51 - /usr/rep/openpose/src/openpose/pose/poseextractorcaffe.cpp:addcaffenetonthread():131 - /usr/rep/openpose/src/openpose/pose/poseextractorcaffe.cpp:netinitializationonthread():196 - /usr/rep/openpose/src/openpose/pose/poseextractornet.cpp:initializationonthread():93,Error,question
716,https://github.com/dusty-nv/jetson-inference/issues/716,How to get the frame from cudaImage,"i need to use the frame to get some variables, i discover the cudaimage and videooutput could not return the frame. so, i use another way to get the frame, using cv2 to get the rtsp frame.however, in the end, i could not use the cv2.imshow('frame',frame) to show the video even there is no any error showed in the terminal",question,question
995,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/995,about tracking,"hello ,first of all, thanks for release the code, and it is really a good job, i want to ask a question about tracking, why it is only single person tracking ? or multi person tracking will be released later? i am looking for you reply ,thanks again :)",question,question
887,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/887,openpose.dll is infected with Gen:Variant.Razy,"issue summary my virus software scanned and restricted openpose.dll. after the scan it moved the file to the virus chest and reported it was infected with gen:variant.razy. i can confirm that does not trigger the same warning, so i presume the infection occurred between january and march. executed command (if any) none openpose output (if any) none errors (if any) none type of issue - execution error your system configuration 1. windows 10 professional, up-to-date on 20/10/2018 bitdefender antivirus plus 2019 build 23.0.11.48 (last update 16/10/2018 11:36) threat database updates: 11700521, engine 7.77893 2. ** system: - portable demo, although the issue seems to be with openpose.dll which is a compiled library.",question,question
1208,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1208,"compiling caffe error,ubuntu18.04","when i make all -j8 for caffe in ubuntu18.4, i meet this problem, as is follows: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",other,deployment
210,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/210,High level API & Python Bindings?,"hi everyone, i'm working on a project of which skeleton and people tracking, as well as gesture and intention recognition are essential parts. i would really like to use openpose for some experiments and maybe further down the road even in a kind of productive environment. however, since it's quite cumbersome and low level to use the caffe net directly (which pretty much works) and implement everything based on the heatmaps sort of from scratch i'm wondering if there's already something like a high(er) level api for openpose and if it can also be easily used in python. thanks",question,other
1561,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1561,"[Ubuntu] ""tutorial_api_thread"" example prints unexpectedly empty pose keypoints","issue summary hello, team! i'm working on . i added exactly two lines, each logging filename and keypoint, similar to the following code. (you can browse my complete code, .) (added: confirmed this happens also for the .) please help me to figure out what went wrong. thanks! executed command openpose output errors none type of issue - help wanted - question your system configuration 2. ** issue:",question,Error
551,https://github.com/dusty-nv/jetson-inference/issues/551,How to load custom trained model on my-detection.py,"hi, i have tried retraining a detectnet on digits and it was success. i got the snapshotxxx.caffemodel, deploy.prototxt and other stuff. it works well when i tried ` ./detectnet-camera --model=/home/nvidia/testscript-jetson-inference/coco-person/snapshot903600.caffemodel --prototxt=/home/nvidia/testscript-jetson-inference/coco-person/deploy.prototxt --camera=/dev/video0 --outputlabels=/home/nvidia/testscript-jetson-inference/coco-person/classes.txt ` it can detect as i expected. but then i want to use this on a python script according to your guide on i tried to point detectnet to my custom model path and prototxt file like this but i got this error i am not sure how to properly pass the custom model on detectnet in the code. i checked on this , seems like i can pass model and prototxt to detectnet class, isn't it? please guide me a bit, thank you.",question,question
232,https://github.com/dusty-nv/jetson-inference/issues/232,"Nvidia DPX2: run imagenet-console successfully ,but imagenet-camera gets errors","i can run imagenet-console successfully on dpx2. when i use c920 webcam and run imagenet-camera, it comes up with the errors below: i guess i've already loaded the model and get the class result(in the terminal log). however, i cannot open the display window and see the result. imagenet-camera args (1): 0 [./imagenet-camera] [gstreamer] initialized gstreamer, version 1.8.2.0 [gstreamer] gstreamer decoder pipeline string: v4l2src device=/dev/video0 ! video/x-raw, width=(int)1920, height=(int)1080, format=rgb ! videoconvert ! video/x-raw, format=rgb ! videoconvert !appsink name=mysink imagenet-camera: successfully initialized video device height: 1080 [gie] tensorrt version 3.0.2, build 3002 [gie] attempting to open cache file networks/bvlcgooglenet.caffemodel.2.tensorcache nvrmgooglenet.caffemodel loaded [gie] cuda engine context initialized with 2 bindings [gie] networks/bvlcgooglenet.caffemodel input dims (b=2 c=3 h=224 w=224) size=1204224 [cuda] cudaallocmapped 1204224 bytes, cpu 0x1053e0000 gpu 0x1053e0000 [gie] networks/bvlcgooglenet.caffemodel output 0 prob dims (b=2 c=1000 h=1 w=1) size=8000 [cuda] cudaallocmapped 8000 bytes, cpu 0x1055e0000 gpu 0x1055e0000 networks/bvlcgooglenet.caffemodel loaded imagenet -- loaded 1000 class info entries networks/bvlcstate_playing [gstreamer] gstreamer changed state from null to ready ==> mysink [gstreamer] gstreamer changed state from null to ready ==> videoconvert1 [gstreamer] gstreamer changed state from null to ready ==> capsfilter1 [gstreamer] gstreamer changed state from null to ready ==> videoconvert0 [gstreamer] gstreamer changed state from null to ready ==> capsfilter0 [gstreamer] gstreamer changed state from null to ready ==> v4l2src0 [gstreamer] gstreamer changed state from null to ready ==> pipeline0 [gstreamer] gstreamer changed state from ready to paused ==> videoconvert1 [gstreamer] gstreamer changed state from ready to paused ==> capsfilter1 [gstreamer] gstreamer changed state from ready to paused ==> videoconvert0 [gstreamer] gstreamer changed state from ready to paused ==> capsfilter0 [gstreamer] gstreamer stream status create ==> src [gstreamer] gstreamer changed state from ready to paused ==> v4l2src0 [gstreamer] gstreamer changed state from ready to paused ==> pipeline0 [gstreamer] gstreamer stream status enter ==> src [gstreamer] gstreamer msg new-clock ==> pipeline0 [gstreamer] gstreamer msg stream-start ==> pipeline0 [gstreamer] gstreamer changed state from paused to playing ==> videoconvert1 [gstreamer] gstreamer changed state from paused to playing ==> capsfilter1 [gstreamer] gstreamer changed state from paused to playing ==> videoconvert0 [gstreamer] gstreamer changed state from paused to playing ==> capsfilter0 [gstreamer] gstreamer changed state from paused to playing ==> v4l2src0 imagenet-camera: camera open for streaming [gstreamer] gstreamer decoder onpreroll [cuda] cudaallocmapped 6220800 bytes, cpu 0x1059e0000 gpu 0x1059e0000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x105fd0000 gpu 0x105fd0000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x1065c0000 gpu 0x1065c0000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x106bb0000 gpu 0x106bb0000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x1071a0000 gpu 0x1071a0000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x107790000 gpu 0x107790000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x107d80000 gpu 0x107d80000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x108370000 gpu 0x108370000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x108960000 gpu 0x108960000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x108f50000 gpu 0x108f50000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x109540000 gpu 0x109540000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x109b30000 gpu 0x109b30000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x10a120000 gpu 0x10a120000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x10a710000 gpu 0x10a710000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x10ad00000 gpu 0x10ad00000 [cuda] cudaallocmapped 6220800 bytes, cpu 0x10b2f0000 gpu 0x10b2f0000 [cuda] gstreamer camera -- allocated 16 ringbuffers, 6220800 bytes each [gstreamer] gstreamer changed state from ready to paused ==> mysink [gstreamer] gstreamer msg async-done ==> pipeline0 [gstreamer] gstreamer changed state from paused to playing ==> mysink [gstreamer] gstreamer changed state from paused to playing ==> pipeline0 [cuda] gstreamer camera -- allocated 16 rgba ringbuffers [gstreamer] gstreamer msg qos ==> v4l2src0 class 0526 - 0.107836 (desk) class 0527 - 0.125700 (desktop computer) class 0553 - 0.030763 (file, file cabinet, filing cabinet) class 0620 - 0.016277 (laptop, laptop computer) class 0632 - 0.011526 (loudspeaker, speaker, speaker unit, loudspeaker system, speaker system) class 0651 - 0.056109 (microwave, microwave oven) class 0664 - 0.244295 (monitor) class 0673 - 0.048897 (mouse, computer mouse) class 0713 - 0.010147 (photocopier) class 0745 - 0.014054 (projector) class 0760 - 0.019738 (refrigerator, icebox) class 0782 - 0.066791 (screen, crt screen) class 0811 - 0.015403 (space heater) class 0851 - 0.012936 (television, television system) imagenet-camera: 24.42952% class #664 (monitor) [cuda] cudagraphicsglregisterbuffer(&minteropcuda, mdma, cudagraphicsregisterflagswritediscard) [cuda] operation not supported (error 71) (hex 0x47) [cuda] /home/nvidia/data/src/jetson-inference/util/display/gltexture.cpp:253 [cuda] cudagetlasterror() [cuda] operation not supported (error 71) (hex 0x47) [cuda] /home/nvidia/data/src/jetson-inference/util/cuda/cudargb.cu:62 [cuda] cudargbtorgbaf((uchar3)mrgba[mlatestrgba], mwidth, mheight) [cuda] operation not supported (error 71) (hex 0x47)",Error,question
1158,https://github.com/dusty-nv/jetson-inference/issues/1158,Batch inferencing in detectnet.py,"i see in detectnet.h line 64, that there is an option for --batch-size. after i set this flag, how would i pass in a batch of images to net.detect() for batch inferencing?",question,question
1795,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1795,"Error in Version of CUDA, CUDANN","while running this script in colab, i got error nvcc fatal : unsupported gpu architecture 'computecompilegeneratedlayer.cu.o.release.cmake:219 (message) any suggestions would be appreciated.",deployment,question
1384,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1384,Support to include OpenPose as sub project,"hi, i want to add this project with `addsourcecmake/utils.cmakecurrentdir cmake version: 3.15.3 openpose & caffe version: master",other,other
886,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/886,"_openpose.dll exists but openpose.py throws ""not a proper win32 file""",posting rules 1. ** issue:,deployment,Error
1105,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1105,Steps to install openpose python api on jetson Tx2,"can someone list proper steps for installing python api on jetson tx2. while trying to import openpose im facing following ewrror, file """", line 1, in file ""/home/nvidia/openpose/python/openpose/__.py"", line 1, in from . import pyopenpose as pyopenpose importerror: cannot import name 'pyopenpose'",question,question
600,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/600,"ERROR: Empty frame detected, frame number 0 of 0.",posting rules 1. ** issue:,question,other
1241,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1241,About the standalone hand detector,"issue summary why do i use a standalone hand detector, but can not detect only hand picture? executed command (if any) examples/tutorialcpp/07from_image.bin type of issue you might select multiple topics, delete the rest: - help wanted - question",question,question
236,https://github.com/dusty-nv/jetson-inference/issues/236,Models for speech recognition inferencing,"hello, is there any guide about how to work with speech recognition models on jetson tx2. the only thing i could find is which is not clear in many aspects. thanks.",other,question
1085,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1085,Body 25 in Ubuntu,"hi, i cannot seem to find body 25 model in ubuntu installation (in windows it works) is it available",question,question
1122,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1122,the python API   7_hand_from_image.py  not work well sometimes. Give me some suggestions.Thx,"when i try the python api the python api 7from_image.py .i find that sometimes, it works well like follows. however ,sometimes,it cannot detect the joints of hand.(i have gave an appropriate rectangle of hand in a image ) could you give me some suggestions about this, thanks a lot.(how can i get a good result by using this api ) some image works well:",Performance,question
454,https://github.com/dusty-nv/jetson-inference/issues/454, Semantic Segmentation models inference performance compare between pytorch & tensorrt,"i'm trying to test my semantic segmentation custom model's performance on nano, such as fps, accuracy, iou, etc. then i saw there are some pre-trained models' performance test results on readme.md. it looks like what i'm searching for! could you provide the test code?? thanks~",other,question
1161,https://github.com/dusty-nv/jetson-inference/issues/1161,How to check gstreamer changed state from READY to PAUSED i.e. [gstreamer] gstCamera end of pipeline using python api?,"hello all, for reading live input stream we are using ** python api. but gstreamer pipeline get paused and the program execution is getting stuck at img=input.capture() line in code. we also applied this logic to check whether streaming is working or not using the below code snippet: _if input.isstreaming(): but input.isstreaming() returns false when the streaming is open. is this isstreaming() function working properly? so, how to check what is the state of the gstreamer pipeline and start the gstreamer pipeline again using python api?",question,question
326,https://github.com/dusty-nv/jetson-inference/issues/326,imagenet-console shuts down on jetson nano when building CUDA engine,"i am following the and running imagenet-console on jetson nano. the machine ** when it is building cuda engine: > zhiang@dreams-nano1:~/jetson-inference/build/aarch64/bin$ ./imagenet-console orange0.jpg imagenet-console args (3): 0 [./imagenet-console] 1 [orange0.jpg] >imagenet -- loading classification network model from: >[trt] tensorrt version 5.0.6 [trt] detected model format - caffe (extension '.caffemodel') [trt] desired precision specified for gpu: fastest [trt] requested fasted precision for device gpu without providing valid calibrator, disabling int8 [trt] native precisions detected for gpu: fp32, fp16 [trt] selecting fastest native precision for gpu: fp16 [trt] attempting to open engine cache file networks/bvlcgooglenet.caffemodel [trt] retrieved output tensor ""prob"": 1000x1x1 [trt] retrieved input tensor ""data"": 3x224x224 [trt] device gpu, configuring cuda engine [trt] device gpu, building fp16: on [trt] device gpu, building int8: off [trt] device gpu, building cuda engine (this may take a few minutes the first time a network is loaded) i followed the instructions on this web to install jetpack: after it shutting down, i have to replug the power cable to boot it.",question,question
1411,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1411,Convert to pytorch,"now that caffe is dead and caffe2 is being merged with pytorch, should things be updated to use pytorch 1.x? is anyone planning to work on this already? context i'm trying to get the cmu ""monocular total capture"" project running, and it uses openpose as a dependency. i'm dockerizing things, but the dependencies are still a pain. i noticed that one of the maintainers has been . i could be interested in helping with that, as i'm doing a lot of pose research, but it would be helpful to know what exactly needs to be changed. i assume it's mainly reading the caffe protobuf and converting that instead to pytorch?",other,question
1025,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1025,"No ""Makefile.example_openpose""  in openpose/scripts/ubuntu/","jetson tx2 installation failed because cannot find ""makefile.example_openpose"" in openpose/scripts/ubuntu type of issue - compilation/installation error",deployment,question
1831,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1831,Issue building OpenPose,"type of issue select the topic(s) on your post, delete the rest: - compilation/installation error i have been using openpose for a while now, previously, i had used colab to build compile and install openpose. i had just done it yesterday in fact. however, today i am met with that error as i try to compile and install openpose to the colab session. the error reads: thank you!",deployment,other
981,https://github.com/dusty-nv/jetson-inference/issues/981,Can i ask and How to ?,i'm follow jetson ai fundamentals - s3e3 - training image classification models. everything went well. until 29:24 min in youtube last process i got this can i ask how to fixed,question,question
232,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/232,getCaffe3rdparty.bat & getOpenCV.bat’s problem,"1.run file getcaffe3rdparty.bat ----- downloading caffe ----- --2017-09-02 14:42:40-- windows/caffe3rdparty07201711.zip [following] --2017-09-02 14:42:40-- ose/3rdparty/windows/caffe3rdparty07201711.zip or caffe3r dparty/caffe3rdparty07310.zip resolving posefs1.perception.cs.cmu.edu (posefs1.perception.cs.cmu.edu)... 128.2 .176.37 connecting to posefs1.perception.cs.cmu.edu (posefs1.perception.cs.cmu.edu)128. 2.176.37:80... connected. http request sent, awaiting response... 302 found location: ndows/opencv310.zip connecting to 101.96.10.60:80... connected. http request sent, awaiting response... 403 forbidden 2017-09-02 14:44:34 error 403: forbidden. ----- unzipping opencv ----- unzip: cannot find either opencv/opencv310.zip.zip. the file server does not exist. can you provide a url that can download? thank you very much",question,question
1613,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1613,The best model,"hi i'm looking for the best model to reach the results of the paper. is it available? if not would you please share it with me? there are some models in caffe and keras but apparently they aren't the last one, are they?",other,other
835,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/835,Confidence for each joint,"hi the output of open pose is the joint location and the confidence of that estimates, i could not figure out based on what analysis you give the confidence for each joint is it extracted just from heat map or ... as it is , how you knows that estimated heatmap itself is true and does not have uncertainty?",other,question
830,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/830,How can I detect hand keypoints like 1_extract_pose.py file?,i want to detect hand keypoints using py file like 1pose.py. how can i detect? and i cannot change default model folder /.../.../models/pose/body25/.. into models/hand/....!! please help me.,question,question
1299,https://github.com/dusty-nv/jetson-inference/issues/1299,built-in method Process of jetson.inference.poseNet object at 0x7fae89f510> returned NULL without setting an error,"hi @dusty-nv when i am running the docker container and using `posenet` as below from the `/jetson-inference/data` folder i get the following error on `poses = net.process(img, none, overlay='links,keypoints')` line please note that the `img` object is not none and this code works fine outside the container. here's the complete code",question,Error
680,https://github.com/dusty-nv/jetson-inference/issues/680,imagnet fails to load,"[trt] corereadarchive.cpp (38) - serialization error in verifyheader: 0 (version tag does not match) [trt] invalidconfig: deserialize the cuda engine failed. [trt] device gpu, failed to create cuda engine [trt] failed to load networks/bvlc_googlenet.caffemodel [trt] imagenet -- failed to initialize. imagenet-console: failed to initialize imagenet",question,deployment
120,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/120,Connected with Microsoft Kinect 1.0,"hi! can you tell me how could i adjust the value of so i can obtain the real-time video from kinect? also, if i want to run the c++ source file in the examples/tutorial_wrapper file, what kind of project of visual studio should i plug it into? should the project i created stay in the folder of windows-project?",question,question
727,https://github.com/dusty-nv/jetson-inference/issues/727,How to run custom Object Detection in jesonnano on host pc?,"hello! i’m studying machine learning in korea. your github and youtube videos helped me a lot, thank you! from your github, see the link below and create a custom object detection model with a custom dataset. this model worked nicely on the jetson nano. i tried to run this model on my pc with rtx2070 super graphics card for higher speed and higher fps. as much as possible, the pc environment was configured as similar to jetson nano. os : ubuntu 18.04 lts, nvidia driver : 450.66, cuda : 10.2, cudnn : 7.6.5, tensorrt : 7.1, python : 3.6.9, pytorch : 1.3.1, deepstream sdk : 5.0 and i installed jetson inference and build, make and configured the same as i did on jetson nano. finally when i ran the model, the following error occurred it seems to be a problem with nvjpegcodec. i also checked the link below to fix the error, but couldn't fix it. please tell us a specific way to solve it. finally, thank you for reading the question i wrote with poor english skills and look forward to your answer. thank you!",question,question
473,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/473,convert caffe model to caffe2 but can't parse success?,"posting rules 1. **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? compiler (`gcc --version` in ubuntu):",question,question
309,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/309,Cannot download the trained-models from the urls you provided!,rt.,other,other
1560,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1560,COCO Body Format Does Not Match the COCO Dataset,"issue summary the coco format layout that can be found in the output.md does not seems to match the coco skeleton layout seen in their website. yours has an inverted v shape body while theirs has a rectangular one. is this just a matter of drawing? i also count at most 17 keypoints in their images, while openposes shows (and outputs) 18. am i doing anything wrong? here are the reference images: ps: i am aware body25 skeletons are better. i am just doing a comparative study right now. type of issue - question your system configuration 2. **: latest github code. openposedemo.exe.",Performance,question
1053,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1053,OpenPose library could not be found: Symbol not found: ___addtf3 – osx,"issue summary there exists `pyopenpose.cpython-37m-darwin.so` when listing `ls ../../python/openpose`, but still getting ""openpose library could not be found. did you enable `build_keypointsimages.py` errors `error: openpose library could not be found. did you enable `buildkeypointsimages.py"", line 27, in file ""3from_s.1.dylib in /usr/local/opt/gcc/lib/gcc/8/libquadmath.0.dylib ` type of issue - execution error your system configuration 1. ** api:",question,question
1361,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1361,How can I find the training code for the boby model?,how can i find the training code for the boby model?,question,question
1656,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1656,change color of keypoint and set the background black,"thank you for your project! the question is how can i to change color of keypoint and set the background black,i had modified the function that in the keypoint.cpp which named renderkeypointscpu, but it doesn't work.i hope that you can help me !",question,question
1829,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1829,COCO module process killed,issue summary cmmd: ./build/examples/openpose/openpose.bin --imagejson outputdir /home/fyp/desktop/v/data/test/image --modeljson outputdir /home/fyp/desktop/v/data/test/image --modeljson outputbuild/opencv/modules/core/src/matrix64 (cpu_only) thankyou,question,question
1220,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1220,SIGSEGV in python api [input images of different sizes],"i was testing this simple code, but i'm getting sigsegv. crash is in 2nd `opwrapper.emplaceandpop([datum]) ` if any more info is needed, please let me know.",question,question
1019,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1019,Is it possible to obtain 3d keypoints in code.,is there any way to obtain a vector/array of the 3d keypoints used to render the 3d reconstruction in op::wrapperstructgui. preferably i would like to be able to the keypoints directly from code. i am using non-flir cameras so i need my own custom producer so i don't have anyway of accessing the datums as demonstrated in 3fromconfigurable. if there is no way using the wrapper interface is it possible to obtain a reference using one of the lower level interfaces?,question,question
1162,https://github.com/dusty-nv/jetson-inference/issues/1162,output binding stuck for docker ,"i am new to jetson device inference. i have created docker image which performs object detection using tensorrt on jetson nano device. i am running the docker image by passing docker container image to `docker/run.sh` script. it works as i expected. i converted my app to daemon (service) in jetson nano for the same process. in daemon service, it starts docker container as i expected. but stucks in output binding process. here is the complete stacktrace. please let me know, what can i do to resolve the issue.",question,question
832,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/832,Bounding box around each person to search,"hi, in paper `convolutional pose machine` , bounding box is taken from the ground truth for training , and during test it wants from the user to draw a bounding box around person, to have a multi-scaled region for finding joints, i was wondering if open pose use a people detector inside to have regions to search in? thanks,",question,other
974,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/974,About Skelteon data,"hello, i am doing a project in open pose and i need your help for this my idea is to get walking pattern of persons i have series of images with walking of person for some certain distance and now i have given those to open pose from open pose i will get skeleton data which has location of key joints in x and y my idea is i will consider the ankle point and plot now from the values i should be able to tell weather the leg is on ground or air but i am not getting values correctly so can you please help in this",other,question
220,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/220,Anaconda - VideoCapture (video) could not be opened for path: 'examples/media/video.avi'.,> starting pose estimation demo. > error: > videocapture (video) could not be opened for path: 'examples/media/video.avi'. > > coming from: > - src/openpose/producer/videocapturereader.cpp:videocapturereader():32 > - src/openpose/producer/videocapturereader.cpp:videocapturereader():36 > - src/openpose/utilities/flagstoopenpose.cpp:flagstoproducer():133 > terminate called after throwing an instance of 'std::runtime_error' > what(): > error: > videocapture (video) could not be opened for path: 'examples/media/video.avi'. > > coming from: > - src/openpose/producer/videocapturereader.cpp:videocapturereader():32 > - src/openpose/producer/videocapturereader.cpp:videocapturereader():36 > - src/openpose/utilities/flagstoopenpose.cpp:flagstoproducer():133 > > aborted (core dumped) i have tried #41 as well #17 but nothing is works.....i have install boost library freshly and also i rebuild my opencv 3.1.0. i can see in media folder all video file is available: ls examples/media/,question,question
1132,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1132,Origin point of pose_keypoints_2d in output json file ?,"in the output.json file, we can see the location of keypoints in an array in the format of x1,y1,c1,x2,y2,c2,.... i was wondering, where is the origin point of x and y-axis ? because i need to pad my images in accordance with the origin point not to lose the exact positions of keypoints.",question,question
793,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/793,windows 10 cmake complite error,"windows 10 2015 cmake 3.12.0 cuda 8.0 the c compiler identification is msvc 19.0.24210.0 the cxx compiler identification is msvc 19.0.24210.0 check for working c compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/x86amd64/cl.exe -- works detecting c compiler abi info detecting c compiler abi info - done detecting c compile features detecting c compile features - done check for working cxx compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/x86amd64/cl.exe -- works detecting cxx compiler abi info detecting cxx compiler abi info - done detecting cxx compile features detecting cxx compile features - done found cuda: c:/program files/nvidia gpu computing toolkit/cuda/v8.0 (found version ""8.0"") building with cuda. downloading windows dependencies... opencv201826.zip already exists. caffe01subdirectory): the source directory adding example openposedemo adding example 1postextractimage adding example 2poseheatmatimage adding example 1readdisplay adding example 2processinguserprocessingoutput adding example 4inputoutputdatum adding example 1synchronoususerinput adding example 3synchronoususerall adding example 5asynchronous adding example 6asynchronous25 model... model already exists. not downloading body (coco) model not downloading body (mpi) model downloading face model... model already exists. downloading hand model... model already exists. models downloaded. configuring incomplete, errors occurred! see also ""f:/rb_git/openpose/build/cmakefiles/cmakeoutput.log"".",deployment,question
1143,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1143,"""sudo make install"" does not install python api ubuntu 1604","issue summary i am trying to use the openpose library/api in conjunction with another python project. the tutorials fail @: ""from openpose import pyopenpose as op"" : ""importerror: cannot import name pyopenpose"" executed command (if any) > cd path/to/openpose/examples/tutorialpython > python 01frompython` in cmake and have this python script in the right folder? > traceback (most recent call last): > file ""01fromapibodyimage.py > error: openpose library could not be found. did you enable `buildbodyimage.py"", line 26, in > raise e > benjamin@helios0:~/cmu/openpose$ git status > on branch master > your branch is up-to-date with 'origin/master'. 3. ** api:",deployment,question
1534,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1534,Recommended way to run 3D pose estimation on 2 images using Python API,"hello there, sorry if it's the wrong place to post this but i've been trying to wrap my head around this for a while now. i am trying to run 3d pose estimation from 2 images using the openpose python api. the straightforward way to run 2d pose estimation from the python api is this: (i omitted information about my setup as i think there is no specific issue with it)",question,other
173,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/173,Ubuntu:acceleration of single person,issue summary i had successfully run on my ubuntu. i'd rather detect only single person with a higher speed. how can i speed it up by adjusting the structure?,question,question
1085,https://github.com/dusty-nv/jetson-inference/issues/1085,how to convert segnet mask output to cv::Mat Opencv c++,"as discussed in i am trying to add cv::mat cv8uc3, imgmask); i am facing problem during compilation. i think opencv is not linked properly. i changed build_experimental as yes to enable opencv usage. correct if i am wrong. help me.",question,question
97,https://github.com/dusty-nv/jetson-inference/issues/97,some changes been made to TensorRT?,"hello, i was previously able to compile all the provided examples in this repo for jetson. but recently after upgrading my host and reinstalling all required packages (latest updates) i got the following error message while compiling examples for jetson tx2: (to me it looks like some variables have changed their names in the latest tensorrt release but the current repo is not updated accordingly?) _t segnet::getnumclasses() const’: /home/shervin/desktop/jetson-inference/segnet.h:101:66: error: ‘const valuet getnumclasses() const { return moutputs[0].dims.c; } /home/shervin/desktop/jetson-inference/segnet.cpp: in static member function ‘static segnet, const char, const char, const char, float)’: /home/shervin/desktop/jetson-inference/segnet.cpp:356:30: error: ‘_cxx::_traits >::valuew = moutputs[0].dims.w; /home/shervin/desktop/jetson-inference/segnet.cpp:357:30: error: ‘_cxx::_traits >::valueh = moutputs[0].dims.h; /home/shervin/desktop/jetson-inference/segnet.cpp:358:30: error: ‘_cxx::_traits >::valuec = moutputs[0].dims.c; cmakefiles/jetson-inference.dir/build.make:1373: recipe for target 'cmakefiles/jetson-inference.dir/segnet.cpp.o' failed make[2]: **** [all] error 2__",question,question
709,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/709,How to set python version?,"my system is ubuntu16.04, and got python2.7 and python3.6 i set /usr/bin/python to python3.6. but after building openpose, it can only run by python2. the 1pose.py demo will encounters error like below if run by python3:",question,question
1456,https://github.com/dusty-nv/jetson-inference/issues/1456,SyntaxError: future deature annotations is not defined,"dear all, i 'm trying to run the training examples (**) but i got problem as the below screenshot: thanks all !",question,Error
410,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/410,OpenPose build failed VS2017 + Windows 10 + CUDA 9.1,posting rules 1. **: 3.3.1 compiler (`gcc --version` in ubuntu):,deployment,deployment
712,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/712,Extracting 3D keypoints in Json file.,"hi, is it possible to extract 3d keypoints from images of videos(both with single person only) using openpose? if yes then how? i tried many combinations similar to 2d extraction but ended up with error. thanks, mritula",question,question
1877,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1877,How to use multi thread to display multiple cameras on windows ?,"using either method or the second method will not work, the second camera will not show. the first method as follow: `for (uint8imagevideo), op::string(flagscamera), 1, flagscamera, flagscameraprocesstime ? op::producerfpsmode::originalfps : op::producerfpsmode::retrievalfps); auto producersharedptr02 = createproducer( producertype02, producerstring02.getstdstring(), camerasize, flagsparameterframe3dshared(producersharedptr01); auto wdatumproducer01 = std::makeshared>(producersharedptr02); auto wdatumproducer02 = std::makeshared(); // gui (display) auto gui01 = std::makefullscreen, threadmanager.getisrunningsharedptr()); // auto wgui01 = std::makeshared(outputsize, flagsshared(outputsize, flagsshared>(gui02); auto threadid00 = 0ull; auto queuein00 = 0ull; auto queueout00 = 1ull; auto threadid01 = 0ull; auto queuein01 = 0ull; auto queueout01 = 1ull; auto threadid02 = 0ull; auto queuein02 = 0ull; auto queueout02 = 1ull; threadmanager.add(threadid00++, wdatumproducer01, queuein01++, queueout01++); threadmanager.add(threadid00++, wgui01, queuein01++, queueout01++); threadmanager.add(threadid00++, wdatumproducer02, queuein01++, queueout01++); threadmanager.add(threadid00++, wgui01, queuein01++, queueout01++);",question,question
619,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/619,- Execution error Check failed: error == cudaSuccess (2 vs. 0) out of memory,"i run openpose in the nvidia tx2 board, but it always fails when some demo (path:build/examples/openpose/openpose.bin ) was runned. in fact ,i can run caffe with cuda8.0 and cudnn6.1 ubuntu： 16.04 memory ： 8g gpu（only one） ：gp10b the error is folowed : nvidia@tegra-ubuntu:/opt/openpose$ ./build/examples/openpose/openpose.bin starting openpose demo... auto-detecting camera index... detected and opened camera 0. auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. starting thread(s)... f0525 11:42:48.736145 22182 syncedmem.cpp:71] check failed: error == cudasuccess (2 vs. 0) out of memory **** @ 0x7f9a68b718 google::logmessage::fail() @ 0x7f9a68d614 google::logmessage::sendtolog() @ 0x7f9a68b290 google::logmessage::flush() @ 0x7f9a68deb4 google::logmessagefatal::~logmessagefatal() @ 0x7f99db71f8 caffe::syncedmemory::mutabledata() @ 0x7f99f0b8cc caffe::blob<>::mutabledata() @ 0x7f99f61ce4 caffe::cudnnconvolutionlayer<>::forwardmembase<>::operator()<>() @ 0x7f9b4c5868 bindmemptrist6vectorins1eees36workeris8ee9invokeijlm0eeeevst12tupleijxsptbindimpl<>::run() @ 0x7f9a8df280 (unknown) @ 0x7f9a9e6fc4 start_thread aborted (core dumped) nvidia@tegra-ubuntu:/opt/openpose$",question,question
396,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/396,Tensorflow (with GPU) as deep_net,issue summary it is not an issue but a question. how is it possible to use tensorflow as deep_net as in the makefile. does it mean that we won't need to use caffe at all? type of issue - question,question,question
628,https://github.com/dusty-nv/jetson-inference/issues/628,Mobilenet config setting,"hi dustin, actually its not an issue about this repo, however i wish you can help me. i have trained my object detector with pytorch, however i want to improve this, it has only one class and background, and makes false positives a lot, i think there is a problem in pytorch configs on the image augmentation part. my question is : when we adopt pytorch to onnx, how the tensorrt on jetson implements the image augmentations before inference? does it automatically or i should make something? thank you for reading, have a nice day",question,question
1235,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1235,Question : Should the new tracking feature work in CPU ONLY mode?,"it appears that with an i7 it works, but with i5 it does not work (it runs the same way as without the flag) both tried with the new 1.5 release (cpu only) thanks",question,question
80,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/80,Error Compiling OpenPose,"issue summary compile caffe and openpose by running these lines: then , there is an error as: ------------------------- compiling openpose error ------------------------- executed command (if any) openpose output (if any) type of issue you might select multiple topics, delete the rest: - compilation/installation error - execution error - help wanted - question - enhancement / offering possible extensions / pull request / etc - other (type your own type) your system configuration **: installed with `apt-get install libopencv-dev` compiler (`gcc --version` on ubuntu):gcc (gcc) 5.2.0",question,other
77,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/77,Performance Issue : Rapid switching between left and right bodyparts,"issue summary in the scenario when a pedestrian is moving perpendicular to the camera direction, openpose frequently confuses between the left and right body parts detection, switching between them frequently. so, for example, it would detect the knee as right in one frame and the same knee as left two/three frames later. i suppose, this can be improved using temporal information in the video or optical flow estimation of the target's movement. is there some functionality i'm overlooking or any suggestions? type of issue - help wanted - enhancement / offering possible extensions",Performance,other
245,https://github.com/dusty-nv/jetson-inference/issues/245,Implement ROI pooling layer in tensorRT,"hi, i have some problems when implementing my own roi pooling layer in tensorrt. it's similar to the thread ( i don't know how to change the batch size after roi pooling layer before feeding the tensor to the fully connected layers. the `getoutputdimension` of roi pooling layer is nchw (for example, for 2000 rois we have (2000, 256, 7,7). but is seems not work, the batch size of next fc layer is still only have `batchsize=1`. i have no idea about changing the batch size after the plugin layer. and advise? thanks",question,question
1757,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1757,The cuDNN still have a problem when compatibility with NVIDIA Geforce RTX 3080,"issue summary hi, we got the issue of ""out of memory"" error. we found that this is usually related to ""cudnn not being used"". could you please double-checked that vs project was built with ""enable cudnn"" in cmake? we have reinstalled cudnn but nothing change happened. then we used ""dependencies""/""dependencywalker"" on ""caffe.dll"" in the bin folder and found that it was depending on cudnn648.dll"" in openpose's bin folder. thank you so much! openpose output (if any) you might select multiple topics, delete the rest: - compilation/installation error - execution error your system configuration 1. **** issue: nvidia-smi 456.81 driver version: 457.09 cuda version: 11.1 -------------------------------+----------------------+----------------------+ gpu name tcc/wddm bus-id disp.a volatile uncorr. ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m. ===============================+======================+====================== 0 geforce rtx 3080 wddm 00000000:01:00.0 on n/a 0% 51c p8 34w / 340w 654mib / 10240mib 2% default +-------------------------------+----------------------+----------------------+",question,deployment
1098,https://github.com/dusty-nv/jetson-inference/issues/1098,Retraining ssd-mobilenet support,"hey, first of all, thank you for this wonderful tutorial, it helped me a lot. i am working on retraining ssd-mobilenet on my own choices of the class list (about 6 classes). i just wanna understand what --workers is used for and is it fine if i increase it or not? thanks in advance",other,question
1243,https://github.com/dusty-nv/jetson-inference/issues/1243,Failed to capture video frame,"i tried to find a solution to this problem but was unsuccessful. i installed jetson-inference according to the excellent documentation but i'm running into a problem. i'm using the raspberry pi camera 2.1 and have tested it on the nano and it works fine. here's the output: according to the instructions i'm running ./video-viewer /dev/video0 any help would be greatly appreciated. [gstreamer] gstcamera -- attempting to create device v4l2:///dev/video0 [gstreamer] gstcamera -- didn't discover any v4l2 devices [gstreamer] gstcamera -- device discovery failed, but /dev/video0 exists [gstreamer] support for compressed formats is disabled [gstreamer] gstcamera pipeline string: [gstreamer] v4l2src device=/dev/video0 ! appsink name=mysink [gstreamer] gstcamera successfully created device v4l2:///dev/video0 [video] created gstcamera from v4l2:///dev/video0 ------------------------------------------------ gstcamera video options: ------------------------------------------------ -- uri: v4l2:///dev/video0 -- devicetype: v4l2 -- iotype: input -- codec: unknown -- width: 1280 -- height: 720 -- framerate: 30.000000 -- bitrate: 0 -- numbuffers: 4 -- zerocopy: true -- flipmethod: none -- loop: 0 -- rtsplatency 2000 ------------------------------------------------ [opengl] gldisplay -- x screen 0 resolution: 1920x1080 [opengl] gldisplay -- x window resolution: 1920x1080 [opengl] gldisplay -- display device initialized (1920x1080) [video] created gldisplay from display://0 ------------------------------------------------ gldisplay video options: ------------------------------------------------ -- uri: display://0 -- devicetype: display -- iotype: output -- codec: raw -- width: 1920 -- height: 1080 -- framerate: 0.000000 -- bitrate: 0 -- numbuffers: 4 -- zerocopy: true -- flipmethod: none -- loop: 0 -- rtsplatency 2000 ------------------------------------------------ [gstreamer] opening gstcamera for streaming, transitioning pipeline to gstplaying [gstreamer] gstreamer changed state from null to ready ==> mysink [gstreamer] gstreamer changed state from null to ready ==> v4l2src0 [gstreamer] gstreamer changed state from null to ready ==> pipeline0 [gstreamer] gstreamer stream status create ==> src [gstreamer] gstreamer changed state from ready to paused ==> v4l2src0 [gstreamer] gstreamer changed state from ready to paused ==> pipeline0 [gstreamer] gstreamer message new-clock ==> pipeline0 [gstreamer] gstreamer stream status enter ==> src [gstreamer] gstreamer changed state from paused to playing ==> v4l2src0 [gstreamer] gstcamera -- end of stream (eos) [gstreamer] gstreamer message stream-start ==> pipeline0 [gstreamer] gstreamer v4l2src0 error internal data stream error. [gstreamer] gstreamer debugging info: gstbasesrc.c(3055): gstsrcstate_null [gstreamer] gstcamera -- pipeline stopped video-viewer: shutdown complete",other,question
1502,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1502,OpenPose Python package on jetson TX2,"hi guys, i'm trying to use your wonderful repo openpose on my jetson tx2. i can install it, and build it... no problem. but i meet problems when trying it with python. here is the log: > error: openpose library could not be found. did you enable `buildhandimage.py"", line 27, in > raise e > file ""07from_.py"", line 1, in > from . import pyopenpose as pyopenpose > importerror: cannot import name 'pyopenpose' i don't understand: ""...have this python script in the right folder?"" none of the other issues helped me. thank you,",question,question
1956,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1956,can't import pyopenpose on win10 after successfully compiling,"issue summary hi,i successfully compiled the code ,and i got these files. and then i successfully ran the example demos but then i ran into error when trying to import pyopenpose. i will appreciate if anyone can give me some help! executed command (if any) import pyopenpose errors (if any) importerror: dll load failed type of issue - compilation/installation error system configuration",question,question
580,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/580,Run demo failed---Segmentation fault (core dumped),"run demo i get failed , could you tell me how i can modify it. ./build/examples/openpose/openpose.bin --video examples/media/video.avi dmesg [21534.368592] traps: openpose.bin[11475] general protection ip:7f6463b075a6 sp:7ffe9e335640 error:0 in libopencvjan13:22:032017 cuda compilation tools, release 8.0, v8.0.61 +-----------------------------------------------------------------------------+ nvidia-smi 384.130 driver version: 384.130 -------------------------------+----------------------+----------------------+ gpu name persistence-m bus-id disp.a volatile uncorr. ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m. ===============================+======================+====================== 0 geforce gtx 1070 off 00000000:01:00.0 on n/a n/a 45c p2 34w / n/a 804mib / 8108mib 0% default +-------------------------------+----------------------+----------------------+ thanks !",question,question
757,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/757,make error  -  list sub-command REMOVE_ITEM requires list to be present,"issue summary cmake -dbuildpython=on ..` openpose output (if any) type of issue you might select multiple topics, delete the rest: - compilation/installation error - execution error - help wanted - question - enhancement / offering possible extensions / pull request / etc - other (type your own type) your system configuration 1. ** issue:",question,other
1200,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1200,"OpenPose Windows 10 Build: Cuda 8, Cudnn 5.1, VS 2015, Cmake 3.14.2, GTX 1060","issue summary: i am trying to build openpose from source but i am receiving an error in cmake gui. errors (if any): cmake warning (dev) at cmakelists.txt:81 (set): implicitly converting 'type' to 'string' type. this warning is for project developers. use -wno-dev to suppress it. type of issue: - compilation/installation error - help wanted your system configuration: 2. openpose version: latest release as i cloned it today. 3. general configuration: 4. non-default settings: 5. 3rd-party software: 6. if gpu mode issue: 7. cmake output: selecting windows sdk version 10.0.14393.0 to target windows 10.0.17134. the c compiler identification is msvc 19.0.24215.1 the cxx compiler identification is msvc 19.0.24215.1 check for working c compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/cl.exe check for working c compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/cl.exe -- works detecting c compiler abi info detecting c compiler abi info - done detecting c compile features detecting c compile features - done check for working cxx compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/cl.exe check for working cxx compiler: c:/program files (x86)/microsoft visual studio 14.0/vc/bin/cl.exe -- works detecting cxx compiler abi info detecting cxx compiler abi info - done detecting cxx compile features detecting cxx compile features - done cmake warning (dev) at cmakelists.txt:81 (set): implicitly converting 'type' to 'string' type. this warning is for project developers. use -wno-dev to suppress it. found cuda: c:/program files/nvidia gpu computing toolkit/cuda/v8.0 (found version ""8.0"") building with cuda. downloading windows dependencies... downloading extracting c:/users/ashle/documents/github/openpose/3rdparty/windows/opencvv14201914.zip... downloading extracting c:/users/ashle/documents/github/openpose/3rdparty/windows/caffe3rdparty201914.zip... downloading extracting c:/users/ashle/documents/github/openpose/3rdparty/windows/caffe201914.zip... windows dependencies downloaded. adding example calibration adding example openposedemo adding example 1postbodyimagewholefromdefault adding example 03fromkeypointsimages adding example 05frommultifaceimage adding example 07fromheatmapsimage adding example 09fromasynchronousinput adding example 11customasynchronousinputandsynchronousinput adding example 14customsynchronouspostprocessing adding example 16customsynchronousalldatum adding example 1userfunction adding example 2userprocessingand25 model... note: this process might take several minutes depending on your internet connection. not downloading body (coco) model not downloading body (mpi) model downloading face model... note: this process might take several minutes depending on your internet connection. downloading hand model... note: this process might take several minutes depending on your internet connection. models downloaded. configuring done generating done",deployment,question
68,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/68,crash with a video input,issue summary the openpose.bin crashes with input of a video executed command (if any) ./build/examples/openpose/openpose.bin --video input.mp4 openpose output (if any) starting pose estimation demo. starting thread(s) real-time pose estimation demo successfully finished. total time: 8.784754 seconds. ****: installed with opencv 3.x.,deployment,question
1890,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1890,[Mac] Makefile returns due to undefined symbols for architecture x86_64,"issue summary thanks for all the great work. i have generated the makefile by using the recommended options (see snapshot below), but when i run the suggested makefile build command i am getting some linking errors that i'm unable to resolve by relinking, reinstalling brew/pip packages. i've tried to reinstall caffe (`brew uninstall caffe; brew install caffe`), i reinstalled all the brew collaterals (`brew list xargs brew reinstall`) and i tried rebuilding cmake multiple times. (fyi - i was getting a different set of errors before installing `freeglut`, `glew`, `glfw`, `glm`. i don't have the error messages saved, but maybe this information could be helpful) executed command (if any) ``make -j`sysctl -n hw.logicalcpu` `` openpose output (if any) errors (if any) [ 39%] linking cxx shared library libopenpose.dylib undefined symbols for architecture x86glutleavemainloop"", referenced from: ""64 clang: error: linker command failed with exit code 1 (use -v to see invocation) make[2]: ***** api:",deployment,question
702,https://github.com/dusty-nv/jetson-inference/issues/702,Failed Using Xavier ONNX Model on Nano,"should an image classification model built and used on a xavier run when moved to a nano? here is what i did and the error i received: successfully test onnx model with imagenet on xavier install and build the same version of jetson-inference on nano error: warning: onnx model has a newer irname) [trt] failed to parse onnx model './aslmodel/resnet18.onnx [trt] failed to load ./asl_model/resnet18.onnx [trt] imagenet -- failed to initialize. imagenet: failed to initialize imagenet i tried reconverting the model from tf to onnx again on the nano, but that did not help. next, i will retrain the model from scratch on the nano. tia.",question,question
100,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/100,can openpose process multi images at one time,if can't what should i do,question,question
959,https://github.com/dusty-nv/jetson-inference/issues/959,too many resources requested for launch,"2021-03-21 13:50:20 - start training from epoch 0. /usr/local/lib/python3.6/dist-packages/torch/optim/lrscheduler.step()` before `optimizer.step()`. in pytorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lrreduction.py:44: userwarning: sizessd.py"", line 343, in file ""train_.py"", line 127, in backward **",question,Error
1606,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1606,Python synchronous output,"issue summary hello, i am curious if it is possible to replicate the same results from one of the c++ examples '17custom_output.cpp' with the python api. the example above prints the keypoint coordinates while simultaneously streaming the detected joints in the gui. thanks in advance!",question,other
733,https://github.com/dusty-nv/jetson-inference/issues/733,Question about mobilenet-v1-ssd-mp-0_675.pth,"does anyone know which classes this network is pre-trained on? i understand that the original ssd-mobilenet-v1 is pre-trained on the 80 classes of the coco dataset. however this repo documentation that the base net we are using, ** pre-trained on coco and then fine-tuned on pascal voc? thanks in advance!",question,question
522,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/522,Best way to run it on a folder of videos [Ubuntu],"i have openpose installed and running correctly on my ubuntu system. i just wanted to ask, my goal is to run openpose on all the videos i have on a dataset. what is the most efficient way of doing so? currently i'm using a shell script to call the openpose demo on a single video and then looping over all the videos. is there a way to do it the same way i can do it for images? (i.e. i just input the folder where all the images are and it goes through all of them) thanks!",question,question
350,https://github.com/dusty-nv/jetson-inference/issues/350,runtime.cpp (30) - Cuda Error in free: 4 terminate called after throwing an instance of 'nvinfer1::CudaError',"works fine when i change my code to run the inference on a recorded video > [opengl] gldisplay -- x screen 0 resolution: 1600x900 > [opengl] gldisplay -- display device initialized > jetson.utils -- cudafromnumpy() ndarray dim 0 = 480 > jetson.utils -- cudafromnumpy() ndarray dim 1 = 640 > jetson.utils -- cudafromnumpy() ndarray dim 2 = 3 > [trt] engine.cpp (555) - cuda error in execute: 4 > [trt] engine.cpp (555) - cuda error in execute: 4 > [trt] detectnet::detect() -- failed to execute tensorrt context > traceback (most recent call last): > file ""./detectnet-camera.py"", line 82, in > detections = net.detect(img, width, height) > exception: jetson.inference -- detectnet.detect() encountered an error classifying the image > pytensornet_dealloc() > [cuda] cudafreehost(mdetectionsets[0]) > [cuda] unspecified launch failure (error 4) (hex 0x04) > [cuda] /home/rlpl123/jetson-inference/detectnet.cpp:66 > [cuda] cudafreehost(mclasscolors[0]) > [cuda] unspecified launch failure (error 4) (hex 0x04) > [cuda] /home/rlpl123/jetson-inference/detectnet.cpp:74 > [trt] runtime.cpp (30) - cuda error in free: 4 > terminate called after throwing an instance of 'nvinfer1::cudaerror' > what(): std::exception > aborted type of data being fed into the detection using camera and the recorded video is the same. `img` -> `pycapsule` 'height` -> `int` `width` -> `int` any solution?",question,question
946,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/946,A question about video background,"dear authors,",question,other
91,https://github.com/dusty-nv/jetson-inference/issues/91,unspecified launch failure (error 4) (hex 0x04),"everything is running smoothly until i included the following line into my project: cudafloat2(0.0f, 255.0f), (float4)input, (float4)imgrgba, makefloat2(0.0f, 1.0f), camera->getwidth(), camera->getheight()) [cuda] invalid device pointer (error 17) (hex 0x11) [cuda] /home/ubuntu/jetson-inference/detectnet-camera/detectnet-camera.cpp:247 [cuda] cudagraphicsglregisterbuffer(&minteropcuda, mdma, cudagraphicsregisterflagswritediscard) [cuda] unspecified launch failure (error 4) (hex 0x04) [cuda] /home/ubuntu/jetson-inference/util/display/gltexture.cpp:235 [cuda] cudagetlasterror()",question,question
151,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/151,Error saying drivers_types.h file is not found,"while executing the demo on windows 10, the following error came up: cannot include drivers_types.h no such directory or file. any help would be appreciated. thanks in advance :) your system configuration **: installed with `apt-get install",question,question
1334,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1334,problem about pose keypoints detection,"issue summary i tried to use coco model for pose keypoints detection. i obtained keypoints information in json format. i found that some pictures tested in my test had been detected twice. why these pictures had been detected twice? type of issue you might select multiple topics, delete the rest: - help wanted - question",question,question
964,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/964,VS2015+openpose-master→LINK : fatal error LNK1561: Entry points must be defined,"hello，@gineshidalgo99 im a rookie from china，it seems a dump question，cuz nobody answer me in the study group，i dont know how to fix it. can you help me? operations performed (if any) when i build the “openposedemo” programme openpose output (if any) 1> all 10112 functions were compiled because no usable ipdb/iobj from previous compilation was found. 1> the code generation is complete 1> openpose.vcxproj -> d:\openpose-master\windows\x64\release\openpose.dll 1> openpose.vcxproj -> d:\openpose-master\windows\x64\release\openpose.pdb (full pdb) 2>------ all rebuilds have been started: project: openposedemo, configuration: release x64 ------ *argv[]) { google::initgooglelogging(""openposedemo""); gflags::parsecommandlineflags(&argc, &argv, true); return openposedemo(); } have a nice day！：）",question,question
893,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/893,how to extract keypoints quickly by body25?,"i want to write a c++ function which input a image by opencv, and output the keypoints coordinate cv::point, how can i do?",question,question
599,https://github.com/dusty-nv/jetson-inference/issues/599,threshold hard coded in jetson.inference.detectNet()?,"the default value for the threshold is 0.5, but when i changed the default value to 0.2 in the code, or modified it through --threshold 0.2. the detectnet was still loading the model using 0.5. i even tried to change the line of code below net = jetson.inference.detectnet(""ssd-inception-v2"", sys.argv, opt.threshold) to net = jetson.inference.detectnet(""ssd-inception-v2"", sys.argv, 0.2) the following part was printed to the terminal when running the code. detectnet -- loading detection network model from:",question,question
338,https://github.com/dusty-nv/jetson-inference/issues/338,./imagenet-camera googlenet GST_ARGUS: Available Sensor modes : Segmentation fault (core dumped),"ran the command `./imagenet-camera googlenet` inside > jetson-inference/build/aarch64/bin i get this > imagenet-camera > args (2): 0 [./imagenet-camera] 1 [googlenet] > > [gstreamer] initialized gstreamer, version 1.14.1.0 > [gstreamer] gstcamera attempting to initialize with gstnvcamera > [gstreamer] gstcamera pipeline string: > nvcamerasrc fpsrange=""30.0 30.0"" ! video/x-raw(memory:nvmm), width=(int)1280, height=(int)720, format=(string)nv12 ! nvvidconv flip-method=2 ! video/x-raw ! appsink name=mysink > [gstreamer] gstcamera failed to create pipeline > [gstreamer] (no element ""nvcamerasrc"") > [gstreamer] failed to init gstcamera (gstnvcamera) > [gstreamer] gstcamera attempting to initialize with gstnvargus > [gstreamer] gstcamera pipeline string: > nvarguscamerasrc ! video/x-raw(memory:nvmm), width=(int)1280, height=(int)720, framerate=30/1, format=(string)nv12 ! nvvidconv flip-method=2 ! video/x-raw ! appsink name=mysink > [gstreamer] gstcamera successfully initialized with gstnvargus > > imagenet-camera: successfully initialized video device > width: 1280 > height: 720 > depth: 12 (bpp) > > > imagenet -- loading classification network model from: > -- prototxt networks/googlenet.prototxt > -- model networks/bvlclabels networks/ilsvrc12words.txt > -- inputblob 'prob' > -- batchgooglenet.caffemodel.2.1.gpu.fp16.engine > [trt] loading network profile from engine cache... networks/bvlcgooglenet.caffemodel loaded > [trt] device gpu, cuda engine context initialized with 2 bindings > [trt] binding -- index 0 > -- name 'data' > -- type fp32 > -- in/out input > -- # dims 3 > -- dim #0 3 (channel) > -- dim #1 224 (spatial) > -- dim #2 224 (spatial) > [trt] binding -- index 1 > -- name 'prob' > -- type fp32 > -- in/out output > -- # dims 3 > -- dim #0 1000 (channel) > -- dim #1 1 (spatial) > -- dim #2 1 (spatial) > [trt] binding to input 0 data binding index: 0 > [trt] binding to input 0 data dims (b=2 c=3 h=224 w=224) size=1204224 > [cuda] cudaallocmapped 1204224 bytes, cpu 0x100e30000 gpu 0x100e30000 > [trt] binding to output 0 prob binding index: 1 > [trt] binding to output 0 prob dims (b=2 c=1000 h=1 w=1) size=8000 > [cuda] cudaallocmapped 8000 bytes, cpu 0x100f60000 gpu 0x100f60000 > device gpu, networks/bvlcgooglenet.caffemodel loaded > imagenet -- loaded 1000 class info entries > networks/bvlcstateargus: creating output stream > consumer: waiting until producer is connected... > gstcamera -1` to `#define default_camera dev/video0` what's going wrong?",question,question
606,https://github.com/dusty-nv/jetson-inference/issues/606,Compiling the Project(Hello AI world) erro with Makefile:117: recipe for target 'install' failed make: *** [install] Error 1,"hi @dusty-nv ： `install the project... -- install configuration: """" -- installing: /usr/local/include/jetson-inference/detectnet.h -- installing: /usr/local/include/jetson-inference/homographynet.h -- installing: /usr/local/include/jetson-inference/imagenet.h -- installing: /usr/local/include/jetson-inference/segnet.h -- installing: /usr/local/include/jetson-inference/superresnet.h -- installing: /usr/local/include/jetson-inference/tensornet.h -- installing: /usr/local/include/jetson-inference/imagenet.cuh -- installing: /usr/local/include/jetson-inference/randint8calibrator.h -- installing: /usr/local/lib/libjetson-inference.so -- set runtime path of ""/usr/local/lib/libjetson-inference.so"" to """" -- installing: /usr/local/share/jetson-inference/cmake/jetson-inferenceconfig.cmake -- installing: /usr/local/share/jetson-inference/cmake/jetson-inferenceconfig-noconfig.cmake -- installing: /usr/local/bin/imagenet-console -- set runtime path of ""/usr/local/bin/imagenet-console"" to """" -- installing: /usr/local/bin/imagenet-camera -- set runtime path of ""/usr/local/bin/imagenet-camera"" to """" -- installing: /usr/local/bin/detectnet-console -- set runtime path of ""/usr/local/bin/detectnet-console"" to """" -- installing: /usr/local/bin/detectnet-camera -- set runtime path of ""/usr/local/bin/detectnet-camera"" to """" -- installing: /usr/local/bin/segnet-console -- set runtime path of ""/usr/local/bin/segnet-console"" to """" -- installing: /usr/local/bin/segnet-camera -- set runtime path of ""/usr/local/bin/segnet-camera"" to """" -- installing: /usr/local/bin/superres-console -- set runtime path of ""/usr/local/bin/superres-console"" to """" -- installing: /usr/local/bin/homography-console -- set runtime path of ""/usr/local/bin/homography-console"" to """" -- installing: /usr/local/bin/homography-camera -- set runtime path of ""/usr/local/bin/homography-camera"" to """" -- installing: /usr/local/bin/camera-capture -- set runtime path of ""/usr/local/bin/camera-capture"" to """" -- installing: /usr/local/include/jetson-utils/xml.h -- installing: /usr/local/include/jetson-utils/commandline.h -- installing: /usr/local/include/jetson-utils/filesystem.h -- installing: /usr/local/include/jetson-utils/mat33.h -- installing: /usr/local/include/jetson-utils/pi.h -- installing: /usr/local/include/jetson-utils/rand.h -- installing: /usr/local/include/jetson-utils/timespec.h -- installing: /usr/local/include/jetson-utils/gstcamera.h -- installing: /usr/local/include/jetson-utils/v4l2camera.h -- installing: /usr/local/include/jetson-utils/gstdecoder.h -- installing: /usr/local/include/jetson-utils/gstencoder.h -- installing: /usr/local/include/jetson-utils/gstutility.h -- installing: /usr/local/include/jetson-utils/cudafont.h -- installing: /usr/local/include/jetson-utils/cudamappedmemory.h -- installing: /usr/local/include/jetson-utils/cudanormalize.h -- installing: /usr/local/include/jetson-utils/cudaoverlay.h -- installing: /usr/local/include/jetson-utils/cudargb.h -- installing: /usr/local/include/jetson-utils/cudaresize.h -- installing: /usr/local/include/jetson-utils/cudautility.h -- installing: /usr/local/include/jetson-utils/cudawarp.h -- installing: /usr/local/include/jetson-utils/cudayuv.h -- installing: /usr/local/include/jetson-utils/gldisplay.h -- installing: /usr/local/include/jetson-utils/gltexture.h -- installing: /usr/local/include/jetson-utils/glutility.h -- installing: /usr/local/include/jetson-utils/imageio.h -- installing: /usr/local/include/jetson-utils/loadimage.h -- installing: /usr/local/include/jetson-utils/devinput.h -- installing: /usr/local/include/jetson-utils/devjoystick.h -- installing: /usr/local/include/jetson-utils/devkeyboard.h -- installing: /usr/local/include/jetson-utils/endian.h -- installing: /usr/local/include/jetson-utils/ipv4.h -- installing: /usr/local/include/jetson-utils/networkadapter.h -- installing: /usr/local/include/jetson-utils/socket.h -- installing: /usr/local/include/jetson-utils/event.h -- installing: /usr/local/include/jetson-utils/mutex.h -- installing: /usr/local/include/jetson-utils/process.h -- installing: /usr/local/include/jetson-utils/thread.h -- installing: /usr/local/lib/libjetson-utils.so -- installing: /usr/local/share/jetson-utils/cmake/jetson-utilsconfig.cmake -- installing: /usr/local/share/jetson-utils/cmake/jetson-utilsconfig-noconfig.cmake -- installing: /usr/local/bin/camera-viewer -- set runtime path of ""/usr/local/bin/camera-viewer"" to """" -- installing: /usr/local/bin/gl-display-test -- set runtime path of ""/usr/local/bin/gl-display-test"" to """" -- installing: /usr/local/bin/camera-viewer.py -- installing: /usr/local/bin/cuda-from-numpy.py -- installing: /usr/local/bin/cuda-to-numpy.py -- installing: /usr/local/bin/gl-display-test.py -- installing: /usr/lib/python2.7/dist-packages/jetsonpython.so -- set runtime path of ""/usr/lib/python2.7/dist-packages/jetsonpython.so"" to """" -- up-to-date: /usr/lib/python2.7/dist-packages/jetson -- up-to-date: /usr/lib/python2.7/dist-packages/jetson/utils -- installing: /usr/lib/python2.7/dist-packages/jetson/utils/_pythoninstall.cmake:89 (file): file install cannot find ""/home/nano01/jetson-inference/utils/python/bindings/../python/jetson"". call stack (most recent call first): utils/python/cmakeinstall.cmake:229 (include) cmake_install.cmake:128 (include) make: *** [install] error 1 `",question,question
227,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/227,Missing prototxt - Windows,"system configuration windows 10 visual studio 2015 cuda 8.0 cudnn 8.0 graphic card nvidia geforce gtx 1070 we are a research team and have very little knowledge on installing software. issue summary we are attempting to install the openpose software once we get to step 4 of installation and run openpose.sln microsoft's visual studio says unable to start program c:\openpose-master\windows\x64\release\openpose.dll. it says the file is not a valid win32 application. executed command (if any) when we attempt to run the openposedemo.exe from command we receive. f0828 13:19:44.212276 3052 io.cpp:41 check failed: fd != -1 (-1 vs. -1) file not found: models/pose/coco/poselinevec.protxt. checke failure stack trace: openpose output (if any) type of issue you might select multiple topics, delete the rest: - compilation/installation error - execution error - help wanted - question - enhancement / offering possible extensions / pull request / etc - other (type your own type)",question,question
303,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/303,CNN inferences using Movidius NCS,is it possible to use movidius ncs for cnn inferences to get paf and confidence map and rest calculation on rpi 3?,other,question
19,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/19,Release version works but debug version does not - CUDA (7 vs. 0): too many resources requested,"issue summary @zhaishengfu issue #13: > when i compile using debug mode, there are still errors with: terminate called after throwing an instance of 'std::runtimerelease -a` on ubuntu): distributor id: ubuntu description: ubuntu 14.04.3 lts release: 14.04 codename: trusty ** (`nvidia-smi`): gtx-1070 compiler (`gcc --version` on ubuntu): and my cpu is 4 core",Error,deployment
684,https://github.com/dusty-nv/jetson-inference/issues/684,'jetson.utils' has no attribute 'videoSource',"i'm attempting to run my-detection.py using an rtsp feed on my network. using the instructions it said i can add it modifying this line. #camera = jetson.utils.videosource(""rtsp://*****"") however, jetson.utils is reporting this... attributeerror: module 'jetson.utils' has no attribute 'videosource' pytensornet_dealloc()",question,Error
342,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/342,Can't use OpenPose in my project,"issue summary i tried to use openpose in my project, but i think i'm having errors without cmakelists, which is this: executed command (if any) after running cmake . when i run make i find the error below. openpose output (if any) `/usr/bin/ld: cmakefiles/openpose.dir/openpose.cpp.o: undefined reference '64-linux-gnu/libgflags.so.2: error adding symbols: dso missing from command line collect2: error: ld returned 1 exit status cmakefiles/openpose.dir/build.make:96: recipe for target 'openpose' failed make[2]: *****: compiled from source. version 3.3.1 gcc compiler: 5.4.0",question,question
207,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/207,[help] Windows Complie error,i follow the setups in /doc/installation.md#installation---library， however，in the setup ， some errors come out like these: anything i should deploy in vs?,question,question
1995,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1995,"How do python API call parameters (NMSThreshold, ConnectMinSubScore, ConnectInterThreshold, ConnectInterMinAboveThreshold, ConnectMinSubsetCnt)?","i find ""gui help"" in demo mode can adjust openpose performance by using ""- = _ + [ ] { } ; \\"" to change parmeters (nmsthreshold, connectminsubscore, connectinterthreshold, connectinterminabovethreshold, connectminsubsetcnt). how do python api call these parameters ? python version: 3.6.9 any suggestion is appreciated.",question,question
311,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/311,is it possible to use the streaming data from the webcam and analyze in Kinect SDK 2.0? ,"i am getting the data(x,y) from the webcam and i want it to pass to the kinect sdk and perform some analysis on it. but the issue with the kinect sdk so far it it doesn't take any video format other than ""**"" . i tried using kinect as a web cam directly but when i do so, the rgb data that i got also has only x and y data and the stream data format is not .xef as well so i am not able to process it in kinect sdk. any kind of help regarding this will be appreciated.",other,question
1124,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1124,Jetson install error during openpose compile,"issue summary i am on a jetson tx1. i know this is not officially supported, however i am hoping someone can provide some insight. i am getting an error ""'.buildversion := 3 in /scripts/ubuntu/makefile.config.ubuntu16jetsontx2, /3rdparty/caffe/makefile.config.ubuntu16cuda8release/examples/tests/resizetest.bin /usr/bin/ld: warning: libopencvhighgui.so.2.4, needed by 3rdparty/caffe/distribute/lib/libcaffe.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libopencvrelease/lib/libopenpose.so: undefined reference to `cv::write(cv::filestorage&, std::_string, std::allocator > const&, cv::mat const&)' .buildstring, std::allocator > const&, cv::release/lib/libopenpose.so: undefined reference to `cv::circle(cv::mat&, cv::point const&, int, int, int)' .buildstring, std::allocator > const&) const' .buildoutputarray::release/lib/libopenpose.so: undefined reference to `cv::videowriter::videowriter(std::_string, std::allocator > const&, int, double, cv::sizerelease/lib/libopenpose.so: undefined reference to `cv::drawchessboardcorners(cv::, cv::release/lib/libopenpose.so: undefined reference to `cv::exception::exception(int, std::_string, std::allocator > const&, std::_string, std::allocator > const&, std::_string, std::allocator > const&, int)' .buildstring, std::allocator > const&, int, std::_string, std::allocator > const&)' .buildstring, std::allocator > const&, cv::release/lib/libopenpose.so: undefined reference to `cv::operator, std::allocator > const&)' .buildinputarray::release/lib/libopenpose.so: undefined reference to `cv::videocapture::videocapture(std::_string, std::allocator > const&)' .buildrelease/lib/libopenpose.so: undefined reference to `cv::cascadeclassifier::load(std::_string, std::allocator > const&)' .buildinputarray const&, cv::, cv::sizerelease/lib/libopenpose.so: undefined reference to `vtable for cv::string, std::allocator > const&, cv::release/lib/libopenpose.so: undefined reference to `cv::inputarray(double const&)' .buildinputarray' .buildstring, std::allocator > const&, int, int)' .buildobj()' .buildinputarray::release/lib/libopenpose.so: undefined reference to `cv::cascadeclassifier::detectmultiscale(cv::mat const&, std::vector, std::allocator > >&, double, int, int, cv::size)' .buildstring, std::allocator > const&, cv::point, int, int, bool)' .buildinputarray const&, cv::inputarray const&, cv::outputarray const&, cv::, int, cv::termcriteria, int, double)' .buildobj()' .buildoutputarray::release/lib/libopenpose.so: undefined reference to `cv::line(cv::mat&, cv::point, cv::scalarrelease/lib/libopenpose.so: undefined reference to `cv::gettextsize(std::_string, std::allocator > const&, int, double, int, int)' .buildstring, std::allocator > const&, int)' .buildinputarray const&, cv::, cv::outputarray const&, cv::outputarray const&, int, cv::termcriteria)' .buildstring, std::allocator > const&, int)' collect2: error: ld returned 1 exit status makefile:492: recipe for target '.buildrelease/examples/tests/resizetest.bin] error 1 make: **)' .buildstring, std::allocator > const&, int, double)' .buildrelease/lib/libopenpose.so: undefined reference to `cv::namedwindow(std::_string, std::allocator > const&, int)' .buildinputarray const&, cv::, cv::outputarray const&, cv::outputarray const&, int, cv::termcriteria)' .buildstring, std::allocator > const&, int)' collect2: error: ld returned 1 exit status makefile:492: recipe for target '.buildrelease/examples/tests/cltest.bin] error 1 ------------------------- ------------------------- errors detected. exiting script. the software might have not been successfully installed.",question,Error
249,https://github.com/dusty-nv/jetson-inference/issues/249,Segmentation with 3x8-bit pixels Images in Cityscapes,"hi there @dusty-nv , i was trying to get the segmentation example to work with a different dataset. for labeling, i used the same tools as cityscapes, but changed the colors of my labels. the problem is that sometimes when training on digits i get the `check failed: status == cublas statuserror` error, somehow expanded , it seemed to be the pil conversion (to generate the labels i use the same script with the difference of changing rgba to rgb) and . the error itself was very tricky to find, i think it has to do with the colors i'm using and the values of the generated labels. so my question is: - to train cityscapes on digits, how did you generate the labels? using the same script? any other changes?",question,question
377,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/377,Window portable demo version detects a ghost in a solo dance,"issue summary the portable version on window detect 2 people in a video of a single performer. openpose output (if any) i attached an image of the frame that the error occur type of issue you might select multiple topics, delete the rest: - execution error your system configuration **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? compiler (`gcc --version` in ubuntu):",question,other
432,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/432,osc support,"hello , the demo is quite impressive, thanks for making this available ! i would just like to propose an enhancement : what about giving out some osc support with the binaries ? ideally all the data could be streamed to software used by artists such as : pure-data, processing, openframeworks etc.",other,other
1026,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1026,1_body_from_image.py stops at opWrapper.configure(params) on Windows,"issue summary i am running python api, but 1frombodyimage.py stops at `opwrapper.configure(params)`. i know this because i modified the starting openpose part in 1fromlevel 0 --disablethread` to get higher debug information. `python 1fromlevel 0 --disablethread` openpose output (if any) errors (if any) type of issue - execution error your system configuration 1. ** system:",question,Error
744,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/744,how to use the new trained model,"hi dudes, i have trained the model with my own data, which is sort of coco-like model. i thhink the model name is 'coco' and use the poselinevec.prototxt, but is it a 'must' to rename the .caffemodel file into pose440000.caffemodel ?",question,question
2038,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/2038,Update repo url,since there is no need for the long url of the repo. it can be reduced to `,other,other
623,https://github.com/dusty-nv/jetson-inference/issues/623,install-pytorch.sh issues with yolov5,"feel free to send me somewhere else as this is off-topic. i am trying to run the yolov5 examples and trianing on my xavier from this repo: the first big hurdle was getting pytorch installed which the jetson-interface script fixed. now that all python libraries are installed, running ""python3 detect.py --source ./inference/images/ --weights yolov5s.pt --conf 0.4"" starts, gives a torch warning and core dumps. any ideas? tia `",other,other
335,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/335,Ubuntu. Cmake build Error at OpenPoseConfig.cmake,"hello.i am doing another project with openpose and i build my source code completely just a hour ago. but it is not working now. i don't know why it is. can you give me a little hint ?.. i searched all of your issue and google . but there are no hint. this is my cmakelists.txt (i wrote this from your ""installationminimumdefinitions(-std=c++11) list(append cmakepath ""${cmakesourcepackage(gflags) findpackage(opencv) finddirectories(${openposedirs} ${gflagsdir} ${glogdir} ${opencvdirs}) addlinklibs} ${gflagslibrary} ${opencvlink64-linux-gnu/libgflags.so) -- found glog: /usr/include -- found glog (include: /usr/include, library: /usr/lib/x86package) see also ""/home/hansb/openpose/examples/user_code/cmakefiles/cmakeoutput.log"". -------------------------------------------------------------------------------------------------------------------------- i think one possible reason is that i did ""cmake . && make"" at openpose main folder. so it run and i was surprised and stopped it.. i thought maybe it had a problem because i stopped, so i did it again and it was completed. but ""cmake . "" is not working. can i get a advice..? thank you.",question,question
38,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/38,[question] Output 3D coordinates available,"first of all: this is really something here, really good piece of code and math behind and tricks :-) i have one question: can i export 3d coordinates out or it is not in scope of this project?",question,question
174,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/174,Ubuntu install caffe and openpose script fails on fresh install of ubuntu 16.04,i installed cuda then copied cudnn into cuda-8.0 folder as instructed and ran the ./ubuntu/installcaffe script it runs fine.,deployment,deployment
933,https://github.com/dusty-nv/jetson-inference/issues/933,Image Size for SSD Architecture,"when looking at the ssd architecture, it seems like the input image needs to be 300x300x3. if we are using images much larger than that for training, will that increase the amount of time taken to train a model and would the large image size hurt accuracy? if that's the case, would we have to resize the image and then annotate before training the model?",question,question
382,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/382,Unable to build even after proper installation through cmake,"failure coming up on running the demo after installation starting pose estimation demo. auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. starting thread(s) f0118 11:44:16.078200 13991 cudnnlayer.cpp:53] check failed: status == cudnnsuccess (6 vs. 0) cudnnarch_mismatch **** aborted (core dumped)",question,deployment
1339,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1339,Cuda check failed (35 vs. 0): CUDA driver version is insufficient for CUDA runtime version,"hi all, i have tried to install openpose via the instruction here (no gpu, cpu only on my pc) and everything is ok (pyopenpose.cp37-winapibodyimage.py"". does it mean that i should not install cuda on my pc as i run it using cpu but not gpu. thanks!!",question,question
755,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/755,RUN ./build/examples/openpose/openpose.bin --video examples/media/video.avi,"starting pose estimation demo. gstreamer plugin: embedded video playback halted; module decodebin20 reported: your gstreamer installation is missing a plug-in. auto-detecting gpus... detected 2 gpu(s), using them all. warning: gstreamer: unable to seek (/builddir/build/build/opencv-2.4.5/modules/highgui/src/capgstreamer.cpp:641) warning: gstreamer: unable to query position of stream (/builddir/build/build/opencv-2.4.5/modules/highgui/src/capgstreamer.cpp:641) empty frame detected, frame number 0 of 0. in src/openpose/producer/producer.cpp:checkframeintegrity():148 warning: gstreamer: unable to query position of stream (/builddir/build/build/opencv-2.4.5/modules/highgui/src/capgstreamer.cpp:660) input images must be 3-channel bgr. converting your grey image into bgr. in ./include/openpose/producer/datumproducer.hpp:checkifrunningandgetdatum():107 (openpose.bin:31807): gtk-warning **: cannot open display:",other,question
1554,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1554,how to create openpose human hand grasping object,how to create openpose human hand grasping object,question,question
2143,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/2143,"remote vscode : No module named 'openpose' but success in shell , why？",i use vscode to connect remotely pc and debug demo '01fromeasymocap) lrd@rtx3090-super-server:~/data/lrd/easymocapfiler ; /usr/bin/env /home/lrd/.conda/envs/lfiler/openpose/build/examples/tutorialpython/01frompython` in cmake and have this python script in the right folder? > no module named 'openpose' but input `python 01fromeasymocap/bin/python /usr/bin/python` debug error (lfiler/openpose/build/examples/tutorialpython$ which python /home/lrd/.conda/envs/l_easymocap/bin/python it work in shell where could i have gone wrong? or it cannot be debugged remotely? or openpose lib should be moved to env python site-package？,question,question
557,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/557, error: ‘to_string’ is not a member of ‘std’,hi! by final part of installation ($ sudo make -j`nproc`) i got errors. how should i deal with it?,question,question
898,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/898,"Mac Install, error running make command in build folder - ","mac osx 10.13.6 intel core i7 (4 core) nvidia gforce gt 650m (cuda compatible) issue summary used cmake to successfully configure and generate the code, but then when running cd build/ make -j`nproc`, error occurs after this step: first warning seen in log: i get a number of output warnings like that, and then the last output i see (the only actual error) is: attached is my cmake output log, didn't look like there were any errors in there. executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. make -j`nproc` type of issue - compilation/installation error your system configuration 1. **: cmake after compiling caffe with",deployment,question
1401,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1401,"CMake Error: The following variables are used in this project, but they are set to NOTFOUND.","issue summary i getting this msg always when trying to install openpose! i'm sure i got all the prerequisites and installed them on the right version! i also try faq, but not successful! i've done so far.. 1. installed prerequisites and cmake 2. through cmake-gui i configurate and generate makefile 3. breaks on the last part of tutorial - when typed make -j`nproc` executed command (if any) make -j`nproc` openpose output (if any) cmake error: the following variables are used in this project, but they are set to notfound. please set them or make sure they are set and tested correctly in the cmake files: cudalibrary (advanced) cudalibrary (advanced) -- configuring incomplete, errors occurred! see also ""/data/openpose/build/caffe/src/openposelib-build/cmakefiles/cmakeerror.log"". cmakefiles/openposelib-stamp/openposelib-stamp/openposelib.dir/all' failed make[1]: **** [all] error 2 type of issue - installation process your system configuration openpose -> latast cuda -> 8 cmake -> 3.12.2 ubuntu 16.04",question,question
504,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/504,All examples in tutorial_wrapper segfault in Ubuntu 16.04,"posting rules 1. **: compiled from source? if so, 3.4.1 compiler (`gcc --version` in ubuntu): 5.4.0",question,Error
777,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/777,how can i use mpi model??,posting rules 1. ** issue:,question,question
125,https://github.com/dusty-nv/jetson-inference/issues/125,Allocated memory for new image of the camera,"hi. i need to convert the image coming from the usb camera (flip it to 180). for this i did the following: example.h `bool example(void* output, int width, int height, uint32_t bytesize);` example.cu as a result, i get an inverted image from the camera, but the further use of this image in the detection function is impossible. am i wrong in allocating memory for this image? if you use a pointer to convert to cv::mat the image is displayed inverted and the light is on. how to allocate memory for using this image in further chunks of code?",question,question
984,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/984,How to enable the 3D reconstruction from the calibrated stereo video or images sources,"with the pre-calibrated stereo cameras .xml files and corresponding videos captured by these cameras, is there any flag command for the openpose.bin to support 3d reconstruction from video source other than the flir-camera? i tried adding the --3d options but the openpose.bin seemed not to be able to find either the video sources or the .xml files.",question,question
737,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/737,How to extract BODY_18 key points using python api?,"i am using python api to extract pose key points, and the 'mod.forward' function return 25 body key points. how to get the body_18 key points?",question,question
464,https://github.com/dusty-nv/jetson-inference/issues/464,DetectNet COCO-Dog Train Example doesnt draws all bounding boxes,"hello, i'm new in digits and deep learning.i have learned detectnet to detect dogs according jetson-inference guide and then i saw that it doesn't find all bounding boxes on test images with few dogs. but as i can see from coverage and bbox visualization it can do that (look at pictures). what i'm doing wrong? probably i should change some threshold, but where?",question,question
1167,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1167,question about openpose python API,there is a question that i want to read image from a variable rather than from the images in the floder. however i don't see examples in your examples. could you help me to slove these problems.,question,question
1001,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1001,can not open BOOST_SYSTEM_LIB_RELEASE.lib when installing Python API,"system config: gpu : nvidia gtx 1060 3gb cpu : intel i7 ram: 8 gb issue summary when i am trying to compile the python api , i met such problem: i am trying to click the buildsystemrelease nofound"",etc so, i turn the they into ""boostlibfilesystemdebug"",""boostlibfsystemdebug"" and now ,i can config it can generate it . then , i open the project using vs2017 of openpose.sln , when i try to build it , it raised ""can not open 'boostlibproject/oldopen/openpose/buildpyt/python/openpose/openpose.py"", line 27, in openpose file ""d:\anaconda\envs\imgclassify\lib\site-packages\numpy\ctypeslib.py"", line 155, in loadproject/oldopen/openpose/buildpyt/python/openpose/openpose.py"", line 17, in file ""e:/githublibrary oserror: no file with expected extension """""" someone can tell me why when all the flag like ""boostlibpython on??? executed command (if any) openpose output (if any) errors (if any) lnk118 unable open file ""..\..\src\openpose\release\openpose.lib"" type of issue - compilation/installation error -execution error your system configuration 1. ** issue:",question,question
725,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/725,writing images and json failed - GUI gtk error,my environment: os: ubuntu 16.04 gpu: nvidia tesla m60 cuda 8 cudnn 5.1 opencv latest caffe i built openpose in ubuntu 16.04 on aws g3.4xlarge instance without gui and tried to test it by this command: ./build/examples/openpose/openpose.bin --imageimages /data/openpose/build/examples/output --write_json /data/openpose/build/examples/output sure the image directory has a jpeg image file for testing. the only output is this message: (openpose 1.3.0:2004): gtk-warning **: cannot open display: it didn't actually output anything to the destination path. do i really need gui to get the output function work? is it possible to get it output something without gui in ubuntu?,question,question
533,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/533,installation error,/usr/lib/gcc/x8664-linux-gnu/crt1.o: in function `_start': (.text+0x20): undefined reference to `main' collect2: error: ld returned 1 exit status examples/openpose/cmakefiles/openpose.bin.dir/build.make:149: recipe for target 'examples/openpose/openpose.bin' failed make[2]: ***** waiting for unfinished jobs.... someone's help is appreciable,question,question
195,https://github.com/dusty-nv/jetson-inference/issues/195,Overlapping Bounding Boxes Are Combined - Can This be Disabled?,"hello all, i've trained a detectnet model with ""subtract mean"" = ""none"" on digits 6.0 with nvidia caffe 0.15.14. so far, following the directions to deploy this to jetson, everything has been going as expected (well other than it taking 8-9 minutes to build the cuda engine the first time). however, i'm seeing very different results from detect-console than the digits inference server on the same test image. i've noticed many other similar issues that trace back to mean image vs pixel subtraction or double subtraction. this might still be an issue, but the behavior i'm seeing is different. in our case, it looks like all of the bounding boxes that would have overlapped with each other are being combined into one box (whereas they're not on digits). i have to assume this is related to some difference between the python clustering layer deleted from the end of deploy.prototext and the equivalent function implemented in detectnet.cpp. is there a clean way to modify detectnet.cpp to get identical results to the digits inference tool? thanks for your help! / r",Performance,question
479,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/479,Where can I find openpose.bin file?- Unable to start,"i want to"" compile the library and use the demo ./build/examples/openpose/openpose.bin."" can you help me how to start for this. i am using matlab. i have caffe installed and working over windows 10. i couldn't find openpose.bin file from the latest binaries i downloaded. kindly direct me how to proceed. thanks",question,question
148,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/148,libcaffe.so.1.0.0: cannot open,thanks for you nice work. but now i have some questions. i have build caffe and openpose successfully on ubuntu16.04 with cuda8 but when i step into quick start and run ./openpose.bin --help i get the error: `error while loading shared libraries: libcaffe.so.1.0.0: cannot open shared object file: no such file or directory` so do you know what's wrong with this.i would appreciate it if you can help me.,question,question
1813,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1813,installation screenshots not available anymore,could you please make the installation screenshots in available again? here's an example that has 404 error if you also click on images:,question,question
308,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/308,Windows portable demo run problem,"issue summary can't get windows standalone demo to run executed command (if any) clicking on openposedemo.exe openpose output (if any) gui window opens (but is blank), other window shows error: ""f1108 16:50:31.670817 388 io.cpp:41] checkfailed: fd != -1 (-1 vs. -1) file not found: models/pose/coco/poselinevec.prototxt ****: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source: 2.4.9, 2.4.12, 3.1, 3.2, ... compiler (`gcc --version` in ubuntu):",question,question
39,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/39,the process is  killed,the command: ./build/examples/openpose/rtpose.bin --imageimages output/ --modeldir examples/media/ --writepose coco starting pose estimation demo. starting thread(s) killed how to solve the problem? by the way: opencv is 2.4.13 gpu is tx1,question,question
654,https://github.com/dusty-nv/jetson-inference/issues/654,Reproducing detection results,"hi nvidia devs i wanted to reproduce results of repo but in master branch i can not find the detectnet_camera.py script, what is the situation? i had to clone depth branch, it has the scripts but when i run i get an error like :",question,question
143,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/143,Ubuntu: Name is blank in the datum.name,"issue summary i have modified the 1asynchronous.cpp to display the image name(filename) with std::cout at(0).name >& datumsptr) you might select multiple topics, delete the rest: - question your system configuration **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x.2.x compiler (`gcc --version` on ubuntu):",question,Error
536,https://github.com/dusty-nv/jetson-inference/issues/536,"Detect successfully, but get the wrong ClassID","hi, nice to meet you. i run detectnet successfully and get the correct bounding bow show on gldisplay(), but when i add `for detection in detections` ` print(detection.classid)` it always shows '1' or '72' actually, it show 'person' on gldisplay() windows it is not match with the number in /data/networks/ssdlabels.txt",Error,Error
371,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/371,Can your approach be used to extract motion capture data from a single video or just 2d pixel coordinates?,i read and noticed that the output has no z component. can your approach be used to extract motion capture data from a single video or just 2d pixel coordinates?,question,question
1538,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1538,"where to add number_people_max parameter for c++ api, like PoseExtractorCaffe?",posting rules 1. ** issue:,question,other
266,https://github.com/dusty-nv/jetson-inference/issues/266,Qustion: about Jetson Xavier port,any plan to update the examples to support jetson xavier?,other,other
682,https://github.com/dusty-nv/jetson-inference/issues/682,Segmentation fault (core dumped) on importing jetson-inference,"dear dusty, i have a problem when i am importing jetson-inference. i flashed a 32gb card around 1 month ago for my jetson nano device. i then realised i am running out of space and upgraded to a 128gb. i installed all of the packages again from scratch, but then found that jetson inference is broken. it wasn't too happy when compiling too, telling me about a load of syntax errors... any help would be much appreciated.",question,question
1248,https://github.com/dusty-nv/jetson-inference/issues/1248,Error on Detection Model on Xavier nx Using DLA ,hello @dusty-nv thanks for this great tutorial. i followed tour tutorial and train the model for detection on custom data-set and saved it as `ssd-mobilenet.onnx`. it run perfectly and give the inference using `detectnet.py` as you explained. but the problem occur when i try to run it using `dla` on `jetson xavier nx`. ** 1) how can we ran the model only on `dla for inference on xavier nx`? if there is any procedure with some script that i can follow. 2) how to resolve the error for `trtexec` so that it can give the banchmark. 3) what is whole procedure that i can run the one model on dla and other on gpu concurrently. thanks in advance for your help,question,question
141,https://github.com/dusty-nv/jetson-inference/issues/141,"DetectNet crashing jetson TX1 on jepack 2.3.1, 3.0, 3.1","dear all, imagenet is working fine tough. `./detectnet-console drone0427.png result.png coco-airplane detectnet-console args (4): 0 [./detectnet-console] 1 [droneiteriter_22500.caffemodel [gie] retrieved output tensor 'coverage' [gie] retrieved output tensor 'bboxes' [gie] configuring cuda engine [gie] building cuda engine` anyone can confirm this ? i have tried 4 different modules",deployment,question
1298,https://github.com/dusty-nv/jetson-inference/issues/1298,"make error ""/usr/bin/ld: cannot find -lnvcaffe_parser""",[ 57%] building cxx object cmakefiles/jetson-inference.dir/plugins/pose/trtall.cpp.o [ 57%] linking cxx shared library aarch64/lib/libjetson-inference.so /usr/bin/ld: cannot find -lnvcaffe_parser collect2: error: ld returned 1 exit status cmakefiles/jetson-inference.dir/build.make:474: recipe for target 'aarch64/lib/libjetson-inference.so' failed make[2]: ***** [all] error 2 i'm using a jetson-nano 2gb and following this tutorial: the error occurs when i input the make command in /jetson-inference/build may i ask how to solve this error? thanks.,question,question
137,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/137,Openpose without cuda?,can i build openpose's lib without cuda?. (my computer not support cuda),question,question
758,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/758,Resading from Video or still images,"hi, i was wondering if there be difference between giving the whole video to the open pose and giving the each frame separately as an still image to the open pose?",question,question
804,https://github.com/dusty-nv/jetson-inference/issues/804,How to upgrade the jetson-inference?,"i installed the jetson-inference in jan 2019 and it is great. i have created built mysql, node js around it. and i saw there is a new version of jetson-inference with custom object detection. i assume i can rename the old jetson-inference folder to something else, and install the new jetson-inference version, right? thanks.",question,question
1689,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1689,What is the gcc version that needs openpose?,issue summary i am trying to install openpose for the first time and finally when i run `make -j16` i get the error `unsupported gnu version! gcc versions later than 7 are not supported!` i have tried to follow the recommendation given by @christian-lanius but it has not been possible to install older versions on my pc (i use ubuntu 19) executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. errors (if any) type of issue compilation/installation error your system configuration 1. ** issue:,other,question
131,https://github.com/dusty-nv/jetson-inference/issues/131,Graphical user interface development for jetson-inference,"hello there, i created detectnet as a qt project. i did the necessary library additions and then started. i get an error like this: : error: cannot find -lcudart : error: collect2: error: ld returned 1 exit status where could i have made a mistake. thank you...",question,question
997,https://github.com/dusty-nv/jetson-inference/issues/997,HOW to convert .onnx file to tensorRT engine file using [jetson-inference]?,the engine file may be used in the deepstream applications. thanks.,question,question
1150,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1150,make file fails,"after successfully generating and running make file i get the following errors: *all prerequisites are installed i try on macos mojave, and my macbook does not have a gpu",question,question
1089,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1089,Do you have pyopenpose documentation？,"i want to know more about pyopenpose, such as what methods and parameters it has and what the return value means, but i haven't found any other documentation except tutorialpython.do you have a complete documentation of pyopenpose?",other,other
986,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/986,"Build issue / cpu mode / ubuntu desktop 16.04 ./ When input make -j`nproc`,appearing these.","[ 12%] performing configure step for 'openposeboostdependencies) /usr/share/cmake-3.5/modules/findboost.cmake:1332 (missingpackage) cmakelists.txt:55 (include) imported targets not available for boost version call stack (most recent call first): /usr/share/cmake-3.5/modules/findboost.cmake:763 (componentboostdependencies) cmake/dependencies.cmake:5 (findboostdependencies) /usr/share/cmake-3.5/modules/findboost.cmake:1332 (missingpackage) cmakelists.txt:55 (include) imported targets not available for boost version call stack (most recent call first): /usr/share/cmake-3.5/modules/findboost.cmake:763 (componentboostdependencies) cmake/dependencies.cmake:5 (findincludedir to the directory containing boost's headers. call stack (most recent call first): cmake/dependencies.cmake:5 (find64-linux-gnu/libglog.so) -- found protobuf compiler: /usr/bin/protoc -- -- cuda is disabled. building without it... -- machine learning scaling library (mlsl) found (/home/hu/桌面/openpose/3rdparty/caffe/./external/mlsl/l2018.1.005//intel64) -- forward overlapping optimization is enabled! weight grad compression is disabled because intel compiler is not found -- found atlas (include: /usr/include, library: /usr/lib/libatlas.so) -- mkldnn download is enabled by customized setting! -- mkldnn will be downloaded from github and installed in /home/hu/桌面/openpose/build/caffe -- mkldnn include directory: /home/hu/桌面/openpose/build/caffe/include -- mkldnn engine will be used as a default engine -- python interface is disabled or not all required dependencies found. building without it... -- -- *************************************** [all] error 2",other,question
1148,https://github.com/dusty-nv/jetson-inference/issues/1148,imageNet.classify() encountered an error classifying the image,"hi @dusty-nv , i followed and have no problems. in terminal, i run command line `imagenet --model=modelblob=inputblob=outputpath/labels.txt imgcoding your own image recognition program (python)_, i write python code like below: the [trt] output in terminal indicates it loads model and label successfully, but finally it throws out error: or, what's the suggested way to load a imagenet with named network type and model path?",question,question
29,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/29,Can this model tracking people or hand movement,i want to use this to make a gesture demo to control application or other iot. does it possible?,other,question
1157,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1157,Python API: ImportError pyopenpose ,"issue summary i am not able to import openpose even after successful compiling. 64-linux-gnu.so_ is getting generated in `/usr/local/python/openpose/`, but python does not recognise pyopenpose executed command `import openpose` errors importerror: cannot import name 'pyopenpose' from 'openpose' type of issue - execution error your system configuration 1. ** api:",question,question
1380,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1380,"How to get x,y of Face Output Format","thank you for creating this and sharing with everyone. issue summary i have no problems running any code. i have searched everywhere and are unable to find an answer. hence my post for assistance. in the face ouput format of, i would like to get the x, y coordinates (in 2d) of each keypoint for a face. thank you for your assistance. executed command (if any) i have tried to get this output from the python 04, 06, 08, 09 samples. openpose output (if any) i do get ouput but none of it makes sense in relation of my end goal of getting all the x, y coordinates of the face keypoints in 2d errors (if any) none type of issue - help wanted your system configuration 1. windows 10 python 3.7.3 in a vituralenv open pose 1.5.0 nvidia 1080 pycharm ide 2. ** system:",question,question
1296,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1296,Zero padding for large net-resolution?,"if net-resolution is higher than image resolution, which do you apply ? resize or zero padding?",other,question
349,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/349,Best way to train OpenPose on custom data?,we've got some data that's more suited to our use case and would like to train openpose on it. is the multi person pose estimation repo ( still the best starting point to train on custom data? thanks.,other,other
671,https://github.com/dusty-nv/jetson-inference/issues/671,Training of my own classification model with very bad results (100 epochs),"hi, i trained my own dataset following the tutorial ""re-training on the plantclef dataset"". my dataset consists of 10 classes of f1 cars per team. so i trained following the steps of the tutorial, with 100 epochs and batch size of 16. i used resnet50 and then resnet 18. with the onnx model i tested on jetson and the results are bad. the classifier only hits me only one class of 10, otherwise it changes everything, for example saying that a ferrari is a mercedes and etc ... in the images i attached, he only hit the toro_rosso, and it seems to me that it is the only class that he hits right. did the problem have been training with 100 epochs, is more needed?",question,question
1173,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1173,How to use openpose-python api to detect the body keypoints twice in demo.py ?,"*.py. however, something went wrong during the running. one for detecting the position of the target (then crop the target from the whole picture and resize it to a uniform size), one for detecting the keypoints of the target in the new resized-image . my issue is similar to [ to some degree. is that ok to use openpose to detect keypoints twice in a program? is there any more documents to refer to? i don't know how to use the pyopenpose rightly. could you give me some advice? thank you very much!!! when the cycle index is bigger than one, the program will stuck and throw a error exception. this error may has something with the gpu. i don't know how to handle with it. need your help. ** if (max(datum_new.posekeypoints[0,j,:])==0): indexerror: too many indices for array",question,question
485,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/485,configuration openpisein a new project,"i want to use openpose in my own project,but i can not configure it right. could u please tell me except the include ,the libpath and the addition include,what else should i add to the property configuration.thank you.",other,question
1391,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1391,Problem with the installing python api,"hi there, i am already followed the readme file and i'm trying to use openpose python api. i have successfully generated the code by using cmake and the vs2017(release/x64). however, i could not find neither pyopenpose.cp36-win_amd64.pyd nor pyopenpose.pyd (and there was no such folder in dir ../../python/openpose/release). i tried to delete the original openpose and cmake it again and still no release file in /python/openpose/ did i do something wrong??",question,question
1658,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1658,What's the correct way to run the Open Pose binary (OpenPoseDemo.exe) from within a Python script?,"i am making a body tracking application where i want to run open pose if the user chooses to track their body movements. as we all know, the openpose binary file can be run like so: `bin\openposedemo.exe --write_json 'path\to\dump\output'` so, in my python script, i want to have a line of code that would run open pose, instead of having to ask the user to manually run openpose by opening a separate command line window. for that, i have tried: which i guess means that `openposedemo.exe` can be opened only by going inside the `openpose` directory where the `bin` subdirectory resides. so, if there's anybody here who made it work, it would be awesome if you could show how. thanks in advance!",question,question
295,https://github.com/dusty-nv/jetson-inference/issues/295,Missing Super-Resolution-BSD500,is there a way to get `network/super-resolution-bsd500/superbsd500.onnx`?,other,question
910,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/910,Mac build error: Undefined symbols for architecture x86_64:,"issue summary when building with `make` and the 3-d reconstruction module enabled the following error appears. if i disable the 3-d reconstruction module in cmake gui everything works fine. executed command (if any) openpose output (if any) errors (if any) type of issue you might select multiple topics, delete the rest: - compilation/installation error your system configuration 1. ** issue:",deployment,question
510,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/510,Openpose cannot detect people and model_folder option not work,"issue summary i am trying the cpugpu 0 ./build/examples/openpose/openpose.bin --video examples/media/video.avi --numfolder ""models/"" ./build/examples/openpose/openpose.bin --video examples/media/video.avi --numfolder ""wrongrelease -a` in ubuntu): no lsb modules are available. distributor id: ubuntu description: ubuntu 16.04.4 lts release: 16.04 codename: xenial **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? pre-compiled `apt-get install libopencv-dev` compiler (`gcc --version` in ubuntu): gcc (ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609",question,question
15,https://github.com/dusty-nv/jetson-inference/issues/15,Jetson TK1 inference,"hi, not really sure if this is the best place to ask this but is there any change to use this repository on a tk1? more generally, is there any change to deploy detectnet on a tk1? thanks in advance",deployment,question
1785,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1785,Low FPS on RTX3080,"issue summary hi, i just tested the rtx 3080 with the openpose demo and only got 8 fps. can you tell me please what could be the reason for such low value and what maximum value should i get with this card? executed command `./build/examples/openpose/openpose.bin --video examples/media/video.avi --profileexpressions.cpp (1334) assign opencv/matexpr: processing of multi-channel arrays might be changed in the future: 4.456380 msec in /home/dradam/openpose-master/include/openpose/producer/wdatumproducer.hpp:workproducer():79 0.002704 msec in /home/dradam/openpose-master/include/openpose/thread/widgenerator.hpp:work():76 0.006478 msec in /home/dradam/openpose-master/include/openpose/core/wscaleandsizeextractor.hpp:work():74 5.387900 msec in /home/dradam/openpose-master/include/openpose/core/wcvmattoopinput.hpp:work():70 3.553041 msec in /home/dradam/openpose-master/include/openpose/core/wcvmattoopoutput.hpp:work():73 119.910851 msec in /home/dradam/openpose-master/include/openpose/pose/wposeextractor.hpp:work():98 0.030387 msec in /home/dradam/openpose-master/include/openpose/pose/wposerenderer.hpp:work():79 6.974877 msec in /home/dradam/openpose-master/include/openpose/core/wopoutputtocvmat.hpp:work():69 1.449959 msec in /home/dradam/openpose-master/include/openpose/gui/wguiinfoadder.hpp:work():73 5.912661 msec in /home/dradam/openpose-master/include/openpose/gui/wgui.hpp:workconsumer():87 openpose demo successfully finished. total time: 39.442333 seconds. type of issue - question your system configuration 1. * 1000 + cudnnpatchlevel) #endif // - ** issue: only approximately 8 fps on demo code",Performance,question
610,https://github.com/dusty-nv/jetson-inference/issues/610,Build error when using make in official nvidia docker,"i'm trying to build this project from source in a docker container (i'm using a clean nvcr.io/nvidia/l4t-tensorflow:r32.4.2-tf1.15-py3 image). however, the make command gives the following error: > scanning dependencies of target jetson-inference [ 60%] building cxx object cmakefiles/jetson-inference.dir/c/detectnet.cpp.o [ 61%] building cxx object cmakefiles/jetson-inference.dir/c/homographynet.cpp.o [ 62%] building cxx object cmakefiles/jetson-inference.dir/c/imagenet.cpp.o [ 63%] building cxx object cmakefiles/jetson-inference.dir/c/segnet.cpp.o [ 64%] building cxx object cmakefiles/jetson-inference.dir/calibration/randint8calibrator.cpp.o [ 65%] building cxx object cmakefiles/jetson-inference.dir/c/superresnet.cpp.o [ 67%] building cxx object cmakefiles/jetson-inference.dir/c/tensornet.cpp.o [ 68%] building cxx object cmakefiles/jetson-inference.dir/plugins/flattenconcat.cpp.o [ 69%] linking cxx shared library aarch64/lib/libjetson-inference.so /usr/bin/ld: cannot find -lnvcaffe_parser collect2: error: ld returned 1 exit status cmakefiles/jetson-inference.dir/build.make:335: recipe for target 'aarch64/lib/libjetson-inference.so' failed make[2]: ***** [all] error 2",question,question
1149,https://github.com/dusty-nv/jetson-inference/issues/1149,Easiest way to convert inference resolution arbitrarily for SSD-Mobilenet-v2?,title.,question,other
1628,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1628,openpose detection sequence for persons in a picture or frame,"hi, i have implemented the openpose demo and found it really wonderful. i have a question when detecting more than one person in a picture or cameras: ""what's the sequence of detection for persons? if there are 10 persons in a picture, does it detect the leftmost person and then from left to right or with other sequence? "" thanks.",question,question
1415,https://github.com/dusty-nv/jetson-inference/issues/1415,cudaEventRecord operation not supported,"i am getting a lot of in my tx2 with jetpack 3.2, but it looks like cuda 9 does have the function. any ideas? we are trying to backport the profiling to trt 3. ping @emmanuel-messulam",question,Error
818,https://github.com/dusty-nv/jetson-inference/issues/818,Training Model - Error With Space and Memory,"hi dusty, i'm working on training my own model and i am getting this error. i am wondering if you would help? df",question,question
1451,https://github.com/dusty-nv/jetson-inference/issues/1451,No easy way to take pictures ?,"to collect my own dataset, it's actually too time-taking, is there any other option to take pics and label them using my phone and laptop instead of the nano ?",other,question
519,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/519,Compilation error on Ubuntu 17.10 with both gcc 5 and 6,"greetings, issue summary i tried compiling openpose with 2 versions of gcc, using cmake with the default cmakelists.txt. using gcc 6.4 outputs errors such as : the whole output being available here: using gcc 5.4.1 outputs : which can be fixed by adding `-std=c++11` to cmakeflags and cmakelinker_flags, but then it causes : and the same error for other .cu.o.cmake files : how can i solve this problem ? system configuration **: gcc 5.4.1, 6.4.0 type of issue - compilation/installation error",question,question
1844,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1844,Regenerate cMakeList.txt files after adding new code and files ?,"hi guys, i'm working on a project to generate 3dpost from a sequence of images and from a video file. after adding my *.cpp files, and try to execute cmake... an error message appears ""no cmakelist.txt"" file exists. what is the file and how can i generate it. thanks",question,question
1462,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1462,Using JSON output in R ,"hello, i am interested in analyzing changes in a single (chest) keypoint location in r. would you explain how to (1) isolate a single keypoint while writing out the json file, and (2) if it is possible to write json as a single file rather than dozens. thanks",question,question
1364,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1364,Which version of the paper (2016 OR 2018) is the current OpenPose repository configured with ?,"there are a couple of differences between the older and newer version of the network described in the most recent openpose paper after looking at the neural network defined in openpose/models/pose/coco/poselinevec.prototxt , it seems like the current repository is still configured to network architecture defined in the old paper. just want to confirm my understanding.",other,other
951,https://github.com/dusty-nv/jetson-inference/issues/951,Re-training SSD-Mobilenet,"i followed the steps of the youtube tutorial to execute the command, but i got this issue, who can help me? `traceback (most recent call last): file ""traindataset.py"", line 33, in __ ` (jetson nano developer kit)",question,question
135,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/135,"Jetson TX 1 demo, Error: Videocapture (webcam) could not be opened","platform: jetson tx1 os : ubuntu 16.04 opencv : 2.4.13 camera : csi cuda 8.0 i have gone with default installation. there were minor issues in lhdf5 which were resolved. after completing the setup, all models were downloaded as well. but when i run the demo, i get an error. i have tried with options, --camera 0 to --camera 9. i realize jetson tx1 is not officially supported yet. if someone has inputs on how to get openpose to detect camera, that will be great. [update] it appears the problem may have something to do with opencv.",question,question
361,https://github.com/dusty-nv/jetson-inference/issues/361,rgba to bgr conversion giving distorted image,"in an attempt to convert to bgr for further processing with opencv i wrote this code cv::mat converted; std::memcpy(im.data, imgcpu, imgwidthsizeof(float)); cv::cvtcolor(im, converted, cv_rgba2bgr); cv::imwrite(""result.jpg"",im); cv::imwrite(""result2.jpg"",converted); std::cout << ""image saved !"" << std::endl; unfortunetly only the top part of the image is visible the rest appears black. help please what am i doing wrong.",question,question
1603,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1603,openpose cannot detect people,"my os is ubuntu16.04 cuda10 cudnn 7.6 i compile opencv3.4 and caffe, cmake -dopencvincludegit/caffe/buildlibsgit/caffe/buildlibs=/home/guiju/workspace/openpose/openpose/3rdparty/caffecpu/install/lib/libcaffe.so -dbuild_caffe=off .. but when i use './build/examples/openpose/openpose.bin --video examples/media/video.avi', it cannot detect people whatever video file",question,question
545,https://github.com/dusty-nv/jetson-inference/issues/545,custom model object detection,"hi, in the tutorial hello ai world it's possible to use pre-trained models. let's say i want to train ssd-mobilenet-v2 with specifics classes, how would it be possible to use the project jetson-inference to use my new model? sincerely,",question,question
375,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/375,CPU version - Running Without CUDA,what if one has no nvidia card and want to run code without cuda?,other,question
1190,https://github.com/dusty-nv/jetson-inference/issues/1190,data augmentation transforms: RandomResizedCrop not available,"hi dusty, i'm trying to do data augmentation using pytorch's randomresizedcrop. however it does not appear to be included in the ssd vision transforms used in jetson-inference. i was wondering why the data augmentation transforms offered in the jetson-inference torchvision vs. are not the same? thanks for your help!",other,question
965,https://github.com/dusty-nv/jetson-inference/issues/965,mobilenet v3 object detection model?,any plans to implement mobilenet v3 for object detection?,other,other
1159,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1159,Request for a less restriction license,"hi, thanks for making such a cool project. do you think is it possible to change the license to less restrictions in the near future please? such as allow the outcome from openpose to be reused for commercial purpose? i am thinking integrate openpose to my if you allow the outcome, i mean the generated pose data, to be used in gamedev or other purpose that would be fantastic. thanks!",other,other
423,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/423,Ubuntu Cmake-gui error while getting default Caffe,"issue summary this should not be a caffe problem per se, but before i install custom caffe i would like to make sure. i followed all the steps in the up until using the cmake-gui. there i ran into an error while downloading caffe. it seems to me cmake-gui does not download caffe at all. i tried to wipe everything and try to install openpose again, but received the same mistake. i also tried to see if cmake follows the ifs in the cmakelists.txt correctly and reaches the branches where he establishes that caffe needs to be downloaded and it seems to me it does so. cmake-gui returns the following error: executed command (if any) cmake-gui according to installation guide. openpose output (if any) none. type of issue you might select multiple topics, delete the rest: - compilation/installation error your system configuration **: compiler (`gcc --version` in ubuntu): `5.4.0 20160609`",question,question
437,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/437,The CXX compiler identification is unknown CMake Error at CMakeLists.txt:10 (project):   No CMAKE_CXX_COMPILER could be found.,"i am getting this error while building openpose in ubuntu 16.04 lts with gcc and g++ version 5.4.1. how can i resolve this error?? this is the error , i am getting *****",question,question
1245,https://github.com/dusty-nv/jetson-inference/issues/1245,Inference problem in SSD-MobileNetV2,"i coded a simple application as given in your video example but the terminal keeps outputting some insane output that i am unable to comprehend, given below. the camera works and i am able to use it in other applications",question,other
105,https://github.com/dusty-nv/jetson-inference/issues/105,About QMutex ,i am tracing imagenet-camera.cpp i have some problem on gstcamera-> capture function i have no idea why using qmutex to lock in my opinion that imagenet-camera.cpp only have main thread why calling camera's capture function need to consider synchronize issue thanks a lot,question,question
1251,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1251,How is MPI implemented compared to COCO(Body_25)?,"type of issue - question is the mpi skeleton model implemented using the same coco/body_25 procedure with different keypoint locations, or is ti implemented using a different mechanism?",question,question
776,https://github.com/dusty-nv/jetson-inference/issues/776,How to make a Directory to train SSD-based Object Detection in PyTorch,"hi, i have trained the model using the camera capture tool it was brilliant. now i am willing to train this model using my own data set which has about 100 pictures of me not annotated yet, 1. i am unable to figure first how to annotate them? 2. how to make a directory structure as camera capture makes automatically to run this model? if you could make a video it would be really helpful.",question,question
873,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/873,Openpose models not available due to wrong url in getModels.bat,posting rules 1. ** issue:,Error,Error
729,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/729,Standalone Hand Keypoint Detection - hand rectangle format,"issue summary i am trying to build a standalone hand keypoint detector. as stated in the docs and explained by @gineshidalgo99 in other issues, i used only the handextractor package from the c++ api with the output of my hand detector which places squared bounding boxes around hands. i read in the code that op::rectangle is supposed to be like cv::rect. does this mean we are supposed to pass in coordinates for the top left part of the image along with the width and height? (which is what i did) executed command (if any) i am more than happy to post my code here in case further clarification is required, but in order to reduce clutter as of right now, i will keep it concise. i used the c++ api and looked through the code on #343. the following are the packages i used: - - ****: 1.3.0",question,question
305,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/305,Compilation/installation error - CMake,"issue summary i was working through the installation instructions here: i got to the ""openpose building"" section successfully and without errors executed command (if any) make -j `nproc` openpose output (if any) [ 0%] performing configure step for 'openposethread [ 0%] built target openposetime -- atomic -- found gflags (include: /usr/include, library: /usr/lib/x8664-linux-gnu/libglog.so) -- found protobuf compiler: /usr/bin/protoc -- found lmdb (include: /usr/include, library: /usr/lib/x8664-linux-gnu/libleveldb.so) -- found snappy (include: /usr/include, library: /usr/lib/x8620 sm30 sm50 sm61 -- opencv found (/usr/share/opencv) cmake error at /usr/share/cmake-3.5/modules/findpackagehandlestandardargs.cmake:148 (message): could not find atlas (missing: atlasincludeclapackdir atlaslibrary atlaslibrary atlaslibrary) call stack (most recent call first): /usr/share/cmake-3.5/modules/findpackagehandlestandardargs.cmake:388 (failurepackagestandardpackage) cmakelists.txt:46 (include) see also ""/home/jon/openpose/build/caffe/src/openposecaffe-build/cmakefiles/cmakeerror.log"". cmakefiles/openposecaffe-stamp/openposecaffe-stamp/openposecaffe.dir/all' failed make[1]: ****: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source: 2.4.9, 2.4.12, 3.1, 3.2, ... compiler (`gcc --version` in ubuntu): gcc (ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609",deployment,question
150,https://github.com/dusty-nv/jetson-inference/issues/150,"make error, ‘NV_TENSORRT_MAJOR’ was not declared in this scope","hi @dusty-nv i finish ""running jetpack on the host"" step, and then i try to build the source on jetson tx2. but i met such error when make: is it because something wrong with my tensorrt? i notice that variables are from ""nvinfer.h"", i find that file, but there's no nvmajor or nvminor in that file, so my tensorrt version is out of date?",Error,question
1522,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1522,3D/2D Kinematic Analysis,"hello, i am in the procedure of devising my master's thesis and i am trying to avoid using complicated and expensive equipment for motion analysis. so i would like to ask if it is possible to extract the movement of the skeletons during the video capture. i would appreciate any response, since it is a very serious matter.",question,other
457,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/457,A problem about CMakeLists.txt while installing,"openstack@ubuntu:~/openpose$ make -- gcc detected, adding compile flags cmake warning at cmakelists.txt:105 (findmoduledir"" to a directory containing one of the above files. if ""opencl"" provides a separate development package or sdk, be sure it has been installed. -- cuda detected: 8.0 -- found cudnn: ver. 5.1.10 found (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so) -- automatic gpu detection failed. building for all known architectures. -- added cuda nvcc flags for: sm21 sm35 sm52 sm61 -- found cudnn: ver. 5.1.10 found (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so) -- found gflags (include: /usr/include, library: /usr/lib/x8664-linux-gnu/libglog.so) -- caffe will be downloaded from source now. note: this process might take several minutes depending -- caffe has already been downloaded. -- caffe will be built from source now. cmake error at src/openpose/cmakelists.txt:4 (addtargetcmakefiles -- adding example 1postextractimage.bin -- adding example 2poseheatmatimage.bin -- adding example 1readdisplay.bin -- adding example 2processinguserprocessingoutput.bin -- adding example 4inputoutputdatum.bin -- adding example 1asynchronoususeruserchecksystem]",deployment,question
112,https://github.com/dusty-nv/jetson-inference/issues/112,How to enable tensorflow for DIGITS,"hi dusty, i installed digits first and then tensorflowx86_64. after that, when i run digits-devserver, it says ""tensorflow support disabled"". after i open the website for digits, i cannot find the ""tensorflow"" option in the custom model part. what should i do to enable tensorflow support?",question,question
201,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/201,Fail in import OpenPose to Unity,"issue summary i'm trying to use openpose in unity. i compile the ""extract from image"" tutorial into a dll through vs2015 and import to unity. the unity crashes in the caffe part. if i comment out the caffe part and make a new dll, everything works fine. thank you very much openpose output > unhandled exception at 0x00007ff9f44e34be (ucrtbase.dll) in unity.exe: fatal program exit requested. break in `netcaffe.cpp` `upcaffenet.reset(new caffe::net{mcaffeproto, caffe::test});` type of issue - help wanted your system configuration **: installed with `apt-get install libopencv-dev` (ubuntu) or default from openpose (windows) or opencv 2.x or opencv 3.x.",question,question
775,https://github.com/dusty-nv/jetson-inference/issues/775,How do I program to flip a camera in the DetectNet Python program? The video is flipped over.,"i'm referring to the code above. the camera video is flipped over, how can i change the code? thank you",question,question
653,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/653,What parameters should be changed if the net output resolution is changed?,"hi @gineshidalgo99, i would like to ask that what parameters should be changed if the network output resolution is changed. i keep the network input resolution to 656x368, but the output resolution is increased to 164x92 from 82x46. when i run the demo video, i can see multiple detected poses, but the poses are rendered at the positions below the correct people locations. it seems like the joint coordinate conversion has some problem. i notice the original matlab or python test version has an image padding step and a stride parameter. is there a similar step and parameter on openpose? how can i get the correct results? thanks.",question,question
1325,https://github.com/dusty-nv/jetson-inference/issues/1325,How to get the http video stream instead of rtsp?,"hi, i can get the rtsp stream from a ip camera but there is lots of lagging. (several seconds) i can use opencv to capture the http video stream and the lagging is small is there anyway i can capture the http stream in your code? thx note: i don't understand is that why my typical nvr (network video recorder) does not have lagging, how it's protocol work? just http? can i do the same thing in nano? plan a: reduce the lagging in rtsp stream. i have added the --input-rtsp-latency = 0, it still has 4 seconds delay for 1920x1080 (h264) plan b: use opencv to capture the http, then convert it using this example but mjpeg only has max 704x576 in the ip camera sub-stream... thanks.",Performance,question
96,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/96,Does it work on Mac OS?,posting rules 1. **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. compiler (`gcc --version` on ubuntu):,question,other
163,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/163,Docs on using caffe model standalone,"hey! i'm trying to use caffe models provided in repo without the lib itself. however, it feels like it is not encouraged to do so, because there is no documentation. what i did was that changed input dims of `hand deploy.prototxt` file to this: and then launched inference. i got an output which is a multi-dim array of this dims: `1x1x22x46x46` however, i don't know how to interpet this data. i would appreciate any help or hints.",question,question
554,https://github.com/dusty-nv/jetson-inference/issues/554,Unable to convert resnet18 ONNX model to TensorRT Engine,"hello @dusty-nv i trained a sample resnet18/resnet50 model with the repo and successfully converted it to onnx format. then i wanted to use it in my code: > nvidia@nvidia:~/ai/algorithms/jetsonnew() > jetson.inference -- pyimagenetblob=inputblob=outputlabels labels.txt > -- input0' > -- output0' > -- batchtrt > [trt] plugin creator registration succeeded - nmstrt > [trt] plugin creator registration succeeded - regiontrt > [trt] plugin creator registration succeeded - normalizetrt > [trt] plugin creator registration succeeded - batchednmstrt in namespace: > [trt] plugin creator registration succeeded - cropandresize > [trt] plugin creator registration succeeded - proposal > [trt] plugin creator registration succeeded - batchtilepluginversion (0.0.4) than this parser was built against (0.0.3). > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (3, 224, 224) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (7, 7), strides: (2, 2), padding: (3, 3), dilations: (1, 1), numoutputs: 64 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (64, 112, 112) > [trt] 123:conv -> (64, 112, 112) > [trt] 124:batchnormalization -> (64, 112, 112) > [trt] 125:relu -> (64, 112, 112) > [trt] 126:maxpool -> (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 64 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (64, 56, 56) > [trt] 127:conv -> (64, 56, 56) > [trt] 128:batchnormalization -> (64, 56, 56) > [trt] 129:relu -> (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 64 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (64, 56, 56) > [trt] 130:conv -> (64, 56, 56) > [trt] 131:batchnormalization -> (64, 56, 56) > [trt] 132:add -> (64, 56, 56) > [trt] 133:relu -> (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 64 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (64, 56, 56) > [trt] 134:conv -> (64, 56, 56) > [trt] 135:batchnormalization -> (64, 56, 56) > [trt] 136:relu -> (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 64 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (64, 56, 56) > [trt] 137:conv -> (64, 56, 56) > [trt] 138:batchnormalization -> (64, 56, 56) > [trt] 139:add -> (64, 56, 56) > [trt] 140:relu -> (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (2, 2), padding: (1, 1), dilations: (1, 1), numoutputs: 128 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (128, 28, 28) > [trt] 141:conv -> (128, 28, 28) > [trt] 142:batchnormalization -> (128, 28, 28) > [trt] 143:relu -> (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 128 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (128, 28, 28) > [trt] 144:conv -> (128, 28, 28) > [trt] 145:batchnormalization -> (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (64, 56, 56) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (1, 1), strides: (2, 2), padding: (0, 0), dilations: (1, 1), numoutputs: 128 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (128, 28, 28) > [trt] 146:conv -> (128, 28, 28) > [trt] 147:batchnormalization -> (128, 28, 28) > [trt] 148:add -> (128, 28, 28) > [trt] 149:relu -> (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 128 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (128, 28, 28) > [trt] 150:conv -> (128, 28, 28) > [trt] 151:batchnormalization -> (128, 28, 28) > [trt] 152:relu -> (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 128 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (128, 28, 28) > [trt] 153:conv -> (128, 28, 28) > [trt] 154:batchnormalization -> (128, 28, 28) > [trt] 155:add -> (128, 28, 28) > [trt] 156:relu -> (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (2, 2), padding: (1, 1), dilations: (1, 1), numoutputs: 256 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (256, 14, 14) > [trt] 157:conv -> (256, 14, 14) > [trt] 158:batchnormalization -> (256, 14, 14) > [trt] 159:relu -> (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 256 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (256, 14, 14) > [trt] 160:conv -> (256, 14, 14) > [trt] 161:batchnormalization -> (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (128, 28, 28) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (1, 1), strides: (2, 2), padding: (0, 0), dilations: (1, 1), numoutputs: 256 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (256, 14, 14) > [trt] 162:conv -> (256, 14, 14) > [trt] 163:batchnormalization -> (256, 14, 14) > [trt] 164:add -> (256, 14, 14) > [trt] 165:relu -> (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 256 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (256, 14, 14) > [trt] 166:conv -> (256, 14, 14) > [trt] 167:batchnormalization -> (256, 14, 14) > [trt] 168:relu -> (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 256 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (256, 14, 14) > [trt] 169:conv -> (256, 14, 14) > [trt] 170:batchnormalization -> (256, 14, 14) > [trt] 171:add -> (256, 14, 14) > [trt] 172:relu -> (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (2, 2), padding: (1, 1), dilations: (1, 1), numoutputs: 512 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (512, 7, 7) > [trt] 173:conv -> (512, 7, 7) > [trt] 174:batchnormalization -> (512, 7, 7) > [trt] 175:relu -> (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 512 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (512, 7, 7) > [trt] 176:conv -> (512, 7, 7) > [trt] 177:batchnormalization -> (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (256, 14, 14) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (1, 1), strides: (2, 2), padding: (0, 0), dilations: (1, 1), numoutputs: 512 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (512, 7, 7) > [trt] 178:conv -> (512, 7, 7) > [trt] 179:batchnormalization -> (512, 7, 7) > [trt] 180:add -> (512, 7, 7) > [trt] 181:relu -> (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 512 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (512, 7, 7) > [trt] 182:conv -> (512, 7, 7) > [trt] 183:batchnormalization -> (512, 7, 7) > [trt] 184:relu -> (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:771: convolution input dimensions: (512, 7, 7) > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:835: using kernel: (3, 3), strides: (1, 1), padding: (1, 1), dilations: (1, 1), numoutputs: 512 > [trt] /home/erisuser/p4sw/sw/gpgpu/machinelearning/dit/release/5.1/parsers/onnxopensource/builtinimporters.cpp:836: convolution output dimensions: (512, 7, 7) > [trt] 185:conv -> (512, 7, 7) > [trt] 186:batchnormalization -> (512, 7, 7) > [trt] 187:add -> (512, 7, 7) > [trt] 188:relu -> (512, 7, 7) > [trt] 189:globalaveragepool -> (512, 1, 1) > [trt] 190:constant -> > [trt] 191:shape -> (4) > warning: your onnx model has been generated with int64 weights, while tensorrt does not natively support int64. attempting to cast down to int32. > successfully casted down to int32. > while parsing node number 69 [gather -> ""192""]: > --- begin node --- > input: ""191"" > input: ""190"" > output: ""192"" > oputils.hpp:277 in function convertdealloc() > traceback (most recent call last): > file ""imagenet-opencv.py"", line 66, in > '--labels=labels.txt']) > exception: jetson.inference -- imagenet failed to load network > double free or corruption (!prev) > aborted (core dumped) >",question,question
712,https://github.com/dusty-nv/jetson-inference/issues/712,Is it possible to train with rtx 3080 graphics card? I wonder if it fits the architecture that rtx introduces.,please answer my question..,question,question
753,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/753,Add Vagrant Setup,"objective lower the entry bar to using openpose to everyone interested in playing/learning about ml. issue summary installation is quite complex. numerous external dependencies increase the likelihood that installation will fail at some point along the way. suggested fix vagrant to the rescue! since vagrant uses virtualbox (which is cross-platform), various templates using ubuntu 18.04 lts could be created. examples would be: - amd w/ opencl gpu version - nvidia w/ cuda gpu version - basic cpu version",other,other
1104,https://github.com/dusty-nv/jetson-inference/issues/1104,Easy way to install pyTorch with  Gpu /Cuda support on Jetson nano,@dusty-nv i have my new jetson nano device here and would like to install pytorch with gpu support. can you please advise the easiest way to install pytorch with gpu / cuda support on jetson nano? i have tried several methods but no luck still ? thank you,question,question
677,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/677,macOS - protobuf compilation errors,steps to reproduce `bash 3rdparty/osx/installmode=cpueigen=build` using commit description compilation fails on mac with numerous errors in the protobuf lib:,deployment,deployment
491,https://github.com/dusty-nv/jetson-inference/issues/491,Problem imagenet-camera.py,"hello, i want to try the jetson nano with my logitech c920 usb cam. when i type `v4l2-ctl -d /dev/video0 --list-formats-ext` this is showed to me: ioctl: vidiocfmt index : 0 jetson.inference._new() jetson.inference -- pyimagenettrt [trt] plugin creator registration succeeded - nmstrt [trt] plugin creator registration succeeded - regiontrt [trt] plugin creator registration succeeded - lrelutrt [trt] plugin creator registration succeeded - normalizetrt [trt] plugin creator registration succeeded - batchednmsgooglenet.caffemodel.1.1.gpu.fp16.engine [trt] loading network profile from engine cache... networks/bvlcgooglenet.caffemodel loaded [trt] device gpu, cuda engine context initialized with 2 bindings [trt] binding -- index 0 [trt] binding -- index 1 [trt] binding to input 0 data binding index: 0 [trt] binding to input 0 data dims (b=1 c=3 h=224 w=224) size=602112 [trt] binding to output 0 prob binding index: 1 [trt] binding to output 0 prob dims (b=1 c=1000 h=1 w=1) size=4000 device gpu, networks/bvlcgooglenet.caffemodel loaded imagenet -- loaded 1000 class info entries networks/bvlcnew() jetson.utils -- pyfontnew() jetson.utils -- pycamerasourcesourcenew() jetson.utils -- pydisplaystategooglenet.caffemodel [trt] ------------------------------------------------ [trt] pre-process cpu 0.04990ms cuda 0.30786ms [trt] network cpu 20.25467ms cuda 12.43911ms [trt] post-process cpu 0.33928ms cuda 0.33854ms [trt] total cpu 20.64385ms cuda 13.08552ms [trt] ------------------------------------------------ [trt] note -- when processing a single image, run 'sudo jetsondealloc() jetson.utils -- pyfontdealloc() [gstreamer] closing gstcamera for streaming, transitioning pipeline to gstnull jetson.utils -- pydisplay_dealloc() i don't know how to fix this. has anybody an idea?",question,question
126,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/126,General questions,"hi，i want to ask you some questions: （1）what format is the data in your openpose? including node data, face feature data, and user id （2）when the human body is blocked or too far away to capture the key points incomplete data, how the bones prior knowledge, calculate the loss of data or do not get? （3）does the joint data have only x, y values, no z values, and no joint rotation information? how to calculate z value? can be considered based on the same length of bone projections. in addition, whether the camera parameters, if you can get the camera parameters, according to the perspective transformation matrix, calculate the z value. （4）which is the major code in the openpose? and whicn is the io code? i hope you can answer my questions. your system configuration **: installed with `apt-get install libopencv-dev compiler (`gcc --version` on ubuntu):g++ 4.9",question,question
1447,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1447,Unable to launch demos,"issue summary i wish to test out whether openpose was able to launch for me, and has used the provided console command ,"" ./build/examples/openpose/openpose.bin --video examples/media/video.avi "" . however, it listed to me that error: starting openpose demo... configuring openpose... /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoproducer():210 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoproducertype():170 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoposemode():11 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoposemodel():33 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoscalemode():82 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoheatmapscalemode():116 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstodetector():147 /home/john/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstodetector():147 starting thread(s)... /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():98 /home/john/openpose/src/openpose/producer/producer.cpp:createproducer():417 /home/john/openpose/src/openpose/wrapper/wrapperauxiliary.cpp:wrapperconfiguresanitychecks():17 /home/john/openpose/src/openpose/wrapper/wrapperauxiliary.cpp:wrapperconfiguresanitychecks():184 auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():337 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():402 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():407 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():418 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():472 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():524 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():568 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():612 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():629 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():646 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():665 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():681 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():714 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():739 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():748 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():764 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():765 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():774 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():791 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():811 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():832 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():841 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():845 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():851 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():910 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():926 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():932 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1008 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1042 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1146 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1170 /home/john/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1177 /home/john/openpose/include/openpose/wrapper/wrapper.hpp:exec():419 /home/john/openpose/include/openpose/thread/threadmanager.hpp:exec():185 /home/john/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /home/john/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /home/john/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /home/john/openpose/include/openpose/thread/threadmanager.hpp:exec():190 /home/john/openpose/include/openpose/thread/thread.hpp:threadfunction():182 /home/john/openpose/include/openpose/thread/thread.hpp:initializationonthread():167 starting initialization on thread. in /home/john/openpose/src/openpose/pose/poseextractorcaffe.cpp:netinitializationonthread():172 [libprotobuf error google/protobuf/messageparam.min, layer[0].clipproto.cpp:97] check failed: readprotofrombinaryfile(param25/pose584000.caffemodel **** issue: is there anything i could do to solve this issue?",question,question
213,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/213,openpose.bin does not seem to take advantage of multiple GPU's,"issue summary executing ./build/examples/openpose/openpose.bin does not seem to take advantage of multiple gpu's even tho multiple gpus are detected as indicated by the output message: ""auto-detecting gpus... detected 2 gpu(s), using them all."" regardless if one gpu or two gpu's are used, the processing for a single image in the imagedir images --writekeypointscale 3 --nopose 0 i've also tried specifying '-numgpu 1'. (i'm only retrieving the keypoint data and not generating an output image.) openpose output (if any) starting pose estimation demo. auto-detecting gpus... detected 2 gpu(s), using them all. starting thread(s) real-time pose estimation demo successfully finished. total time: 4.150646 seconds. type of issue - help wanted your system configuration **: 2.4.9.1, installed with `apt-get install libopencv-dev` (ubuntu) generation mode (only for ubuntu): makefile + makefile.config (default, ubuntu) compiler (`gcc --version` in ubuntu): gcc (ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609",question,question
1154,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1154,make calibration.bin fails: undefined reference to `cv::calcOpticalFlowPyrLK(...,"issue summary when following the steps to compile from source , the build fails at the step 'calibration.bin'. this can be verified by running 'make calibration.bin', which produces the same error: i previously was having the issue described , but i've since run a `make clean` and now we're here. executed command (if any) `make`, `make -j12`, and `make calibration.bin` produce this error. type of issue you might select multiple topics, delete the rest: - compilation/installation error - help wanted your system configuration 1. **:",Error,Error
662,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/662,How to build python API,"i modified this: ./cmakelists.txt:option(build_python ""build openpose python."" on) but no python api is build.",question,question
64,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/64,Compile Error by ‘atomic’,"issue summary compile error in ./include/openpose/core/renderer.hpp i added the following code, there is no problem. add code: place: ./include/openpose/core/renderer.hpp executed command (if any) $ ./installandrelease/src/openpose/core/definetemplates.o] type of issue you might select multiple topics, delete the rest: - compilation/installation error your system configuration **: installed with `apt-get install libopencv-dev` compiler (`gcc --version` on ubuntu):gcc (ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4",deployment,Error
248,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/248,Speedup face tracking / Is there a paper about the face landmarks detection algorithm?,hi guys:,other,other
6,https://github.com/dusty-nv/jetson-inference/issues/6,glibconfig.h: No such file or directory,"hi, @dusty-nv, when i make, i meet this problem, but i can't fix it, can you do me a favor? thanks.",question,question
1746,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1746,Can't build open pose for CPU only build,"i have followed every instruction for cpu only build , everything went as expected , but when i ran the final command , inside build directory `make -j nproc` i get the following error i am using ubuntu 20",deployment,question
1330,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1330,write json in python api for video,"i turn on the params[write_json], and also some simple changes to read video. however, the json output seems like only get the first frame keypoint. so, is there any solution for this case?",question,question
759,https://github.com/dusty-nv/jetson-inference/issues/759,TX2 Video inverted,"hey, thanks for the wonderful tutorial, i'm new to the jetson platform and tried your tutorial showed in . i have tried on my jetson tx2, it works but the video is upside down. i find some of solutions but didn't work. i have tried to change the flip method in jetson-inference/utils/camera/gstcamera.cpp but i don't know where to change as .cpp is shown like this i read the issue in #jetson nano csi raspberry pi camera v2 upside down 180 degree #571 and tries to find the .cpp file you showed to change here so i replaced it in jetson-inference/utils/camera/ and run the command $sudo make install but it gives an error and the video was still inverted 180. my system is nvidia jetson tx2 - jetpack 4.4 [l4t 32.4.3] i've spent a good amount and couldn't solve the problem. please guide me on how to solve this",question,question
1690,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1690,openpose.bin Execution error Mac OSX,"issue summary i compiled all the project and when i am going to run the example an error occurs. executed command (if any) `sudo ./openpose.bin --video /users/juanalbertogarcia/openpose/examples/media/video.avi --modellevel 0 -disablethread` openpose output (if any) starting openpose demo... configuring openpose... /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoproducer():210 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoproducertype():170 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoposemode():11 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoposemodel():33 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoscalemode():82 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstoheatmapscalemode():116 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstodetector():147 /users/juanalbertogarcia/openpose/src/openpose/utilities/flagstoopenpose.cpp:flagstodetector():147 starting thread(s)... running configurethreadmanager... /users/juanalbertogarcia/openpose/src/openpose/producer/producer.cpp:createproducer():417 rendermodepose = 2 rendermodeface = 2 rendermodehand = 2 renderoutput = 1 renderoutputgpu = 1 renderface = 0 renderhand = 0 renderhandgpu = 0 /users/juanalbertogarcia/openpose/src/openpose/wrapper/wrapperauxiliary.cpp:wrapperconfiguresanitychecks():17 ---------------------------------- warning ---------------------------------- we have introduced an additional boost in accuracy in the cuda version of about 0.2% with respect to the cpu/opencl versions. we will not port this to cpu given the considerable slow down in speed it would add to it. nevertheless, this accuracy boost is almost insignificant so the cpu/opencl versions can be safely used. -------------------------------- end warning -------------------------------- /users/juanalbertogarcia/openpose/src/openpose/wrapper/wrapperauxiliary.cpp:wrapperconfiguresanitychecks():184 userinputandpreprocessingwsempty = 1 useroutputwsempty = 1 numbergputhreads = -1 gpunumberstart = 0 writeimagescleaned = writekeypointcleaned = writejsoncleaned = writeheatmapscleaned = modelfolder = /users/juanalbertogarcia/openpose/models/ finaloutputsize = [1280,720] /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():371 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():436 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():452 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():506 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():558 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():602 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():646 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():663 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():680 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():694 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():699 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():715 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():748 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():773 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():782 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():798 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():799 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():808 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():825 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():845 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():866 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():875 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():879 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():885 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():944 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():960 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():966 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1042 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1076 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1180 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1204 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1211 /users/juanalbertogarcia/openpose/include/openpose/wrapper/wrapper.hpp:exec():419 /users/juanalbertogarcia/openpose/include/openpose/thread/threadmanager.hpp:exec():185 /users/juanalbertogarcia/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /users/juanalbertogarcia/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /users/juanalbertogarcia/openpose/include/openpose/thread/queuebase.hpp:addpusher():362 /users/juanalbertogarcia/openpose/include/openpose/thread/threadmanager.hpp:exec():190 /users/juanalbertogarcia/openpose/include/openpose/thread/thread.hpp:threadfunction():182 /users/juanalbertogarcia/openpose/include/openpose/thread/thread.hpp:initializationonthread():167 starting initialization on thread. in /users/juanalbertogarcia/openpose/src/openpose/pose/poseextractorcaffe.cpp:netinitializationonthread():172 finished initialization on thread. in /users/juanalbertogarcia/openpose/src/openpose/pose/poseextractorcaffe.cpp:netinitializationonthread():191 2020-09-03 15:08:02.988 openpose.bin[21393:7923890] cfurlcopyresourcepropertyforkey failed because it was passed an url which has no scheme 2020-09-03 15:08:02.988 openpose.bin[21393:7923890] cfurlcopyresourcepropertyforkey failed because it was passed an url which has no scheme 2020-09-03 15:08:03.009 openpose.bin[21393:7923890] cfurlcopyresourcepropertyforkey failed because it was passed an url which has no scheme segmentation fault: 11 type of issue you might select multiple topics, delete the rest: - execution error",question,question
690,https://github.com/dusty-nv/jetson-inference/issues/690,Issue with custom detection,"hello nvidia, im new to jetson nano and linux so i decided to make my own detection dataset. i did all as in the tutorial (collecting your own detection datasets) and got my own model (ssd-mobilenet.onxx). but when i try to run detectnet.py with my custom model (via this command: detectnet.py --model=models/mymodel/ssd-mobilenet.onnx --labels=models/mymodel/labels.txt \ --input-blob=inputtrt version 1, maybe it causes this? but with this error detectnet.py works pertfectly with default network (ssd-mobilenet-v2). what i have to do to fix this issues? thanks",question,question
640,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/640,Is there 2D skeleton feet position implemented?,is there any future release which detects also feet position? thank you,other,question
1701,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1701,Version of CudNN in Cuda.make,"dear developers, i found and possibly solved a problem related to cudnn version in cuda.make and findcudnn.cmake. they look on the cudnn version in the following line: file(read ${cudnnversioncontents) however, the version information are in cudnninclude}/cudnnversioncontents) thanks, manuel",deployment,other
109,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/109,HDF5: infinite loop closing library,"i am running the openpose demo on a ec2 instance and getting the following error: > hdf5: infinite loop closing library > d,t,f,fd,p,fd,p,fd,p,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e,e command: > ./build/examples/openpose/openpose.bin --video examples/media/video.mp4 --writedisplay regardless of the error, the output video is correct. do you know what might be wrong with openpose & hdf5? your system configuration **: 2.4.8 compiler (`gcc --version` on ubuntu): 4.8.4",Error,question
1138,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1138,Outputs a photo rather than a video frame,posting rules 1. ** issue:,question,other
736,https://github.com/dusty-nv/jetson-inference/issues/736,Possible problem with the onnx representation,resolved,other,other
199,https://github.com/dusty-nv/jetson-inference/issues/199,6 GB is not enough? ,"hi! i have a 6gb 1060 card, and followed manual from this repo to train detectnet. i`ve used batch size 2 and accumulation 5 as it stated here( but digits says that my gpu is out of memory. whats wrong? thanks in advance.",question,question
1197,https://github.com/dusty-nv/jetson-inference/issues/1197,2688x1520 input has problem in object detection...,"hi, when i use 1280x720 input, everything works. however, i want to object detect some small objects, so, i tune up my ip camera to 2688 x 1520 when i did the camera capture for detection, i can only capture the first image, the camera view freezes. and i need to restart the camera capture for another capture. for testing, i have 4 pictures in data set (mix), when i did the training, it keeps printing this error at epoch 0 openblas warning : detect openmp loop and this application may hang. please rebuild the library with use_openmp=1 option. may i know if 2688 x 1520 input is ok? or there is some code i need to modify? thx and what is a good way to detect small objects? thx",question,question
1799,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1799,Mac: The --video flag does not run with any video.,"summary the example video, and any other video that i try to run openpose.bin with results in an error i think thrown by opencv. 'openpose.bin' works successfully when ran on its' own (autodetects webcam) and when used with the --imagedir flag to function correctly. openpose output (if any) errors (if any) opencv: couldn't read video stream from file ""media/video.avi"" [error:0] global /users/chrisnash/documents/pythonenvironments/py37/lib/python3.7/site-packages/opencv/opencv/modules/videoio/src/cap.cpp (142) open videoio(cvimages.cpp:253: error: (-5:bad argument) cap_images: can't find starting number (in the name of file): media/video.avi in function 'icvextractpattern' error: videocapture (ip camera/video) could not be opened for path: 'media/video.avi'. if it is a video path, is the path correct? coming from: - /users/chrisnash/pycharmprojects/op/openpose/src/openpose/producer/videocapturereader.cpp:videocapturereader():54 - /users/chrisnash/pycharmprojects/op/openpose/src/openpose/producer/videocapturereader.cpp:videocapturereader():58 - /users/chrisnash/pycharmprojects/op/openpose/src/openpose/producer/producer.cpp:createproducer():475 - /users/chrisnash/pycharmprojects/op/openpose/include/openpose/wrapper/wrapperauxiliary.hpp:configurethreadmanager():1222 - /users/chrisnash/pycharmprojects/op/openpose/include/openpose/wrapper/wrapper.hpp:exec():424 type of issue you might select multiple topics, delete the rest: - execution error - help wanted your system configuration 2. ** api:",question,question
955,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/955,Did openpose attend posetrack challenge?,"i was wondering, whether openpose took the posetrack challenge and what the results were or if the challenge wouldn't be applicable for openpose?",other,other
1195,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1195,Single person about Openpose python API,hi！how to call the openpose api to turn off the multi-person posture and only show the single posture? i only found the multi-person posture in the openpose api example.can you give me an answer?,question,question
1688,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1688,Openpose how to change default camera ,openpose how to change default camera such as to use intelrealsense camera,question,other
1198,https://github.com/dusty-nv/jetson-inference/issues/1198,RTSP problem (gstDecoder -- Could not get/set settings from/on resource),"hi, my other ip cameras work except this unknown brand. i can view it in my window vlc player. this command works and i can see the pop up window video (gst-launch-1.0 uridecodebin uri=‘rtsp://rtsp_uri’ ! nvoverlaysink) but this command does not work video-viewer --input-codec=h264 rtsp://192.168.50.3:554/user=…&password=…&channel=1&stream=0.sdp? there is an error of “gststreamer rtspsrc0 error could not get/set settings from/on resources”. when i google it, this error is related to gst-launch (but i have no problem with gst-launch, only the video-viewer and detectnet…) here is the youtube for a better explanation and i have posted it in may i know how to solve this problem? do i need to modify the code? thx",question,question
1224,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1224,Build fails - file DOWNLOAD HASH mismatch Ubuntu,"issue summary cmake appears to fail because the downloaded file does not match the one expected. executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. cmake .. errors (if any) - compilation/installation error your system configuration 1. ** issue: i have attached the , but it only mentions something about pthread. all the best, peter",question,question
647,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/647,Compile under Windows vs2015 x64,"i use windows 10. after using cmake (windows) to create the vs 2015 x64 project i get the following error when compiling the openpose library: error lnk2005: ""public: virtual __cdecl op::personidextractor::~personidextractor(void)"" (??1personidextractor@op@@ueaa@xz) already defined in personidextractor.cpp.obj because we already have a user interface in wpf c# tried building it with /clr option but the mutex class can not be compiled with this option. is this the only class that has this problem and is it even possible to use the /clr code with the other code? regards eddy",question,question
470,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/470,VideoCapture (IP camera/video) could not be opened for path: 'examples\media\video.avi',"issue summary videocapture (ip camera/video) could not be opened for path: 'examples\media\video.avi' executed command (if any) note: add `--loggingmultirelease -a` in ubuntu): window 10 **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? openpose default compiler (`gcc --version` in ubuntu):",question,question
1210,https://github.com/dusty-nv/jetson-inference/issues/1210,[QST] How to limit gpu usage in the context of tensorrt?,"hi @dusty-nv , as jetson devices are resource constrained devices, sometimes competing for resources happens in the context of gpu usage. for example, process a(a thrust based pointcloud processing node) and b(a tensorrt based inference node) both use more than a half of gpu cores. process a can be blocked by b, so processing time/latency increases a lot, which is unacceptable when a is time-critical. so my thoughts of this problem is about two ideas. 1. implement realtime setup to a to get higher priority and sched policy 2. limit gpu usage in the context of thrust and tensorrt my questions are: 1. is it feasible for gpu programes with realtime setup? 2. how to limit gpu usage of tensorrt, even in thrust? i have checked and it seems there is no good idea for jetson devices. thanks.",other,question
2032,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/2032,Install Error(Ubuntu) `make` cannot detect the cudnn but `cmake-gui` can do this,"posting rules 2. ** if you are facing an error or unexpected behavior. some posts (e.g., feature requests) might not require it. issue summary follow the installation, cmake-gui can detect the updated `cudnnpath` and configure with success, but `make` cannot. cmake-gui: make -j12: executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. openpose output (if any) errors (if any) type of issue select the topic(s) on your post, delete the rest: - compilation/installation error - execution error - help wanted - question - enhancement / offering possible extensions / pull request / etc - other (type your own type)",deployment,other
870,https://github.com/dusty-nv/jetson-inference/issues/870,"Retrained ResNet152 ONNX model - ""Dynamic Shape....No Optimization..""","hey dusty, i am getting this error when attempting to load a retrained resnet152 `network has dynamic or shape inputs but no optimization profile has been defined` i have retrained the resnet152 as a bird species classifier using a tutorial i found . the examples folder in that repository has the second and the final notebook which displays how the model is trained and then exported as onnx. i have looked through the code, and in the training notebook i have found where it has specified optimization. it seems that the network is already optimized in training. i am at a loss for how to solve this issue. i've been searching around on how to just build my own program in deepstream, but i have a steep learning curve. if you could help me get this .onnx model properly loaded into your repo it would save me some time in getting the first version of my system that i am building up and running. please have a look at the final notebook in the example notebooks folder and/or help me figure out how to get my network properly optimized so i can deploy. thanks",question,question
84,https://github.com/dusty-nv/jetson-inference/issues/84,Can I run a LeNet model on Jetson?,"i have trained my own lenet model using digits and following the tutorials. i have tested the network on my host node and it works. i now want to run it on my jetson, but in the code i can only see: is there a way to run custom lenet models?",question,question
432,https://github.com/dusty-nv/jetson-inference/issues/432,detectnet-console.py failed,"when i run ""./detectnet-console.py --network=ssd-mobilenet-v2 images/peds0.jpg' (1920 x 1080, 3 channels) jetson.inference -- pytensornetinit() jetson.inference -- detectnet loading network using argv command line params jetson.inference -- detectnet._0.jpg' jetson.inference -- detectnet._dealloc() traceback (most recent call last): file ""./detectnet-console.py"", line 51, in exception: jetson.inference -- detectnet failed to load network jetson.utils -- freeing cuda mapped memory ############################################################################## i just follow the guide, i don't know why this issue occurred.",question,question
310,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/310,Cuda check failed (7 vs. 0): too many resources requested for launch on TX2,"at first ,thank you for the wonderful work! issue summary i run the command blew on tx2(already sudo ./jetsondebug/examples/openpose/openpose.bin --loggingresolution, ""640x480"" netresolution, ""640x480"" netresolution, ""640x480"" netresolution, ""640x480"" netresolution, ""640x480"" netdebug/examples/openpose/openpose.bin --loggingmultisserror' what(): error: cuda check failed (7 vs. 0): too many resources requested for launch coming from: - src/openpose/pose/renderpose.cu:renderposekeypointsgpu():408 - src/openpose/utilities/cuda.cpp:cudacheck():36 - src/openpose/pose/renderpose.cu:renderposekeypointsgpu():413 - src/openpose/pose/posegpurenderer.cpp:renderpose():168 - ./include/openpose/pose/wposerenderer.hpp:work():74 - ./include/openpose/thread/subthread.hpp:worktworkers():135 - ./include/openpose/thread/subthreadqueueinout.hpp:work():84 - ./include/openpose/thread/thread.hpp:threadfunction():203 - ./include/openpose/thread/thread.hpp:exec():129 - ./include/openpose/thread/threadmanager.hpp:exec():180 - ./include/openpose/wrapper/wrapper.hpp:exec():945 aborted (core dumped) type of issue you might select multiple topics, delete the rest: - execution error your system configuration **: opencv4tegra 2.4.13.1(pkg-config --modversion opencv) compiler (`gcc --version` in ubuntu): gcc (ubuntu/linaro 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609 copyright (c) 2015 free software foundation, inc. this is free software; see the source for copying conditions. there is no warranty; not even for merchantability or fitness for a particular purpose. 1.how to clone the tensorrt pull request in",question,question
1033,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1033,Question: Pose estimation for top-view images,issue summary i see that the model is trained for front/side-view human pose estimation. are there plans for extending this to top-view human pose estimation or am i missing something where this is already implemented? type of issue - enhancement / offering possible extensions / pull request / etc thanks!,question,other
1060,https://github.com/dusty-nv/jetson-inference/issues/1060,How to train semantic segmentation,"hi, this repo really helped me a lot. is there any specific reason for not adding procedure for semantinc segmentation re-training and data collection. i am facing problem in training semantic segmentation model. in below link required code is available. i am new to python so facing problem in executing it. it will be grateful if i get needed support. thanks in advance.",question,question
294,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/294,Raspberry 3,"hello gines! i have a question, do you think that i could use openpose in a raspberry 3 with linux?",question,question
1750,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1750,About run on RTX 3070,i build it with gtx 1080 but it can't run on the computer with rtx 3070 and no error message. is cuda version incompatibility?,question,deployment
484,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/484,How to process the image more faster?  ,modify the parameter？,question,question
1307,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1307,Python Standalone detectors (Hands),"type of issue - help wanted - question - enhancement summary i have a video i would like to detect the hand keypoints on, but there are just hands, no body (piano playing). i have trained a custom hands detector, which finds where the hands are, and now i have for every hand a bounding box. here is a crop, centering the hand and including some padding. 2. for the standalone detector, i couldn't find best practices for image sizes - what crop should be done, i'm assuming centering the hand, but with what padding? what background-color is optimal if we need to add extra pixels? 3. just a general question about the data - i currently use an in tensorflow for the pose estimation, but it is quite bad on my data. do you know if the data the hands pose model was trained on includes dark images or inconsistent lighting?",question,question
1469,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1469,Generate results only when detecting one or multiple body parts,"type of issue - enhancement / offering possible extensions / pull request / etc i think it would be a great feature if you could tell openpose to output a result only when one or multiple body parts are detected in the scene, it can take a lot of time to clean up all the bad data from the results folder, this would cut out the bad stuff right out of the tap. would such a feature be possible? i think it is extremely useful. thanks for your awesome work.",other,other
1863,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1863,Run the demo from remote server,"i have my code setup on a remote server, and i want to run a real-time demo there. is there any way by which i can open webcam on my local mac machine, and run openpose on the remote server, which has ubuntu?",other,question
1256,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1256,Run OpenPose enabling shared GPU?,"dear, issue: execution error. i have an issue when running most of the examples (except for: 06frombodyimageauto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. f0614 15:34:27.376272 7600 syncedmem.cpp:71] check failed: error == cudasuccess (2 vs. 0) out of memory._ it seems that the gpu dedicated memory hits the maximum (2gb), but the shared gpu memory usage is very low (< 1 gb). how could i make use of the gpu shared memory? is the issue actually related to this? is it related to the flags (how can they be changed using vs2017)? in the same machine i could use a previous version of openpose from december 2017 and a previous version of cuda (8.0). i am using windows 10 and visual studio 2017 community. cuda 10, cudnn 7.5, the latest version of , with the eigen version from . i am using release mode, x64 in visual studio. i have geforce gtx 960m (dedicated memory 20148 mb, and shared memory 8136 mb). any piece of advice will be appreciated. thank you very much. best regards, gil",question,question
279,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/279,[Ubuntu 16]pip is already up-to-date(9.0.1) but was told using older version of pip during installation,"issue summary when executing `bash ./ubuntu/installcmake.sh`, i thought it successfully installed everything(no errors, but just warnings that told me the version of pip is 8.x while the newest version is 9.0.1) and then i tried to configure using `cmakecaffeopenposecuda8.sh`. after all that, i ran `bash ./ubuntu/installrelease -a` in ubuntu): **: compiler (`gcc --version` in ubuntu):",question,deployment
1728,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1728,"Auto-detecting all available GPUs... Detected 1 GPU(s), using 1 of them starting at GPU common.cpp:66 Cannot use GPU in CPU-only Caffe: check mode.","~/openpose$ ./build/examples/openpose/openpose.bin --video examples/media/video.avi starting openpose demo... configuring openpose... starting thread(s)... auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. f1023 20:27:05.959867 17384 common.cpp:66] cannot use gpu in cpu-only caffe: check mode. **** 중지됨 (core dumped) i just need to process of openpose what i wrong",question,question
1245,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1245,VS2017: Error generating openpose_generated_renderFace.cu.obj?,"hello all, i followed installation instructions and generated successfully. vs2017 build error! ** windows7、cuda v10.0.130、cudnn 7.3.0.29、vs2017、cmake3.10.1、gtx1050 c:\users\dev001>nvcc --version nvcc: nvidia (r) cuda compiler driver copyright (c) 2005-2018 nvidia corporation built on sat25centraltime_2018 cuda compilation tools, release 10.0, v10.0.130",question,question
1076,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1076,[Mac OSX]  'cublas_v2.h' file not found when building with OpenCL ,"issue summary when trying to build with opencl on mac pro, i got the ` 'cublasalternate.hpp:34:10: fatal error: #include ` type of issue - compilation/installation error - execution error your system configuration 1. ** issue:",deployment,question
1549,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1549,Openpose Python API crashes at opWrapper.emplaceAndPop([datum]),"executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. openpose output (if any) none errors (if any) type of issue you might select multiple topics, delete the rest: - execution error your system configuration 1. ** issue:",question,other
1024,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1024,built the openpose project,"when i built the openpose through vs 2015 professional,it show me the message: opencvworld310.dll) : fatal error lnk1112: module machine type 'x64' conflicts with target machine type 'x86'",question,question
1508,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1508,getting error CUDNN_STATUS_NOT_INITIALIZED,"i have installed cuda=10.2 and cudnn=7.6.5. the code is built suceessfully, but when i run something from example, rather with pythonconvstatusstatusinitialized **** aborted (core dumped) can someone please help me in this. i have reinstalled cudnn three times and it is not working.",question,question
184,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/184,Cannot create Cublas handle. Cublas won't be available.,">highly recommend you to read the following blog first, which records the whole process from ""git clone"" till now. it'll be helpful for you to understand my situation. ：） here's my blog: >and i'm a student from china, may be it's a little weird of my english. hope you can forgive about this. >i want you to know that i have carefully read each issue related to my question. thank you! about the running environment the latest clone of openpose issue summary when running the example command of openpose, the problem occured. i have made caffe and openpose successfully as the installation instruction. the problem occured when i want to run the quick start. here's the command from quick start: `./build/examples/openpose/openpose.bin --imagelevel 0` to get higher debug information. ./build/examples/openpose/openpose.bin --imagelevel 0`): you might select multiple topics, delete the rest: - execution error - help wanted your system configuration **: installed opencv 2.4.13 generation mode (only for ubuntu): makefile + makefile.config (default, ubuntu) compiler (`gcc --version` in ubuntu):5.4.0",question,question
877,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/877,Getting rid of false positives,"hi, is there anyway i could filter out the false positives. for example it detects humans in trees( the confidence is high cannot filter with that criteria as well). and is there anyway i can check for the id of the human. for example i'm getting the pose of a person and his id is 0 and in the next frame a new person walks in, can i keep the id 0 for the person who was there already and 1 for the new person in frame. thanks in advance. ps: the algorithm is pretty awesome. thumbs up",question,question
1127,https://github.com/dusty-nv/jetson-inference/issues/1127,Retraining cat/dog imagenet.py,i'm a newbie. who can tell me where to download imagenet.py? thanks!,question,question
942,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/942,compile error. ‘caffe::Blob’ is not a template type,"issue summary i use custom caffe to compile and get this error:. executed command (if any) errors (if any) the verbose make output : type of issue you might select multiple topics, delete the rest: - compilation/installation error your system configuration 1. ** issue:",Error,Error
206,https://github.com/dusty-nv/jetson-inference/issues/206,How to draw a diagnol line on the camera output,i want to modify the code to draw a diagnol line on the camera output. the changes need to be done in the file cudaoverlay.cu within the jetson-inference folder. can someone help me with a code to draw a diagnol line on the real time video. opencv and opengl methods tried did not work. thanks.,question,question
126,https://github.com/dusty-nv/jetson-inference/issues/126,how drawBoxes function work?,i really do not understand how `drawboxes` in `detectnet-camera` work...,question,question
869,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/869,Segmentation fault (core dumped),issue summary i get the error on running executed command (if any) openpose output (if any) errors (if any) backtrace type of issue - execution error - help wanted your system configuration 2. ** api: ====================== any help fixing this would be greatly appreciated,question,other
1486,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1486,Increase performance by reducing detection area,"type of issue - enhancement / offering possible extensions / pull request / etc usually bigger pictures takes longer to process as openpose tries to detect people on the entire picture, but sometimes it's known that a target person will always stay in a specific small area (ie. a person poledancing) and never move out from there for the entire clip (ie. imagine a 256x512 area out of a 1920x1080 picture), so it would be faster to crop that area in 256x512 pictures and run openpose on them, but then the json aligments won't match with the original 1920x1080 pictures anymore, making them unusable, would you consider adding a feature that allows to focus only on a smaller area to speed up detection considerably? it's so much faster when i crop the pictures first, but i also need the json data to align with the originals afterwards. i think that would be a really useful feature, especially on landscape shots where the person is usually at the center and their left and right have nothing important to detect.",Performance,Performance
1273,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1273,How to get 18 COCO keypoints written to JSON (instead of 17 keypoints),"issue summary hi! i have a question concerning the keypoint output of openpose. i am trying to get the 18 coco keypoints as visualized in . however, when passing the `--writejson` flag to `openpose.bin`, the resulting .json file only contains 17 keypoints. i looked through the source code, and it seems that in the `cocojsonsaver::record` function, only 17 indices of the detected pose keypoints are chosen. (it does not make a difference if i pass ` --modeldir $outputpose coco --writejson $output_folder/keypoints/keypoint.json` type of issue you might select multiple topics, delete the rest: - question your system configuration 1. **:",question,question
464,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/464,Broken link in doc page,issue summary the links on bullet point no.1 (“demo” and “openpose wrapper”) in are broken. type of issue - documentation error,Error,Error
393,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/393,Scale key points failed,"i tried to scale key points like this: but it gives me something like this: apparently,it doesn't give right result,how to solve this problem?",question,question
741,https://github.com/dusty-nv/jetson-inference/issues/741,Re-training SSD-Mobilenet EOFError: Ran out of input,"there was an error that when i re-training ssd-mobilenet,there was an error out put nvidia@nvidia-desktop:~/jetson-inference/python/training/detection/ssd$ python3 traindata=false, basenetsize=4, checkpointtype='opensteps=10, extralr=none, freezenet=false, freezewidthepochs=30, numssd='models/mobilenet-v1-ssd-mp-0max=100, useepochs=1, weight675.pth traceback (most recent call last): file ""trainfromssd file ""/home/nvidia/.local/lib/python3.6/site-packages/torch/serialization.py"", line 585, in load file ""/home/nvidia/.local/lib/python3.6/site-packages/torch/serialization.py"", line 755, in load eoferror: ran out of input i hope someboy could help me",question,question
1791,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1791,Can it work with 3DS max or Maya?,"my title say mostly want i want to know. i was planning to get a motion capture kit, but if openpose can work with decent quality in 3ds max or maya. i would be glad to give it a go and send feedback to improve openpose. i got a freshly builded ryzen 9 5950x desktop, just kept my old gpu evga 1050ti sc. not afraid to add more rams if 32 gb ddr4 isn't enough and/or a bigger gpu if needed. i'm mostly working with c#, but not afraid to explore and edit the scripts myself to help.",other,other
1827,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1827,Makeathon University of Michigan - What webcam should we buy?,"dear openposers, first, let me congratulate you on this great open-source initiative. it opens the possibilities to many projects within the biomechanics field. here at the university of michigan, we are designing a makeathon for interested graduate-undergraduate students. one of the projects involves your tool (details are below). our questions are, do you have recommendations about the quality of the video/lighting/environment in order to estimate joint angles? do you have a specific camera that you recommend (preferably a webcam or that can transmit video through usb)? our idea is to provide each team with a webcam and we wanted to know your opinion before we start purchasing. any information is appreciated. thank you for this initiative! edgar measuring rehabilitation outcomes with openpose for prosthetists and orthotists, it is important to measure objective rehabilitation outcomes to asses patient progress. these rehabilitation outcomes typically include specific metrics of the human lower-limb kinematics, such as step length, cadence, and range of motion (rom) of the joints. these metrics are available using motion capture systems, e.g., using or . however, the cost of these systems makes these kind of analyses prohibitive for most clinicians. * estimate rom of the knee and ankle joints from a video recording. offline or real-time estimation is valid. calculate the step length and cadence. include a plot of the joint angle vs. time for the knee and ankle joints. select a time window that is easy for visualization and is representative of the data. spicy (includes objectives at medium level) include a violin plot to report the statistics of the rom, step length, and cadence. * create a real-time video that shows in parallel the motion of the patient with the skeleton from openpose, the joint angle vs. time for the knee and ankle, and the statistical information from the the data currently in display. resources 1. 1. webcam 1080p hd. constraints 1. develop the tool for windows 10/8. our alpha test will be to deploy our tool at the um-orthotics and prosthetic center (). the umopc will have access only to windows machines.",other,other
22,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/22,How can I use OpenPose in another project?,"issue summary my test project: i just copy the file rtpose.cpp to the project.then i write a cmakelists.txt: cmakerequired(version 2.8) project(test) set(cmakecompiler ""g++"") set(cmaketype debug) set(cmakeflags ""-std=c++0x"") finddirectories( ${cudadirs} /home/wsh/projects/openpose/include /home/wsh/projects/openpose/3rdparty/caffe/include ) addlinklibs} /home/wsh/projects/openpose/build/lib/libopenpose.so /home/wsh/projects/openpose/3rdparty/caffe/build/lib/libcaffe.so ) i can run the examples successfully in openpose, but in my test project, it can not. type of issue help wanted openpose output (if any) /home/wsh/projects/openpose/include/openpose/experimental/face/faceextractor.hpp:48:29: error: ‘resizeandmergecaffe’ was not declared in this scope std::sharedptr spnmscaffe; ......",question,question
43,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/43,"Low speed - OpenPose is quite slow, is it normal? How can I speed it up?","issue summary low speed - openpose is quite slow, is it normal? how can i speed it up? type of issue you might select multiple topics, delete the rest: - help wanted - question",Performance,question
671,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/671,What is the processing order for the post-processing after getting the net output?,"issue summary hi @gineshidalgo99, i would like to ask you that how to process the net output (e.g., heatmaps) for post-processing. for example, is it resize the heatmaps first and then perform nms and joint grouping, or vice versa? and during the step of resize, is it resize the heatmaps to the original image size directly, or resize to the net input size (e.g., 368*:",question,question
190,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/190,COCO JSON output image_id capped at 9999?,"issue summary trying to process a video with 20k frames, with output in coco json. the `imagerelease -a` in ubuntu): win10 **: installed with `apt-get install libopencv-dev` (ubuntu) or default from openpose (windows) or opencv 2.x or opencv 3.x. generation mode (only for ubuntu): makefile + makefile.config (default, ubuntu) or cmake (ubuntu, windows) or visual studio (windows). compiler (`gcc --version` in ubuntu):",Performance,other
607,https://github.com/dusty-nv/jetson-inference/issues/607,Detecting objects from RSTP Stream (IP Camera) ,"hi guys! i’ve implemented the object detection model by following the tutorial in youtube (real-time object detection in 10 lines of python code on jetson nano) and it’s using the raspberry pi v2 camera attached on my jetson nano. ** i had tested out the solution to edit the gstcamera.cpp code but it can’t work (text files attached below are gstcamera.cpp, error message when launching program and mydetection python script could someone please provide me some well-detailed solutions? thank you so much!",question,question
800,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/800,How to run 3d module using different brand cameras?,"hi there, i'm really new to this. now i have 3 logitech cameras. is it possible to use them for 3d reconstruction? i notice the tutorial: copy any of the examples/tutorialuserpeople_max 1? 4. do you have a plan to add 3d reconstruction to python module? thank you in advance.",question,question
981,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/981,"identifier ""nullptr"" is undefined","when i try openpose building, i have an error message: **. make -l `nproc` compile.dir/layers/./cudageneratedlayer.cu.o /usr/lib/gcc/x86 please tell me how to solve this problem. cuda / 8.0 cudnn / 5.1",Error,question
875,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/875,"Openpose hand, face and body models not found due to wrong url in getModels.bat","i'm trying to execute the cpu binaries of openpose on windows 10 using the file video.avi provided in the folder examples. i get an error message telling me that caffe models are not found: caffe trained model file not found: c:\users\salle\documents\openpose-1.4.0-win64-cpu-binaries\models\pose/bodyiter_584000.caffemodel. then the error message goes: possible causes : 1. not downloading the openpose trained models 2. not running openpose from the same directory where the model folder is located 3. using paths with spaces i make sure to specify the path to the models directory and i don't have spaces in my paths. the problem is the first point : the trained models aren't downloaded. these should be downloaded by running the file getmodels.bat. but when i do that i get this error: --2018-10-11 09:52:44-- (try: 2) connecting to posefs1.perception.cs.cmu.edu (posefs1.perception.cs.cmu.edu)128.2.176.37:80... failed: unknown error. retrying. the thing is, the url is wrong, i.e such a page doesn't exist. where can i download this model? although i doubt it a lot, but can the problem by building the source code of the openpose library instead of using the executables?",question,question
1777,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1777,run './build/examples/openpose/openpose.bin --video examples/media/video.avi' error,"error is as following: auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. [libprotobuf error google/protobuf/messageparam.min, layer[0].clipproto.cpp:97] check failed: readprotofrombinaryfile(param25/pose584000.caffemodel **** aborted (core dumped)",other,question
99,https://github.com/dusty-nv/jetson-inference/issues/99,related to issue #84,"on a single class trained detectnet i am able to run detectnet-camera successfully on my jetson tx1 with a webcam. i also successfully trained on a multiple of 3 classes on my digits box and the tests ran fine there. however, when i port this multiclass model to my jetson tx1 i get the error thats popping up in issue #84, ie ""kernel weights has count 1 but 12288 was expected"" and i confirmed the digits box caffe version is 0.15 so not sure whats going on. i did have to customize the prototxt file on the digits side to accommodate the extra classes and the tests ran fine. full error message listed below. ubuntu@tegra-ubuntu:~/jetson-inference/build/aarch64/bin$ ./detectnet-camera beachtrash detectnet-camera args (2): 0 [./detectnet-camera] 1 [beachtrash] [gstreamer] initialized gstreamer, version 1.8.3.0 [gstreamer] gstreamer decoder pipeline string: v4l2src device=/dev/video0 ! video/x-raw, width=(int)1280, height=(int)720, format=rgb ! videoconvert ! video/x-raw, format=rgb ! videoconvert !appsink name=mysink detectnet-camera: successfully initialized video device height: 720 [gie] attempting to open cache file networks/beachtrash/snapshot2820.caffemodel.2.tensorcache [gie] cache file not found, profiling network model [gie] platform has fp16 support. [gie] loading networks/beachtrash/deploy.prototxt networks/beachtrash/snapshot2820.caffemodel [gie] retrieved output tensor 'coverage' [gie] retrieved output tensor 'bboxes' [gie] configuring cuda engine [gie] building cuda engine [gie] conv1/7x7reduce: kernel weights has count 1 but 4096 was expected [gie] conv2/3x3: kernel weights has count 1 but 110592 was expected [gie] inception3a/3x33a/3x3: kernel weights has count 1 but 110592 was expected [gie] inceptionreduce: kernel weights has count 1 but 3072 was expected [gie] inception3a/pool3b/1x1: kernel weights has count 1 but 32768 was expected [gie] inceptionreduce: kernel weights has count 1 but 32768 was expected [gie] inception3b/5x53b/5x5: kernel weights has count 1 but 76800 was expected [gie] inceptionproj: kernel weights has count 1 but 16384 was expected [gie] inception4a/3x34a/3x3: kernel weights has count 1 but 179712 was expected [gie] inceptionreduce: kernel weights has count 1 but 7680 was expected [gie] inception4a/pool4b/1x1: kernel weights has count 1 but 81920 was expected [gie] inceptionreduce: kernel weights has count 1 but 57344 was expected [gie] inception4b/5x54b/5x5: kernel weights has count 1 but 38400 was expected [gie] inceptionproj: kernel weights has count 1 but 32768 was expected [gie] inception4c/3x34c/3x3: kernel weights has count 1 but 294912 was expected [gie] inceptionreduce: kernel weights has count 1 but 12288 was expected [gie] inception4c/pool4d/1x1: kernel weights has count 1 but 57344 was expected [gie] inceptionreduce: kernel weights has count 1 but 73728 was expected [gie] inception4d/5x54d/5x5: kernel weights has count 1 but 51200 was expected [gie] inceptionproj: kernel weights has count 1 but 32768 was expected [gie] inception4e/3x34e/3x3: kernel weights has count 1 but 460800 was expected [gie] inceptionreduce: kernel weights has count 1 but 16896 was expected [gie] inception4e/pool5a/1x1: kernel weights has count 1 but 212992 was expected [gie] inceptionreduce: kernel weights has count 1 but 133120 was expected [gie] inception5a/5x55a/5x5: kernel weights has count 1 but 102400 was expected [gie] inceptionproj: kernel weights has count 1 but 106496 was expected [gie] inception5b/3x35b/3x3: kernel weights has count 1 but 663552 was expected [gie] inceptionreduce: kernel weights has count 1 but 39936 was expected [gie] inception5b/pooliter_2820.caffemodel detectnet -- failed to initialize. detectnet-camera: failed to initialize imagenet ubuntu@tegra-ubuntu:~/jetson-inference/build/aarch64/bin$",question,question
380,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/380,Ubuntu 16 libcudnn.so.5 not found,after run `bash ./ubuntu/installandif_cuda8.sh` . ** environment: - ubuntu 16 - cuda 8 - cudnn 5,deployment,other
874,https://github.com/dusty-nv/jetson-inference/issues/874,Memory issues after I loaded my models.,good day! my project runs on desktop and i see the following messages after i loaded all required models. please help me understand the problem. if i don't load the models all is ok.,question,question
1905,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1905,change the skeleton of ROI image to original image ,"i put the roi image into function emplaceandpop() . it was worked , but i want to output the skeleton on my original image. where can i change the image that i want to output the skeleton ?",question,question
1824,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1824,Unexpected behaviour when processing images using python API.,"issue summary i am using the python api (working off example 04from_images) to process frames from a video that i intend to stitch back together after processing. i expect (perhaps wrongly) that the images in the folder are processed in order as this seems to be the case, however there are certain images that appear to be consistently out of order. context i am putting this together to provide a demo for a potential product that would detect when someone is, for example touching their face, so ** api:",question,question
397,https://github.com/dusty-nv/jetson-inference/issues/397,tensorrt inference for yolov3-ssp.weight  for error,"try to inference for yolov3-ssp.weight with tensorrt on jeson nano ,cuda10.0 ,tensorrt5.1.5, cd deepstreamapps/yolo/ $trt-yolo-app --flagfile=config/yolov3-ssp.txt error: lagfile=config/yolov3-1.txt loading pre-trained weights... loading complete! total number of weights read : 63052381 (1) conv-bn-leaky 3 x 608 x 608 32 x 608 x 608 992 (2) conv-bn-leaky 32 x 608 x 608 64 x 304 x 304 19680 (3) conv-bn-leaky 64 x 304 x 304 32 x 304 x 304 21856 (4) conv-bn-leaky 32 x 304 x 304 64 x 304 x 304 40544 (5) skip 64 x 304 x 304 64 x 304 x 304 - (6) conv-bn-leaky 64 x 304 x 304 128 x 152 x 152 114784 (7) conv-bn-leaky 128 x 152 x 152 64 x 152 x 152 123232 (8) conv-bn-leaky 64 x 152 x 152 128 x 152 x 152 197472 (9) skip 128 x 152 x 152 128 x 152 x 152 - (10) conv-bn-leaky 128 x 152 x 152 64 x 152 x 152 205920 (11) conv-bn-leaky 64 x 152 x 152 128 x 152 x 152 280160 (12) skip 128 x 152 x 152 128 x 152 x 152 - (13) conv-bn-leaky 128 x 152 x 152 256 x 76 x 76 576096 (14) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 609376 (15) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 905312 (16) skip 256 x 76 x 76 256 x 76 x 76 - (17) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 938592 (18) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 1234528 (19) skip 256 x 76 x 76 256 x 76 x 76 - (20) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 1267808 (21) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 1563744 (22) skip 256 x 76 x 76 256 x 76 x 76 - (23) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 1597024 (24) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 1892960 (25) skip 256 x 76 x 76 256 x 76 x 76 - (26) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 1926240 (27) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 2222176 (28) skip 256 x 76 x 76 256 x 76 x 76 - (29) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 2255456 (30) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 2551392 (31) skip 256 x 76 x 76 256 x 76 x 76 - (32) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 2584672 (33) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 2880608 (34) skip 256 x 76 x 76 256 x 76 x 76 - (35) conv-bn-leaky 256 x 76 x 76 128 x 76 x 76 2913888 (36) conv-bn-leaky 128 x 76 x 76 256 x 76 x 76 3209824 (37) skip 256 x 76 x 76 256 x 76 x 76 - (38) conv-bn-leaky 256 x 76 x 76 512 x 38 x 38 4391520 (39) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 4523616 (40) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 5705312 (41) skip 512 x 38 x 38 512 x 38 x 38 - (42) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 5837408 (43) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 7019104 (44) skip 512 x 38 x 38 512 x 38 x 38 - (45) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 7151200 (46) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 8332896 (47) skip 512 x 38 x 38 512 x 38 x 38 - (48) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 8464992 (49) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 9646688 (50) skip 512 x 38 x 38 512 x 38 x 38 - (51) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 9778784 (52) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 10960480 (53) skip 512 x 38 x 38 512 x 38 x 38 - (54) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 11092576 (55) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 12274272 (56) skip 512 x 38 x 38 512 x 38 x 38 - (57) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 12406368 (58) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 13588064 (59) skip 512 x 38 x 38 512 x 38 x 38 - (60) conv-bn-leaky 512 x 38 x 38 256 x 38 x 38 13720160 (61) conv-bn-leaky 256 x 38 x 38 512 x 38 x 38 14901856 (62) skip 512 x 38 x 38 512 x 38 x 38 - (63) conv-bn-leaky 512 x 38 x 38 1024 x 19 x 19 19624544 (64) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 20150880 (65) conv-bn-leaky 512 x 19 x 19 1024 x 19 x 19 24873568 (66) skip 1024 x 19 x 19 1024 x 19 x 19 - (67) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 25399904 (68) conv-bn-leaky 512 x 19 x 19 1024 x 19 x 19 30122592 (69) skip 1024 x 19 x 19 1024 x 19 x 19 - (70) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 30648928 (71) conv-bn-leaky 512 x 19 x 19 1024 x 19 x 19 35371616 (72) skip 1024 x 19 x 19 1024 x 19 x 19 - (73) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 35897952 (74) conv-bn-leaky 512 x 19 x 19 1024 x 19 x 19 40620640 (75) skip 1024 x 19 x 19 1024 x 19 x 19 - (76) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 41146976 (77) conv-bn-leaky 512 x 19 x 19 1024 x 19 x 19 45869664 (78) conv-bn-leaky 1024 x 19 x 19 512 x 19 x 19 46396000 (79) maxpool 512 x 19 x 19 512 x 15 x 15 46396000 (80) route - 512 x 19 x 19 46396000 (81) maxpool 512 x 19 x 19 512 x 11 x 11 46396000 (82) route - 512 x 19 x 19 46396000 (83) maxpool 512 x 19 x 19 512 x 7 x 7 46396000 error: routereferenceutils.cpp:302: std::__cxx11::string dimstostring(nvinfer1::dims): assertion `d.nbdims >= 1' failed. aborted (core dumped)",question,question
276,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/276,Jetson TX2 installation instructions,"hi, the instructions for tx2 installation is not accessible:",deployment,Error
665,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/665,configuration step in mac os,"hi, could you please help me understand the , for ? where is/how should i use the gui?",question,question
1102,https://github.com/dusty-nv/jetson-inference/issues/1102,Could not load VGG-16 SSD,"good afternoon, i am training a custom object detection model using vgg16-ssd, instead of mobilenet ssd for better accuracy. i have trained the model, but i could not load it. can you please help me out to load my model and run detection.",other,question
113,https://github.com/dusty-nv/jetson-inference/issues/113,Build From Source On Jetson (TX2) Fails With JetPack 3.1,"in section ""configuring with make"" in i get the following errors ... nvidia@tegra-ubuntu:~/jetson-inference/build$ cmake ../ cudaroottoolkitdir cudaexecutable cudadirs cudalibrary) -- system arch: aarch64 -- output path: /home/nvidia/jetson-inference/build/aarch64 -- copying /home/nvidia/jetson-inference/segnet.h -- copying /home/nvidia/jetson-inference/detectnet.h -- copying /home/nvidia/jetson-inference/tensornet.h -- copying /home/nvidia/jetson-inference/imagenet.h -- copying /home/nvidia/jetson-inference/util/commandline.h -- copying /home/nvidia/jetson-inference/util/loadimage.h -- copying /home/nvidia/jetson-inference/util/camera/gstcamera.h -- copying /home/nvidia/jetson-inference/util/camera/v4l2camera.h -- copying /home/nvidia/jetson-inference/util/camera/gstutility.h -- copying /home/nvidia/jetson-inference/util/cuda/cudaresize.h -- copying /home/nvidia/jetson-inference/util/cuda/cudaoverlay.h -- copying /home/nvidia/jetson-inference/util/cuda/cudanormalize.h -- copying /home/nvidia/jetson-inference/util/cuda/cudamappedmemory.h -- copying /home/nvidia/jetson-inference/util/cuda/cudargb.h -- copying /home/nvidia/jetson-inference/util/cuda/cudafont.h -- copying /home/nvidia/jetson-inference/util/cuda/cudautility.h -- copying /home/nvidia/jetson-inference/util/cuda/cudayuv.h -- copying /home/nvidia/jetson-inference/util/display/glutility.h -- copying /home/nvidia/jetson-inference/util/display/gldisplay.h -- copying /home/nvidia/jetson-inference/util/display/gltexture.h -- copying /home/nvidia/jetson-inference/data/images/orange0427.png -- copying /home/nvidia/jetson-inference/data/images/dogapple0428.png -- copying /home/nvidia/jetson-inference/data/images/peds-001.jpg -- copying /home/nvidia/jetson-inference/data/images/vehicle0255.png -- copying /home/nvidia/jetson-inference/data/images/vehicle0.jpg -- copying /home/nvidia/jetson-inference/data/images/vehicle0436.png -- copying /home/nvidia/jetson-inference/data/images/banana0.jpg -- copying /home/nvidia/jetson-inference/data/images/drone0.jpg -- copying /home/nvidia/jetson-inference/data/images/vehicle0.jpg -- copying /home/nvidia/jetson-inference/data/images/airplanesmith1.jpg -- copying /home/nvidia/jetson-inference/data/images/fontmapb.png -- copying /home/nvidia/jetson-inference/data/images/peds-005.png -- copying /home/nvidia/jetson-inference/data/images/dogsmithcudarttoolkit_include (advanced) used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-console used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/imagenet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-console used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/detectnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-console used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/segnet-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/gst-camera used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-console used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/util/camera/v4l2-display used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs used as include directory in directory /home/nvidia/jetson-inference/docs -- configuring incomplete, errors occurred! see also ""/home/nvidia/jetson-inference/build/cmakefiles/cmakeoutput.log"". see also ""/home/nvidia/jetson-inference/build/cmakefiles/cmakeerror.log"".",question,question
388,https://github.com/dusty-nv/jetson-inference/issues/388,Jetson TX2 Inference measurement,"hi, i am new to the deeplearning area. i have the jetson tx2 board purchased. i would like to run the inference applications for image classification and object detection on jetson tx2 board. i have set up the system, installed jetpack etc. i am trying to measure the throughput, accuracy, latency and confidence rate etc. for image classification i have executed the program as mentioned in the instructions: when i executed the program i see below results: device gpu, networks/resnet-50/resnet-50.caffemodel initialized. [trt] networks/resnet-50/resnet-50.caffemodel loaded imagenet -- loaded 1000 class info entries networks/resnet-50/resnet-50.caffemodel initialized. [image] loaded 'orange0.jpg' -> 99.80469% class #950 (orange) [trt] ---------------------------------------------- [trt] timing report networks/resnet-50/resnet-50.caffemodel [trt] ---------------------------------------------- [trt] pre-process cpu 0.07712ms cuda 0.48864ms [trt] network cpu 105.64045ms cuda 104.41174ms [trt] post-process cpu 1.95469ms cuda 2.86877ms [trt] total cpu 107.67226ms cuda 107.76915ms [trt] ---------------------------------------------- from the above results, it mentioned that the device is gpu. how do i know if these results are from tensorrt accelerator chip. how can i measure throughput, latency and accuracy or map from these results for the tensorrt accelerator? could someone please clarify urgently? susmitha",question,question
1000,https://github.com/dusty-nv/jetson-inference/issues/1000,nonfunction treshold during detectnet ,hi after creating my own dataset and trainning i tried to detect object and i still can see results under value 0.9 and bigger than 0.5 command: `detectnet --model=models/spider/ssd-mobilenet.onnx --labels=models/spider/labels.txt --treshold=0.9 --input-blob=input_0 --output-cvg=scores --output-bbox=boxes /dev/video0`,question,question
1453,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1453,Can you share the training code for hand model?,"hello! thank you for your great and general share! can you share the training code for ""lmdbdome"" and ""lmdbmpii"" dataset or other hand dataset? i really want to train the hand model but i can't allways get the caffe code. thank you for advance!",other,other
441,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/441,Get Pose from video camera on demand,"issue summary for my application, i converted the 1asynchronoususeroutput.cpp. it worked ok however has one issue: when calling openposetutorialwrapper1, it keeps running and block all other code. the main code of the application does not start and just wait the openposetutorialwrapper1 to finish then main code will start. what i would from the dll is two functions: 1) turn on the video camera 2) get pose information from video camera frame when being called from c#, after being called, it stops. so the main application and the openposetutorialwrapper1 could interactive with each other: when application needs pose data, get from openposetutorialwrapper1, at other case, openposetutorialwrapper1 does not run. could some one give me some hint how to do this? type of issue you might select multiple topics, delete the rest: - help wanted - question - enhancement / offering possible extensions / pull request / etc your system configuration my environment is windows 10 with vs 2015, gtx 1050. thank you very much! george",question,question
368,https://github.com/dusty-nv/jetson-inference/issues/368,Jetson TX2 Library Path not set/updated,"after building, i can see the so's copied over to /usr/local/lib/, however that directory is not in the library path.",question,Error
718,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/718,Calibration.bin not found,"issue summary: so i am trying to use the inbuild calibration toolbox, however i cannot seem to find the calibration.bin executed command (if any) note: add `--loggingmulti_thread` to get higher debug information. you might select multiple topics, delete the rest: - help wanted your system configuration 1. ** issue:",question,question
282,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/282,json,write coco json wrong,other,other
105,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/105,"Windows : Training an ""MPI_4_layers"" model.","issue summary i really like the results i got so far with openpose. thank you guys for developing this excellent tool. i figured out that a gray scale camera gets faster results (higher fps) than an identical color camera. but, openpose (with any trained model) will always give better poses using a color camera. i guess this is simply due to the fact that the trained models included with openpose are trained using color images. for that reason, i'm currently reading and trying to train a new model using the coco images, but converted to gray scale. i'm following the steps explained here : type of issue - help wanted - question my question is about the [quicker] mpi-4-layer trained model. the mentioned link before helps on how to train a coco model, but how do you train an mpi-4-layer model? thank you for you help, anthony your system configuration **: 3.2.0",Performance,question
1215,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1215,pose track,i want to use openpose in my project to get the key points of multiple people in the screen and get a tracking id by tracking to everyone. i don't know how to achieve it. i hope i can give some suggestions. thank you.,other,other
1142,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1142,Link broken in calibration_module.md,"link broken in in section ""installing the calibration module"" the following link doesnt work",Error,Error
722,https://github.com/dusty-nv/jetson-inference/issues/722,ModuleNotFoundError: No module named 'mhp_utils',"when i train my network, the error as follows. ~jetson-inference/python/training/segmentation$ python train.py --model-dir=lv-mhp-v1 ~/ai/lv-mhp-v1 pytorch-segmentation/datasets/_utils' ################# but i do not find any package named 'mhp_utils'. environment: python 3.6.9 (default, jul 17 2020, 12:50:27) cuda compilation tools, release 10.2, v10.2.89",question,question
1040,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1040,Question: does flip still work for video read-write? How does any bool flag work?,"issue summary i'm trying to process a video file and maintain the orientation. for some reason by default openposedemo flips the video horizontally (so a person on the left now appears on the right etc). i've tried the --framevideo 2.mp4 --display 2 --framevideo 2.mp4 --display 2 --frame_flip 1 etc.. for other attempts at trying to get flip to work. openpose output (if any) flipped (mirrored) video errors (if any) video is mirrored horizontally. type of issue - execution error - question your system configuration windows, compiled openposedemo.exe visual studio 2017. not sure any of that matters.",question,question
985,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/985,The speed I download the model through the getModels.bat is very very slow. How I can speed it up?,"i just want to try the window demo, but when i download the models through the getmodels.bat, the speed is very slow? how i can speed it up? or is there any other way to download it?such as google drive.",question,question
1443,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1443,Is openpose really able to track only the hand movement when only the hand is displayed in the video or web camera?,"hello everyone, i just want to know. is openpose really able to track only the hand movement when only the hand is displayed in the video or web camera? or openpose actually needs the whole body as video input in order to track down the hand from there. i have the feeling that their tracking mechanism always starts from the face or upper body and then continues to the upper arm and hand. i have this problem after trying hands only videos several times and openpose is not able to recognize the hands. but if i include my body in the video, then openpose can also recognize my hands (even fingers). i will be very happy about your answer, based on your experiences with openpose, because at the moment i can only run openpose with google colab and can only feed it with an input video and get the extracted video as a result. if not, then: is there another openpose-like project that only recognizes the hand and fingers? many thanks in advance.",question,question
1254,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1254,Crash in demo app when doing 3D reconstruction with two FLIR cameras (Windows),posting rules 1. ** system:,Error,other
337,https://github.com/dusty-nv/jetson-inference/issues/337,bash: ./imagenet-camera: No such file or directory,"beginner here, following the tutorial **** when i run ` ./imagenet-camera googlenet` i get this line > bash: ./imagenet-camera: no such file or directory any help?",question,question
781,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/781,"Could NOT find HDF5 (missing: HL) (found version ""1.8.9"")","issue summary when i build openpose, i always have the problem"" could not find hdf5 (missing: hl) (found version ""1.8.9"")"", even though i have install hdf5 lib in /usr/local/hdf5, and in /usr/local/lib also have hdf5. type of issue you might select multiple topics, delete the rest: - compilation/installation error - help wanted your system configuration 1. ** issue:",question,question
78,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/78,Segmentation fault (core dumped) when json file generation,"issue summary get ""segmentation fault (core dumped)"" when trying to output the json file. executed command (if any) ./build/examples/openpose/openpose.bin --video 1.avi --novideo result.avi --writejson a.json --face --keypointdisplay true --writekeypointscale 3 the error is gone, but it still doesn't work. but ./build/examples/openpose/openpose.bin --video 1.avi --novideo result.avi --writejson a.json --face --keypoint_scale 3 works perfect. openpose output (if any) libdc1394 error: failed to initialize libdc1394 starting pose estimation demo. examples/openpose/openpose.cpp:gflagstoopparameters():212 examples/openpose/openpose.cpp:gflagstoproducer():180 examples/openpose/openpose.cpp:gflagstoproducertype():160 examples/openpose/openpose.cpp:gflagtoposemodel():122 examples/openpose/openpose.cpp:gflagtoscalemode():138 configuring openpose wrapper. in examples/openpose/openpose.cpp:oprealtimeposedemo():263 ./include/openpose/wrapper/wrapper.hpp:configure():423 auto-detecting gpus... detected 1 gpu(s), using them all. ./include/openpose/wrapper/wrapper.hpp:configure():581 segmentation fault (core dumped) and starting pose estimation demo. examples/openpose/openpose.cpp:gflagstoopparameters():212 examples/openpose/openpose.cpp:gflagstoproducer():180 examples/openpose/openpose.cpp:gflagstoproducertype():160 examples/openpose/openpose.cpp:gflagtoposemodel():122 examples/openpose/openpose.cpp:gflagtoscalemode():138 configuring openpose wrapper. in examples/openpose/openpose.cpp:oprealtimeposedemo():263 ./include/openpose/wrapper/wrapper.hpp:configure():423 auto-detecting gpus... detected 1 gpu(s), using them all. ./include/openpose/wrapper/wrapper.hpp:configure():581 for the second line. type of issue - help wanted",question,question
387,https://github.com/dusty-nv/jetson-inference/issues/387,custom model  loaded on TX2 for inference failed,"i have a custom model which i have trained and tested in digits on host successfully. before downloading the model and transferring on jetson tx2 at the end of deploy.prototxt, deleted the layer named cluster: without this python layer, the snapshot can now be imported into tensorrt onboard the jetson. well my model detected objects perfectly in digits on host. but not sure what is causing the issue. @dusty-nv could you please take a look? thanks. though i am trying my best to solve this issue will update if anything. i also made sure i am naming all the files correctly in command line command",question,question
22,https://github.com/dusty-nv/jetson-inference/issues/22,Inferencing on desktop PASCAL GPU with TensorRT 1.0 RC on CUDA 8.0  ,"i have recently obtained and installed tensorrt 1.0rc on an early access program. however, this jetson-inference repo should work on pascal based gtx 1080 running cuda 8.0 with the gie provided by nvidia for demo. running 'make' command generates error: `error limit reached. 100 errors detected in the compilation of ""/tmp/tmpxft00000000-7generatedgenerated_cudaoverlay.cu.o ` what change should be made to run this code on desktop gpus other than tx1?",question,question
1615,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1615,did anybody know why crop affect PAF so much?,"issue summary as title. we can see in the first picture paf_y is very strong, but in crop picture is bad then full one. it's confuse me. thank you in advance. type of issue - question",question,question
1054,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1054,How to change the file name of the JSON output files (using Python API),issue summary i would like to change the output json file names to match the original image file names. where should i make the modification? openpose output (if any) type of issue - help wanted - question,question,question
65,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/65,DEMO Execution 0xc000007b error - Windows 7 & 10,"executed command (if any) .\bin\openposedemo.exe --video examples\media\video.avi openpose output (if any) it can not be executed type of issue - execution error your system configuration ** (`nvidia-smi`):gtx960 and gtx550ti (different pc, same error)",question,question
1687,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1687,OpenPoseDemo.exe Error Prototxt file not found:module\face/pose_deploy.prototxt ,i'm using a local caffe installation and i'm getting this error when i'm running the openpose command from outside of the openpose directory. this works: ./build/examples/openpose/openpose.bin --video video.mp4 --writejson output/ this doesn't work (outside of the openpose directory): /home/ubuntu/projects/openpose/build/examples/openpose/openpose.bin --video /home/ubuntu/projects/openpose/video.mp4 --writejson /home/ubuntu/projects/openpose/output/,question,question
1897,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1897,OpenPose 2D model output need clarification!!!!! URGENT!!,"i am working on a project: sequential ballet action correction. i am trying to find the movement of each skeleton joint to generate correctional feedback. in a ballet action called ""arabesque"" when i plot the graph for x, y results for joints 12, 13, 14, 19, 20, 21. the x value goes up and the y value goes down. but according to the action, both the value supposed to increase. why do i get the graph curves like this?.. am i missing anything here? this is how the graph looks like -> the generated json output is here ->",question,question
273,https://github.com/dusty-nv/jetson-inference/issues/273,Tutorial imagenet-console script fails,"i'm trying to run the first script in the tutorial, but i am getting an error. no custom code, just exactly as the tutorial says. running a jetson tx2. any help is appreciated.",question,question
1405,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1405,make failing with error: Undefined symbols for architecture x86_64 in in libcaffeproto.a(caffe.pb.cc.o),issue summary build failing with error: undefined symbols for architecture x86lib' [ 3%] built target caffeproto errors type of issue - compilation/installation error cmake logs before build,question,other
14,https://github.com/dusty-nv/jetson-inference/issues/14,How to Install on ubuntu 14.04 with custom host,"hi, i am try to install on my ubutu host. ubuntu 14.04 and cuda version 8.0 but it seen many issue, i reference the another issue ""build problems on ubuntu 14.04 host"" and he's cmakelists.txt but it still has error with uint32_t undefined and i put #include on the of cudaoverlay.cu and cudaoverlay.h it seen ok to run. but after it ,another question is coming. it say nvinfer.h : no such file or directory how to slove it thank you everyone!!",question,question
495,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/495,"Caffe_LIB-NOTFOUND , Caffe_Proto_LIB-NOTFOUND and CUDA_SDK_ROOT_DIR-NOTFOUND errors. Need proper installation steps","posting rules 1. **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? compiler (`gcc --version` in ubuntu):",question,question
1320,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1320,"Configuring incomplete, errors occurred!  ","cmake's configure and generate are complete, then sudo make -j `nproc` in the build directory will report the following error:（this is the cpu installation, not the gpu） -- boost version: 1.58.0 -- found the following boost libraries: -- system -- thread -- filesystem -- regex -- chrono -- date64-linux-gnu/libgflags.so) -- found glog (include: /usr/include, library: /usr/lib/x86libraries hdf5dirs) call stack (most recent call first): /usr/share/cmake-3.5/modules/findpackagehandlestandardargs.cmake:388 (failurepackagestandardpackage) cmakelists.txt:53 (include) see also ""/home/virtual-system/desktop/openpose-master/build/caffe/src/openposelib-build/cmakefiles/cmakeerror.log"". cmakefiles/openposelib-stamp/openposelib-stamp/openposelib.dir/all' failed make[1]: **** [all] error 2",question,question
559,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/559,Scaling Issues in 3D Reconstruction : Windows 10,"issue summary i am running the demo with 3d flag on. i am using two flir cameras and i am able to extract the 2d and 3d keypoints in a json file. however, the 3d reconstruction seems to be converging to a very small area. is there a reason behind scaling the x, y, and z coordinates differently in gui3d.cpp file? could that be the reason for unclear 3d rendering? i am trying to plot the points using another framework but it would be very helpful to see the 3d reconstruction for the live feed. type of issue issue with 3d display system configuration operating system : windows 10 installation mode: vs 2015 enterprise with update 3 build cuda version (cat /usr/local/cuda/version.txt in most cases):cuda 8 cudnn version: v 5.1 cmake version (cmake --version in ubuntu): 3.11.1 release or debug mode? (by defualt: release): release 3-d reconstruction module added? (by default: no): no gpu model (nvidia-smi in ubuntu): nvidia titan x caffe version: default from openpose if anyone knows how to fix this issue, please comment. thanks in advance!",Performance,question
1338,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1338,Cannot Write Video - Producer not available,"issue summary when writting video is attempted, openpose stops as the producer is not available. all other display functions and processing are working well. any suggestions for how to activate the producer or where i should be looking? executed command (if any) custom code for 3d processing from saved videos flags file and c++ code attached openpose output (if any) no output errors (if any) error: writting video is only available if the openpose producer is used (i.e. producersharedptr cannot be a nullptr). type of issue - help wanted your system configuration 2. ** issue:",question,question
970,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/970,Build failed because of error in CMakeLists.txt ; Ubuntu 16.04,system configuration cuda-8.0 cudnn - 5.1.10 ubuntu 16.04 openpose version,Error,deployment
253,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/253,some question about tests/pose_accuracy_coco_test.sh,"when i use this .sh to generate json file, i set '--frame_last‘ to 2644, 3559 and all val set, but i got the same ap accuracy 0.483 by using evaldemo.m in coco toobox /matlabapi. so, why i got the same ap accuracy with different val image numbers in testing? thank you very much",question,question
960,https://github.com/dusty-nv/jetson-inference/issues/960,Return Code in case pipeline stops,"i am calling the jetson infernece as a module but in case of any error, the pipeline just stops and does not return anything. is there a way to return something from the function which is calling the pipeline in case of eos. eg def module(input_urls):",question,question
344,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/344,can you release the training code for the models? thanks,"posting rules 1. **: pre-compiled `apt-get install libopencv-dev` (only ubuntu); openpose default (only windows); compiled from source? if so, 2.4.9, 2.4.12, 3.1, 3.2?; ...? compiler (`gcc --version` in ubuntu):",other,other
264,https://github.com/dusty-nv/jetson-inference/issues/264,I identify a picture and need 3 seconds.How can i  decrease time.,"hi,i used ./imagenet-console 1.jpg 2.jpg command to identifya picture and need 3 seconds.and i found that i need 2.8 seconds between networks xx loaded and cuda engine context initialized with 2 bindings.how can decrease time within one second.please help us check it,thanks.",Performance,question
1730,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1730,Check failed: error == cudaSuccess (48 vs. 0)  no kernel image is available for execution on the device *** Check failure stack trace: ,demo error libprotoc 3.6.1 cudnn7.5 cuda10.0 caffe do well i read all of document,deployment,question
1726,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1726,Docs misleading ,i didn't find this getplugins.bat anywhere in the given code. please help. the following doc is on this link,other,other
913,https://github.com/dusty-nv/jetson-inference/issues/913,python api not able to load custom ssd v2-lite ,"i have a custom model that i am able to run using command line like `detectnet --model=, ` but not able to load model using python api `jetson.inference.detectnet(, detection_threshold)' error - ""jetson.inference -- detectnet invalid built-in network was requested ('/home/abs/documents/onnx/mb2-ssd-lite.onnx')""",question,question
1319,https://github.com/dusty-nv/jetson-inference/issues/1319,Re-trained SSD-Mobilenet does not detect fruit,"i have been following the steps for re-training ssd-mobilenet on the fruit dataset from the github page and tutorial video on my `dustynv/jetson-inference:r32.5.0` docker container on a 4gb jetson nano. however, after i've re-trained the model on the fruit set and run it on the fruit pictures in `/jetson-inference/data/images` none of the pictures have the bounding box, label, or confidence number. i noticed in the tutorial video that dusty's terminal looks like this when he runs his model after training it on the fruit: however, my terminal does not have the lines in the red `[`, only `[image] loaded /jetson-inference/data/images/fruiti.jpg`. therefore, it appears to me that it isn't detecting any fruit at all. i have tried re-training the model, re-downloading the fruit dataset and re-training the model on that, training the model on 5 epochs instead of 1, and cloning a new container and training the model on the fruit data works. however, nothing has fixed this issue of the fruit not being detected. i copied the commands from the github page so it is unlikely that the command is wrong. i didn't have an issue with the bounding boxes on the previous tutorial with the cats & dogs and objects. what else can i try to fix this issue?",question,question
746,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/746,I have error in CMake configure,"cmake error: the following variables are used in this project, but they are set to notfound. please set them or make sure they are set and tested correctly in the cmake files: cudalibrary (advanced) cudainclude (advanced) used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/src/openpose used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/calibration used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/openpose used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialmodule used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialpose used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialthread used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialthread used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialwrapper used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialwrapper used as include directory in directory c:/users/team elysium/desktop/openpose/openpose/examples/tutorialwrapper configuring incomplete, errors occurred! see also ""c:/users/team elysium/desktop/openpose/openpose/build/cmakefiles/cmakeoutput.log"". ---------------------------------------------------------------------------------------------------------------- my system spec ~ os : windows 10 graphic card : geforce 1050 visual studio : 2015 enterprise update 3 cuda : cudawindows cudnn : cudnn-8.0-windows10-x64-v5.1 cmake : cmake-3.12.0-win64-x64 i think i installed everything. but i blocked in cmake configureing... i need your help. i got these errors. is there any solution ? i could not find anything on docs and issue.. if you know other issues of this error , mention at below. thanks",question,question
782,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/782,Using BODY_25 with demo,"issue summary i have installed and run the openposedemo, but the only models downloaded were coco and mpi. how would i go about using body_25? will i need to install the program and change the code or is there an easier way to do this with the demo? version: 1.3.0 windows",question,question
1460,https://github.com/dusty-nv/jetson-inference/issues/1460,Anybody make it past Posenet on jetson nano 4GB? How about the FPS?,"i have tried trt_pose on nano,which is 10fps. i want to know the fps about posenet. thanks！！",other,question
1404,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1404,Hyperparameters for hand pose,"issue summary i am trying to apply openpose for hand pose detection in the following settings: - the head of the recorded subject is not visible - the hand is close to the camera, which means that its proportion to the body is not standard. in other words, the subject is close to the camera and brings its hand forward towards the camera, so the hand looks bigger. i am attaching an image that exemplifies the settings. given these settings, i am using the command: ** i would like to optimise the hand pose performance for this specific application, so i am wondering how i could improve the hyperparameters selection ( --handnumber, --handrange). is there a guideline explaining the function and range of these hyperparameters? i am also wondering... do the values of these hyperparameters affect the runtime? if yes, how? finally, i noticed that openpose struggles estimating the pose of hands close to the camera. is this due to the lack of this type of images in the training dataset used to train openpose or is it consequence of a bad hyperparams choice? i would be grateful for any support. type of issue - help wanted - question - hyperparameters",Performance,question
1773,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1773,Minor error in Quick Start - Face and Hands section,"minor error in quick start - face and hands section in the `quick start - face and hands` section, it says: the `\` should be `/` under ubuntu and mac. ps: thanks for maintaining the repo so well!",Error,other
202,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/202,protobuf 3.3 not support?,"issue summary executed command (if any) sudo ./build/examples/openpose/openpose.bin --logging64-linux-gnu/src/protobuf/mir64-linux-gnu/src/protobuf/mir_protobuf.pb.cc"".) type of issue - execution error your system configuration **: opencv 3.x. generation mode (only for ubuntu): makefile + makefile.config (default, ubuntu) compiler (`gcc --version` in ubuntu): gcc (gcc) 4.8.5",question,question
1318,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1318,the theory about the Accurate Peak Position？,type of issue - help wanted - question i have a question about the function of nmscpu after get the peak score in the heatmap，why it need accurate peak position？the peak soce in the featuremap which from vgg16 is not right？ thanks,question,question
753,https://github.com/dusty-nv/jetson-inference/issues/753,Problems cloning jetson--interference,"hi, i'm having problems cloning jetson-inference: desktop:~$ git clone --recursive cloning into 'jetson-inference'... remote: enumerating objects: 164, done. remote: counting objects: 100% (164/164), done. remote: compressing objects: 100% (108/108), done. error: rpc failed; curl 56 gnutls recv error (-9): a tls packet with unexpected length was received. fatal: the remote end hung up unexpectedly fatal: early eof fatal: index-pack failed does anyone have similar problems?",question,question
1736,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1736,Implementing Openpose images output model on Google Colab: Can anyone share a working colab notebook,"i have been taking the help of the google colab helper script, and tried all the installation methods , reinstalling different versions of cuda, cmake, etc., but in the notebook there are still configuration errors. i tried the solutions mentioned in the script, but they don't seem to work. if anyone could share a colab notebook, it would be great!",other,other
1031,https://github.com/dusty-nv/jetson-inference/issues/1031,jetson.inference.detectNet waiting time of 5 minutes,"hey there, we use the following line of code `self.net = jetson.inference.detectnet(""ssd-mobilenet-v2"", threshold=0.5)` for object detection and it takes about 5 minutes to load. is there a way to speed up the process or to preload the model if we start the robot? we want to use it in a path finding problem and every time we restart the kernal of the notebook we need to wait for about 5 minutes.",question,question
89,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/89,Compilation error with opencv 3,"issue summary compilation error with the new face and hand tracking version, with opencv 3. executed command (if any) ./installrelease -a` on ubuntu): ubuntu 16.04 **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. compiler (`gcc --version` on ubuntu):",deployment,other
1842,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1842,[Question] Where is source code of thesis formula 11,i am student. i read openpose thesis. i want to change formula 11 on thesis. where should i change source code?,other,question
783,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/783,Order of handKeypoints when not all keypoints found ,"issue summary with ` const auto numberhandparts = handkeypoints[0].getsize(1);` so if one wants to iterate over all they points a loop going from 0 to 20 will be enough? or will the size decrease due to some key points not found - and if so, how can one know the corresponding elements (like 6 elements were found, how do i know where these 6 are located, if there is no fixed size) type of issue - help wanted - question",question,question
1585,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1585,Where is the no keypoints heatmap but pafs heatmap human pose model?,"hi,in 2019 openpose paper, there is no keypoint heatmaps in the human pose model but only pafs heatmaps .i see in the models file, there are only old models. where can i find the network? thinks!",question,question
1677,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1677,Error downloading the models from the .bat file,"i tried to run the 1.4.0 version on windows 10 and followed the instructions txt but when i double clicked on the getmodels.bat i get this message in a loop. what could be the reason? ----- downloading body pose (coco and mpi), face and hand models ----- ------------------------- pose models ------------------------- body (body_25) --2020-08-23 13:20:07-- resolving posefs1.perception.cs.cmu.edu (posefs1.perception.cs.cmu.edu)... 128.2.176.37 connecting to posefs1.perception.cs.cmu.edu (posefs1.perception.cs.cmu.edu)128.2.176.37:80... failed: unknown error. retrying.",question,question
507,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/507,error: /usr/local/lib/libcurl.so.4: no version information available,wanggw@liutl:~/work/openpose/build$ make -j`nproc` [100%] built target handfromjsontest.bin /usr/bin/cmake: /usr/local/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake) wanggw@liutl:~/work/openpose/build$ locate libcurl.so.4 /home/chenm/anaconda2/lib/libcurl.so.4 /home/chenm/anaconda2/lib/libcurl.so.4.5.0 /home/chenm/anaconda2/pkgs/libcurl-7.58.0-h1ad7b7a0/lib/libcurl.so.4.5.0 /home/dongxd/anaconda2/lib/libcurl.so.4 /home/dongxd/anaconda2/lib/libcurl.so.4.4.0 /home/dongxd/anaconda2/pkgs/curl-7.55.1-hcb0b3142/lib/libcurl.so.4.4.0 /home/liutl/anaconda/lib/libcurl.so.4 /home/liutl/anaconda/lib/libcurl.so.4.3.0 /home/liutl/anaconda/pkgs/curl-7.38.0-0/lib/libcurl.so.4 /home/liutl/anaconda/pkgs/curl-7.38.0-0/lib/libcurl.so.4.3.0 /home/pinga/anaconda3/lib/libcurl.so.4 /home/pinga/anaconda3/lib/libcurl.so.4.4.0 /home/pinga/anaconda3/pkgs/curl-7.55.1-hcb0b3142/lib/libcurl.so.4.4.0 /home/wanggw/anaconda2/lib/libcurl.so.4 /home/wanggw/anaconda2/lib/libcurl.so.4.4.0 /home/wanggw/anaconda2/pkgs/curl-7.55.1-hcb0b3142/lib/libcurl.so.4.4.0 /home/wangj/anaconda2/lib/libcurl.so.4 /home/wangj/anaconda2/lib/libcurl.so.4.4.0 /home/wangj/anaconda2/pkgs/curl-7.49.0-0/lib/libcurl.so.4 /home/wangj/anaconda2/pkgs/curl-7.49.0-0/lib/libcurl.so.4.4.0 /home/wanjw/anaconda2/lib/libcurl.so.4 /home/wanjw/anaconda2/lib/libcurl.so.4.4.0 /home/wanjw/anaconda2/pkgs/curl-7.55.1-hcb0b3142/lib/libcurl.so.4.4.0 /home/wanjw/anaconda3/lib/libcurl.so.4 /home/wanjw/anaconda3/lib/libcurl.so.4.4.0 /home/wanjw/anaconda3/pkgs/curl-7.55.1-hcb0b3142/lib/libcurl.so.4.4.0 /usr/lib/x8664-linux-gnu/libcurl.so.4.4.0 /usr/local/matlab/r2013b/bin/glnxa64/libcurl.so.4 /usr/local/matlab/r2013b/bin/glnxa64/libcurl.so.4.2.0 /usr/local/matlab/r2015b/bin/glnxa64/libcurl.so.4 /usr/local/matlab/r2015b/bin/glnxa64/libcurl.so.4.3.0 /usr/local/matlab/r2015b/toolbox/compilerclients/c/glnxa64/lib/libcurl.so.4 /usr/local/matlab/r2015b/toolbox/compilerclients/c/glnxa64/lib/libcurl.so.4.3.0 /usr/local/lib/libcurl.so.4 wanggw@liutl:~/work/openpose/build$ ls -l /usr/local/lib/libcurl.so.4 lrwxrwxrwx 1 root root 37 5月 21 2015 /usr/local/lib/libcurl.so.4 -> /home/liutl/anaconda/lib/libcurl.so.4 wanggw@liutl:~/work/openpose/build$ cmake -version cmake: /usr/local/lib/libcurl.so.4: no version information available (required by cmake) cmake version 3.5.1 cmake suite maintained and supported by kitware (kitware.com/cmake). wanggw@liutl:~/work/openpose/build$,question,other
222,https://github.com/dusty-nv/jetson-inference/issues/222,How to compile for desktop GPUs ?,"are there any cmakesfiles for compiling dusty interference for desktops gpus because with my 1080 ti i am getting invalid device function something like that: [cuda] cudagetlasterror() [cuda] invalid device function (error 8) (hex 0x08) [cuda] /home/ddk/ddk-repo/jetson-inference/imagenet.cu:50 [cuda] cudapreimagenet((float4*)rgba, width, height, minputcuda, mwidth, mheight, make_float3(104.0069879317889f, 116.66876761696767f, 122.6789143406786f)) [cuda] invalid device function (error 8) (hex 0x08) [cuda] /home/ddk/ddk-repo/jetson-inference/imagenet.cpp:146 any guess how to adapt the cmakefile ?",question,question
82,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/82,"Compiling error - Ubuntu  - cannot convert ‘std::array<unsigned int, 6ul>’ to ‘unsigned int’","issue summary when i compile openpose with hand added source, i get compiling errors. executed command (if any) in both way, ./make all -j8 ./installandmrelease/src/openpose/hand/handdetector.o] error 1 make: ***:opencv 2.4.13 compiler (`gcc --version` on ubuntu): gcc 4.8.4",Error,question
1309,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1309,Not able to Integrate OpenPose in other projects in Windows,"issue summary i am unable to use openpose in other external project in windows. the instructions to for making changes in cmake file are only applicable for ubuntu/mac. i get this error when i make the same changes in the link above this can be done only ubuntu and not on windows. to install the openpose headers and libraries into the system environment path (e.g., /usr/local/ or /usr/), run the following command. cd build/ sudo make install what changes do i make to cmake to be able to use openpose in other projects in windows? errors (if any) by not providing ""findopenpose.cmake"" in cmakepath this project has asked cmake to find a package configuration file provided by ""openpose"", but cmake did not find one. of the following names: you might select multiple topics, delete the rest: - help wanted - question your system configuration 1. ** system:",question,question
230,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/230,how to computer the mean average precision ?,"hi,i try to drop the net resolution to raise the fps. and then i use the mpii human pose dataset mentioned in your article to check out how much the map drop.on the website: i mess around by the readme. where can i get the rect ? i search on the web for a day, and get little effective information. can you give me some suggestion ? thx",question,question
1221,https://github.com/dusty-nv/jetson-inference/issues/1221,Collecting your own Detection Datasets automatically,"hi, just curious about if could be possible to automatically collect the objects for the datasets instead of take 100 pictures where each one i have to adjust the rounding box manually and save it. i mean, should be easy to paint a box around some object different from background and assign the label you decide for this set of pictures automatically. maybe anyone have done it, just don't want to reinvent the wheel.",other,other
211,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/211,Problem installing openpose on windows 10 using visual studio 17,issue summary i tried to build openpose on visual studio 17. i followed all the steps of installation as provided in the doc/installation.md. when i try to build the project i get the following error: severity code description project file line suppression state error lnk1181 cannot open input file '..\x64\release\openpose.lib' openposedemo executed command (if any) build openposedemo on vs 2017 openpose output (if any) none type of issue compilation/installation error help wanted your system configuration windows 10 cuda: v8.0 cudnn version:5.1 **: default from openpose (windows) visual studio (windows): visual studio 17,question,question
271,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/271,How to use OpenPose in QtCreator,could you please show me your .pro file? i want to use openpose in qt project. thanks!,question,question
230,https://github.com/dusty-nv/jetson-inference/issues/230,error while compiling CMakelists.txt,"cmakerequired(version 2.8) project(tensorrt-sample) set(cudastaticruntime off) set(cmakeflags ""${cmakeflags} -std=c++11"") set( cudaflags ${cudaflags}; -o3 -gencode arch=compute53 -gencode arch=compute62 ) finddirectories(${opencvdirs} /usr/local/cuda/include /usr/include/aarch64-linux-gnu) linkexecutable(samplelinkfasterrcnn nvcaffeplugin nvinfer ${opencvfasterrcnn.dir/samplefasterrcnn.cpp.o: undefined reference to symbol 'cudafree@@libcudart.so.9.0' /usr/local/cuda/lib64/libcudart.so.9.0: error adding symbols: dso missing from command line collect2: error: ld returned 1 exit status cmakefiles/samplefasterrcnn' failed make[2]: ***** [all] error 2 =========================================================================",question,question
28,https://github.com/dusty-nv/jetson-inference/issues/28,Does TensorRT support Batch Normalization?,"hi, i have re-trained detectnet with batch normalization layers , but i failed to run tensorrt over this caffe model. when load the model file, here shows some error infos: `message type ""diccaffe.batchnormparameter"" has no filed name ""scalenorm_param filed in the deploy file, it shows another error info: `caffeparser.cpp:613: bool bnconvert(const nvinfer1::weights&, const nvinfer1::weights&, const nvinfer1::weights&, float, nvinfer1::weights&, nvinfer1::weights&, std::vector&) [with t = float]: assertion `mean.count == variance.count && movingaverage.count == 1' failed.` it will be appreciated if you can give me some advice.",question,question
1061,https://github.com/dusty-nv/jetson-inference/issues/1061,Error while loading shared libraries in docker container,"hi! i tried to run segnet-console from docker image, but i got error about loading shared libraries, like: docker image that i use: `dustynv/jetson-inference:r32.4.4` output of jetson_release:",question,question
333,https://github.com/dusty-nv/jetson-inference/issues/333,Slow Jetson nano,"is there anyone who can explain why jetson nano is so slow? i have wrote the code below. i have a ip camera which supports rtsp. when i run below python3 code, the video stream on my nano is look so good. it seems like slow motion video. targetcam = 'rtsp://windowratio), int(windowheight*windowratio)) cap = cv2.videocapture(targetcam) cv2.namedwindow('main', cv2.window_normal) cv2.resizewindow('main',windowsize) while(cap.isopened()): cap.release() cv2.destroyallwindows() rtsp spec. : 2560 x 1920 @ 30 fps w/ h.264 this rtsp server works well. i have tested under vlc player and run the same code above with my laptop.",other,question
116,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/116,How to import/use trained model in OpenPose from Realtime_Multi-Person_Pose_Estimation repo,"i used the training code from the full project repo ( to generate new models (coco) with less stages. to apply them in openpose, i changed the "".caffemodel"" file in ""./models/pose/coco"" into my trained model, but i don't know what to do with the ""poselinevec.prototxt"" file. is it the same as the ""posepose.sh""? i tried to use that for experiments, but i received the following error: the output blob is a nullptr. did you use the same name than the prototxt? (used: net_output).",question,question
1345,https://github.com/dusty-nv/jetson-inference/issues/1345,"Jetson Xavier AGX Dev kit no internet, no bluetooth","hello, a quick question. i bought a jetson xavier agx dev kit and i did not use it for about 2-3 months. now, i try to use it and to my surprise, os and jetpack was already installed, so no need to flash it. moreover, on startup, i see a line that says wifi and bluetooth script init failed. and ethernet connection is also not working properly hence i do not have any internet on my board. thank you!",other,other
1134,https://github.com/dusty-nv/jetson-inference/issues/1134,custom detectnet model. LPD_net from Nvidia,"hi, i am trying to run the license plate detection model from nvidias . i've converted the .tlt to a tensorrt engine using tltengine.engine --output-bbox=boxes img.png output.png ` the resulting error : from the documentation for lpdv2 detector with resnet18 as feature extractor. is this a supported model configuration out of the box?",other,question
968,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/968,Is Jetson TX2 installation possible without flashing it with Jetpack,"hello, i am having just one machine (which is jetson tx2) and as per installation instructions, it is mentioned that i am supposed to flash my jetson with jetpack. unfortunately, jetson cannot flash itself. is there any workaround possible here?",question,question
171,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/171,the code seems works well with '.jpg' images but not '.JPG',"issue summary my system is win7 x64, the code seems works well with '.jpg' images but not '.jpg' ,is there something i used not correctly or it is just what it is.",Error,question
1835,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1835,windows10  did not respond,ps c:\openpose_cpu\openpose> bin\openposedemo.exe --video examples\media\video.avi ....................there is no output 。。。。。。。。。。。。。。。。。there is no output do you need to install any more environments ？,question,question
30,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/30,"The system runs with low efficiency, and  is only 6.5fps, any what should I modify？",issue summary the fps is only +=6.5 executed command (if any) ./build/examples/openpose/openpose.bin --video examples/media/video.avi type of issue - execution error - help wanted your system configuration ** (`nvidia-smi`): nvidia-smi tue may 16 15:02:50 2017 +-----------------------------------------------------------------------------+ nvidia-smi 375.26 driver version: 375.26 -------------------------------+----------------------+----------------------+ gpu name persistence-m bus-id disp.a volatile uncorr. ecc fan temp perf pwr:usage/cap memory-usage gpu-util compute m. ===============================+======================+====================== 0 geforce gtx 980 off 0000:01:00.0 on n/a 36% 65c p2 185w / 180w 1514mib / 4032mib 95% default +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ processes: gpu memory gpu pid type process name usage ============================================================================= +-----------------------------------------------------------------------------+ compiler (`gcc --version` on ubuntu): gcc (ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4,question,question
672,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/672,3d tracking output as fbx format?,"i am working on a pipeline to combine skeleton tracking with some 3rd-party optimization algorithm, which will only take fbx format. so i am wondering if there is a way that the 3d skeleton tracking results can be achieved from openpose and saved as fbx format. any idea will be appreciated.",other,question
924,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/924,Install Openpose on Ubuntu 16.04 LTS Server,"hi, i'm trying to install openpose on ubuntu server 16.04 lts, but with poor results. the virtual machine has these features: 8 cpu (4 sockets, 2 cores, intel xeon d-1520), 20 gb ram, 500 giga free space, no gpu. i followed all that is written in the installation guide via cli, i tried new installations and configurations even recreating the virtual machine, but nothing. i report in principle the sequence of instructions i use for the installation. i state that at the moment i would use only the demo to process images git clone openpose cd git submodule init git submodule update sudo ubuntu/installcmake.sh sudo apt-get install libopencv-dev sudo mkdir build cd build option 2) sudo cmake .. - for both options, after executing the command to launch the demo, or freezes with ""webcam not found"" (if i install cudnn) or after a while it interrupts the process. sudo make -j`nproc` cd .. ./build/examples/openpose/openpose.bin -imagejson output / --display 0 --renderresolution and other things described in ""speed ​​up, memory reduction, and benchmark"", but nothing… solutions to avoid interrupting the process?",question,question
203,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/203,Maximum accuracy configuration query,"@gineshidalgo99 i'm using openpose for body pose detection on a 4k video, and have a few questions, would be great if you answered them: 1. the maximum accuracy configuration is {netnumber 4 and scaleresolution should be closest possible to aspect ratio of the input video. so what does the netnumber to 3 and got it running, but was getting some noise (it could detect more people than default configs, but also had more erroneous detections). would you recommend reducing the resolution or the other parameters to get the next best accuracy i.e what should affect the accuracy more? 3. you haven't really mentioned what net_resolution means in the documentation, except that increasing it increases accuracy. is it the resolution the video frames are downscaled to before feeding into the net or something? unfortunately, i can't really show the videos i'm using or the openpose output at this point because our data is classified. your system configuration **: installed with `apt-get install libopencv-dev` (ubuntu) compiler (`gcc --version` in ubuntu): 5.4.0",Performance,question
1467,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1467,some questions about detecting hand alone,when i run 07frompose 0`. you could also set mthreadmanagermode = mthreadmanagermode::asynchronous(out) and/or add your own output worker class before calling this function. thanks for your help,question,question
526,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/526,Which parameter controls GPU memory consumption?,"hello,i am using pyopenpose with below settings: under my laptop with a nvidia ** gpu memory, where does this big memory consumption comef from?any parameter to specify gpu memory consumption?",question,question
751,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/751,"Click noise when running openpose, harddisk read/write ?",issue summary i'm running openpose on a alienware-like laptop with ssd and nvidia 1080 gpu. my laptop makes clicking noise when and only when i run openpose. it might come from hard disk. i'm just wondering whether there's a heavy harddisk read/write in openpose? thanks executed command (if any) demo openpose output (if any) normal type of issue - other (type your own type) your system configuration 1. ** issue: no,question,question
1610,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1610,Performance drop in docker(GPU),"issue summary you might select multiple topics, delete the rest: - help wanted - question your system configuration 2. ** issue:",question,other
324,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/324,"GTX980,  Cuda check failed (9 vs. 0)  invalid configuration argument",issue summary when run the openpose.bin to get the keylevel 0 --disablethread` to get higher debug information. openpose output (if any) error: cuda check failed (9 vs. 0): invalid configuration argument coming from: - src/openpose/pose/renderpose.cu:renderposekeypointsgpu():408 - src/openpose/utilities/cuda.cpp:cudacheck():36 - src/openpose/pose/renderpose.cu:renderposekeypointsgpu():413 - src/openpose/pose/posegpurenderer.cpp:renderpose():168 - ./include/openpose/pose/wposerenderer.hpp:work():74 - ./include/openpose/thread/subthread.hpp:worktworkers():135 - ./include/openpose/thread/subthreadqueueinout.hpp:work():84 - ./include/openpose/thread/thread.hpp:threadfunction():203 aborted (core dumped) type of issue - execution error your system configuration **: default 2.4.9 compiler (`gcc --version` in ubuntu): 5.4.0,deployment,question
133,https://github.com/dusty-nv/jetson-inference/issues/133,detectnet - ros publisher,i want to know how to use detectnet to publish detected images in a ros message. i am not familiar with cpp. thank you for your help. // convert from yuv to rgba void camera->getheight() 3; uint32img_pub.publish(img); `,question,other
214,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/214,Running OpenposeDemo have a display problem in desktop,"recently, i was running openposedemo in vs2015. it was able to normal run as following. however, the gui is not maximize display in desktop. i don't know why it is. i need help. thanks!",question,other
1452,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1452,Guide to run as Restful API.,i am having difficulty in running this package as a webservice. would appreciate if we could provide any kind of documentation on implementing an api to get the keypoints from an image. our aim is to able to deploy this api as an azure function and also know if it is feasible.,other,other
674,https://github.com/dusty-nv/jetson-inference/issues/674,own detection datasets -Error  too many indices ,"dear, thank you for the tutorial. i successfully done mosto of the exaples that you procided. now i am on and i keep getting an error. so what i've done, 1. i created my own dataset as you described in the tutorial, 2. i run python3 ../trainssd.py "") ** 3. i tried to change the folder - i thought this is the problem 4. i tried to copy the pictures to the test/train/val folders (originally it is in jpegimages only) 5. i've seen some problems in the forum but it seams the other problems also gives too many indices but for other reason. 6. as you've seen i haven't use specific model - it got a new one. 7. i tried to avoid doing changes in the code... please help... the error that i got is : > 2020-08-08 15:50:22 - start training from epoch 0. /home/michal/.local/lib/python3.6/site-packages/torch/optim/lrscheduler.step()` before `optimizer.step()`. in pytorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lroptions) /home/michal/jetson-inference/python/training/detection/ssd/vision/transforms/transforms.py:247: visibledeprecationwarning: creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. if you meant to do this, you must specify 'dtype=object' when creating the ndarray mode = random.choice(self.samplereduction.py:44: userwarning: sizessd.py"", line 343, in file ""../trainnextprocessutils.py"", line 395, in reraise indexerror: caught indexerror in dataloader worker process 1. original traceback (most recent call last): file ""/home/michal/.local/lib/python3.6/site-packages/torch/utils/data/workerutils/fetch.py"", line 44, in fetch file ""/home/michal/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 64, in _preprocessing.py"", line 34, in __ indexerror: too many indices for array: array is 1-dimensional, but 2 were indexed segmentation fault (core dumped)",Error,question
1116,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1116,"Check failed: ReadProtoFromBinaryFile(param_file, param) Failed to parse NetParameter file","starting openpose python wrapper... auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. f0304 22:21:56.302070 8548 upgradefile, param) failed to parse netparameter file: e:\graduatelib\models\hand/pose102000.caffemodel **** process finished with exit code -1073740791 (0xc0000409) can anyone tell me how to fix it?",question,question
1201,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1201,one question about detail of training  BODY_25 ,"i want to know ,when u trained body^",question,question
1156,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1156,Custom output window size,"hello there, i want to resize the output im getting from my webcam. i'm using zed stereo camera and i want to edit code so i can get the output for the 1 camera only. as you can see from the image below, i want to edit the width of the output so i can get only the output of the left camera. so i could just simply edit the output width of the window down to half? which file should i edit?",question,question
437,https://github.com/dusty-nv/jetson-inference/issues/437,How to download segnet manually？,"i try to download segnet model manually (i cannot connect to nvidia.box.com), however, there is only alexnet related fcn : i tried the alexnet realted fcn: however there comes the error: therefore, i want use the default resnet realted fcn. where can i manully down those models?",question,question
1429,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1429,hand action output,"-question/help -portable demo on windows 10 the software works well on my setup but now i want to do something with it. i am looking to print ""waving"" on screen when i wave my hand. i am pretty new to coding and i am looking for a place to start. any help or guidance would be greatly appreciated. i understand the hand key points tracking information found here( however, i am unsure how to integrate it to make something happen. should i be using the information found here:( or here:( i understand that after i write new code i need to recompile and to get the outcome for ""waving' plan to use the euclidean distance formula to track distance between hand key points. thank you",other,question
220,https://github.com/dusty-nv/jetson-inference/issues/220,Error when trying to use Inception-v3 model trained with Digits,"i am trying to use the inception-v3 model in place of googlenet or alexnet. i trained inception-v3 using digits, and now i am trying to deploy it on the jetson. when i run imagenet-camera like so: at first i was getting the following error: -- ** -- i upgraded the jetson using the latest jetpack and it now works.",question,question
1333,https://github.com/dusty-nv/jetson-inference/issues/1333,Segmentation Fault (Core Dumped) Error while training,"hi dusty, i am following your tutorial on training an object detector on the jetson nano. when i run the training command: it gives the following error: nothing else is printed to the screen at all, so i'm assuming it doesn't even begin the training process. i have searched online and in other issues here and have found no solution that works for me. i have a 6 gb swap file, so memory isn't the issue. i ran tegrastats while it was running and saw that memory was not an issue. i also tried disabling the desktop gui but no luck. i tried re-downloading the files from github using but the issue remained. i am using jetpack version 4.6 and python 3.6. using the docker container is not an option for me as it doesn't suit my project. i am at a loss and have run out of ideas. if you have any suggestions at all they would be greatly appreciated. thanks in advance, ciaran",question,question
1094,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1094,"Failed to run the exmple:""2_thread_user_input_processing_output_and_datum.bin""","issue summary failed to run the example:""2userprocessingandapilevel 0 openpose output (if any) starting openpose demo... starting thread(s)... /home/leo/openpose/openpose-master20190219/include/openpose/thread/threadmanager.hpp:exec():185 /home/leo/openpose/openpose-master20190219/include/openpose/thread/queuebase.hpp:addpusher():362 /home/leo/openpose/openpose-master20190219/include/openpose/thread/queuebase.hpp:addpusher():362 /home/leo/openpose/openpose-master20190219/include/openpose/thread/threadmanager.hpp:exec():190 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:startinthread():137 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:startinthread():137 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():182 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:initializationonthread():167 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():185 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():182 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:initializationonthread():167 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():185 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():182 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:initializationonthread():167 /home/leo/openpose/openpose-master20190219/include/openpose/thread/thread.hpp:threadfunction():185 type of issue - help wanted system configuration ubuntu18.04 x86_64 gpu geforce gtx 1060 6g driver version:415.25 opencv 3.4 cuda 9.2",deployment,question
776,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/776,Cannot Use Multiple GPU's using pyopenpose,"hi, i am using pyopenpose, currently there is option available for just single gpu through the flag ""numstart"", through this flag i can set the gpu id, but the problem is using single gpu the processing speed is slow is there a way that i can use multiple gpus ? if there is how can i use multiple gpu's. any suggestion would be appreciated. thanks is advance,",question,question
1252,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1252,Tons of error: /usr/bin/ld: cannot find -l<Libaray Name>,issue summary i get tons of error while compile openpose. the error as below: /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -lmvn.cpython-35m-x8664-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -lconvolve.cpython-35m-x8664-linux-gnu /usr/bin/ld: cannot find -lmultivariate.cpython-35m-x86quadpack.cpython-35m-x8664-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -lodeint64-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu /usr/bin/ld: cannot find -ldense.cpython-35m-x8664-linux-gnu /usr/bin/ld: cannot find -l64-linux-gnu ..... ..... (more than 200 similar errors as above) executed command (if any) 1: setting envs export path=/usr/local/cuda/bin:$path export ldpath=/usr/local/cuda/lib64:$ldpath export cpath=/usr/local/cuda/include:$cpath export cpath=/usr/local/include/opencv:/usr/local/include/opencv2:$cpath 2: cmake cmake -dopencvdirs=/usr/local/include/opencv -dopencvdir=/usr/local/lib/ ../openpose 3: make make -j 8 openpose output (if any) not apply errors (if any) errors as mentioned above. type of issue compilation/installation error your system configuration 1. system info distributor id: ubuntu description: ubuntu 16.04.6 lts release: 16.04 codename: xenial 2. ** issue:,other,question
1100,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1100,"make -j`nproc`  error,how to do?","ubuntu16.04 nvidia gtx1060 cuda 8.0 cudnn 5.1 ../../src/openpose/libopenpose.so.1.4.0: undefined reference to `cv::addweighted(cv::inputarray const&, double, double, cv::inputarray::kind() const' ../../src/openpose/libopenpose.so.1.4.0: undefined reference to `cv::videocapture::videocapture(cv::string const&)' collect2: error: ld returned 1 exit status examples/tutorialmodule/cmakefiles/1postaddcustomprocessing.bin' failed make[5]: ******** [all] error 2",question,question
187,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/187,Compili,"i run the installation script after all package fully downloaded i got such this errors . im noob in linux . plz guide me . `compilation terminated. makefile:478: recipe for target '.buildrelease/src/openpose/wrapper/definetemplates.o] error 1 in file included from src/openpose/core/netcaffe.cpp:4:0: ./include/openpose/core/netcaffe.hpp:5:25: fatal error: caffe/net.hpp: no such file or directory compilation terminated. makefile:478: recipe for target '.buildrelease/src/openpose/core/netcaffe.o] error 1 in file included from src/openpose/pose/bodypartconnectorcaffe.cpp:3:0: ./include/openpose/pose/bodypartconnectorcaffe.hpp:5:26: fatal error: caffe/blob.hpp: no such file or directory compilation terminated. makefile:478: recipe for target '.buildrelease/src/openpose/pose/bodypartconnectorcaffe.o] error 1 in file included from ./include/openpose/pose/headers.hpp:6:0, ./include/openpose/pose/bodypartconnectorcaffe.hpp:5:26: fatal error: caffe/blob.hpp: no such file or directory compilation terminated. makefile:478: recipe for target '.buildrelease/src/openpose/pose/definetemplates.o] error 1 `). ubuntu version 16.04 cuda 8 cudnn 5.1 opencv 2.4.1",question,question
1459,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1459,There's no video display,"hello! thanks for the sharing. i have finished all the steps of installation, but after i running ./build/examples/openpose/openpose.bin --video examples/media/video.avi it displays: starting openpose demo... configuring openpose... starting thread(s)... auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. openpose demo successfully finished. total time: 18.273847 seconds. and there is no video coming, what should i do? thank you!",question,question
1739,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1739,05_keypoints_from_images_multi_gpu.py,"i get ""segmentation fault (core dumped)"" despite other examples (e.g 01from64-linux-gnu/libcudnn.so) -- added cuda nvcc flags for: sm64-linux-gnu/libcudnn.so) -- found gflags (include: /usr/include, library: /usr/lib/x8664-linux-gnu/libglog.so) thanks",question,question
1814,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1814,CMAKE error: CUDA detected: 10.1 Added CUDA NVCC flags for: sm_75 cuDNN not found Could NOT find GFlags (missing: GFLAGS_INCLUDE_DIR GFLAGS_LIBRARY)  Could NOT find Glog (missing: GLOG_INCLUDE_DIR GLOG_LIBRARY)  CMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 (message):   Could NOT find Protobuf (missing: Protobuf_INCLUDE_DIR),"following the installation commands, i get the following error in configure step: ``` $ cat /home/mona/research/code/openpose/build/cmakefiles/cmakeoutput.log the system is: linux - 5.6.0-1036-oem - x8691955/fast && /usr/bin/make -f cmakefiles/cmtc91955.dir/build make[1]: entering directory '/home/mona/research/code/openpose/build/cmakefiles/cmaketmp' building c object cmakefiles/cmtc91955.dir/testccompiler.c.o -c /home/mona/research/code/openpose/build/cmakefiles/cmaketmp/testccompiler.c linking c executable cmtclink91955.dir/link.txt --verbose=1 /usr/bin/cc -rdynamic cmakefiles/cmtc91955 make[1]: leaving directory '/home/mona/research/code/openpose/build/cmakefiles/cmaketmp' detecting c compiler abi info compiled with the following output: change dir: /home/mona/research/code/openpose/build/cmakefiles/cmaketmp run build command(s):/usr/bin/make cmtc35340.dir/build.make cmakefiles/cmtc35340.dir/cmakeccompilerabi.c.o /usr/bin/cc -v -o cmakefiles/cmtcgcc=/usr/bin/cc offloadnames=nvptx-none:hsa offloaddefault=1 target: x8664-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-hskzea/gcc-9-9.3.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x8664-linux-gnu --target=x86gcc35340.dir/cmakeccompilerabi.c.o' '-c' '-mtune=generic' '-march=x86-64' /usr/lib/gcc/x8664-linux-gnu /usr/share/cmake-3.16/modules/cmakeccompilerabi.c -quiet -dumpbase cmakeccompilerabi.c -mtune=generic -march=x86-64 -auxbase-strip cmakefiles/cmtc64-linux-gnu) compiled by gnu c version 9.3.0, gmp version 6.2.0, mpfr version 4.0.2, mpc version 1.1.0, isl version isl-0.22.1-gmp ggc heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072 ignoring nonexistent directory ""/usr/local/include/x8664-linux-gnu/9/include-fixed"" ignoring nonexistent directory ""/usr/lib/gcc/x8664-linux-gnu/include"" #include ""..."" search starts here: #include search starts here: /usr/lib/gcc/x8664-linux-gnu /usr/include end of search list. gnu c17 (ubuntu 9.3.0-17ubuntu1~20.04) version 9.3.0 (x86gcc35340.dir/cmakeccompilerabi.c.o' '-c' '-mtune=generic' '-march=x86-64' as -v --64 -o cmakefiles/cmtc64-linux-gnu) using bfd version (gnu binutils for ubuntu) 2.34 compiler64-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/:/usr/lib/gcc/x8664-linux-gnu/ library64-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/:/usr/lib/gcc/x8664-linux-gnu/:/lib/../lib/:/usr/lib/x8664-linux-gnu/9/../../../:/lib/:/usr/lib/ collectoptions='-v' '-o' 'cmakefiles/cmtc35340 /usr/bin/cmake -e cmakescript cmakefiles/cmtc35340.dir/cmakeccompilerabi.c.o -o cmtcgcc=/usr/bin/cc collectwrapper=/usr/lib/gcc/x86targettarget64-linux-gnu configured with: ../src/configure -v --with-pkgversion='ubuntu 9.3.0-17ubuntu1~20.04' --with-bugurl=file:///usr/share/doc/gcc-9/readme.bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x8664-linux-gnu --host=x8664-linux-gnu thread model: posix gcc version 9.3.0 (ubuntu 9.3.0-17ubuntu1~20.04) compiler64-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/:/usr/lib/gcc/x8664-linux-gnu/ library64-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/:/usr/lib/gcc/x8664-linux-gnu/:/lib/../lib/:/usr/lib/x8664-linux-gnu/9/../../../:/lib/:/usr/lib/ collectoptions='-v' '-rdynamic' '-o' 'cmtc64-linux-gnu/9/collect2 -plugin /usr/lib/gcc/x86plugin.so -plugin-opt=/usr/lib/gcc/x86s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgccx8635340 /usr/lib/gcc/x8664-linux-gnu/scrt1.o /usr/lib/gcc/x8664-linux-gnu/crti.o /usr/lib/gcc/x8664-linux-gnu/9 -l/usr/lib/gcc/x8664-linux-gnu -l/usr/lib/gcc/x8664-linux-gnu -l/lib/../lib -l/usr/lib/x8664-linux-gnu/9/../../.. cmakefiles/cmtcs --pop-state -lc -lgcc --push-state --as-needed -lgcc64-linux-gnu/9/crtends.o /usr/lib/gcc/x8664-linux-gnu/crtn.o collectoptions='-v' '-rdynamic' '-o' 'cmtc64-linux-gnu/9/include] ==> [/usr/lib/gcc/x8664-linux-gnu] ==> [/usr/include/x8664-linux-gnu/9/include;/usr/local/include;/usr/include/x86link35340/fast && /usr/bin/make -f cmakefiles/cmtc35340.dir/build] ignore line: [make[1]: entering directory '/home/mona/research/code/openpose/build/cmakefiles/cmaketmp'] ignore line: [building c object cmakefiles/cmtc35340.dir/cmakeccompilerabi.c.o -c /usr/share/cmake-3.16/modules/cmakeccompilerabi.c] ignore line: [using built-in specs.] ignore line: [collecttargettarget64-linux-gnu] ignore line: [configured with: ../src/configure -v --with-pkgversion='ubuntu 9.3.0-17ubuntu1~20.04' --with-bugurl=file:///usr/share/doc/gcc-9/readme.bugs --enable-languages=c ada c++ go brig d fortran objc obj-c++ gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x8664-linux-gnu --host=x8664-linux-gnu] ignore line: [thread model: posix] ignore line: [gcc version 9.3.0 (ubuntu 9.3.0-17ubuntu1~20.04) ] ignore line: [collectoptions='-v' '-o' 'cmakefiles/cmtc64-linux-gnu/9/cc1 -quiet -v -imultiarch x8635340.dir/cmakeccompilerabi.c.o -version -fasynchronous-unwind-tables -fstack-protector-strong -wformat -wformat-security -fstack-clash-protection -fcf-protection -o /tmp/ccxj4xga.s] ignore line: [gnu c17 (ubuntu 9.3.0-17ubuntu1~20.04) version 9.3.0 (x8664-linux-gnu""] ignore line: [ignoring nonexistent directory ""/usr/lib/gcc/x8664-linux-gnu/9/../../../../x8664-linux-gnu/9/include] ignore line: [ /usr/local/include] ignore line: [ /usr/include/x8664-linux-gnu)] ignore line: [ compiled by gnu c version 9.3.0 gmp version 6.2.0 mpfr version 4.0.2 mpc version 1.1.0 isl version isl-0.22.1-gmp] ignore line: [] ignore line: [ggc heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072] ignore line: [compiler executable checksum: bbf13931d8de1abe14040c9909cb6969] ignore line: [collectoptions='-v' '-o' 'cmakefiles/cmtc35340.dir/cmakeccompilerabi.c.o /tmp/ccxj4xga.s] ignore line: [gnu assembler version 2.34 (x86path=/usr/lib/gcc/x8664-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/9/:/usr/lib/gcc/x86path=/usr/lib/gcc/x8664-linux-gnu/9/../../../x8664-linux-gnu/9/../../../../lib/:/lib/x8664-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86gcc35340.dir/cmakeccompilerabi.c.o' '-c' '-mtune=generic' '-march=x86-64'] ignore line: [linking c executable cmtclink35340.dir/link.txt --verbose=1] ignore line: [/usr/bin/cc -v -rdynamic cmakefiles/cmtc35340 ] ignore line: [using built-in specs.] ignore line: [collectlto64-linux-gnu/9/lto-wrapper] ignore line: [offloadnames=nvptx-none:hsa] ignore line: [offloaddefault=1] ignore line: [target: x8664-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32 m64 mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-hskzea/gcc-9-9.3.0/debian/tmp-nvptx/usr hsa --without-cuda-driver --enable-checking=release --build=x8664-linux-gnu --target=x86path=/usr/lib/gcc/x8664-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/9/:/usr/lib/gcc/x86path=/usr/lib/gcc/x8664-linux-gnu/9/../../../x8664-linux-gnu/9/../../../../lib/:/lib/x8664-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86gcc35340' '-mtune=generic' '-march=x86-64'] link line: [ /usr/lib/gcc/x8664-linux-gnu/9/liblto64-linux-gnu/9/lto-wrapper -plugin-opt=-fresolution=/tmp/ccvqudn6.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgccs --build-id --eh-frame-hdr -m elf64 --hash-style=gnu --as-needed -export-dynamic -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie -z now -z relro -o cmtc64-linux-gnu/9/../../../x8664-linux-gnu/9/../../../x8664-linux-gnu/9/crtbegins.o -l/usr/lib/gcc/x8664-linux-gnu/9/../../../x8664-linux-gnu/9/../../../../lib -l/lib/x8664-linux-gnu -l/usr/lib/../lib -l/usr/lib/gcc/x8635340.dir/cmakeccompilerabi.c.o -lgcc --push-state --as-needed -lgccs --pop-state /usr/lib/gcc/x8664-linux-gnu/9/../../../x8664-linux-gnu/9] ==> [/usr/lib/gcc/x8664-linux-gnu/9/../../../x8664-linux-gnu] collapse library dir [/usr/lib/gcc/x8664-linux-gnu] ==> [/lib/x8664-linux-gnu] ==> [/usr/lib/x8664-linux-gnu/9/../../..] ==> [/usr/lib] implicit libs: [gcc;gccs] implicit dirs: [/usr/lib/gcc/x8664-linux-gnu;/usr/lib;/lib/x86b3ae6/fast && /usr/bin/make -f cmakefiles/cmtcb3ae6.dir/build make[1]: entering directory '/home/mona/research/code/openpose/build/cmakefiles/cmaketmp' building cxx object cmakefiles/cmtcb3ae6.dir/testcxxcompiler.cxx.o -c /home/mona/research/code/openpose/build/cmakefiles/cmaketmp/testcxxcompiler.cxx linking cxx executable cmtclinkb3ae6.dir/link.txt --verbose=1 /usr/bin/c++ -rdynamic cmakefiles/cmtcb3ae6 make[1]: leaving directory '/home/mona/research/code/openpose/build/cmakefiles/cmaketmp' detecting cxx compiler abi info compiled with the following output: change dir: /home/mona/research/code/openpose/build/cmakefiles/cmaketmp run build command(s):/usr/bin/make cmtc7a701.dir/build.make cmakefiles/cmtc7a701.dir/cmakecxxcompilerabi.cpp.o /usr/bin/c++ -v -o cmakefiles/cmtcgcc=/usr/bin/c++ offloadnames=nvptx-none:hsa offloaddefault=1 target: x8664-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-hskzea/gcc-9-9.3.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x8664-linux-gnu --target=x86gcc7a701.dir/cmakecxxcompilerabi.cpp.o' '-c' '-shared-libgcc' '-mtune=generic' '-march=x86-64' /usr/lib/gcc/x8664-linux-gnu -dsource /usr/share/cmake-3.16/modules/cmakecxxcompilerabi.cpp -quiet -dumpbase cmakecxxcompilerabi.cpp -mtune=generic -march=x86-64 -auxbase-strip cmakefiles/cmtc64-linux-gnu) compiled by gnu c version 9.3.0, gmp version 6.2.0, mpfr version 4.0.2, mpc version 1.1.0, isl version isl-0.22.1-gmp ggc heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072 ignoring duplicate directory ""/usr/include/x8664-linux-gnu"" ignoring nonexistent directory ""/usr/lib/gcc/x8664-linux-gnu/9/../../../../x8664-linux-gnu/c++/9 /usr/include/c++/9/backward /usr/lib/gcc/x8664-linux-gnu /usr/include end of search list. gnu c++14 (ubuntu 9.3.0-17ubuntu1~20.04) version 9.3.0 (x86gcc7a701.dir/cmakecxxcompilerabi.cpp.o' '-c' '-shared-libgcc' '-mtune=generic' '-march=x86-64' as -v --64 -o cmakefiles/cmtc64-linux-gnu) using bfd version (gnu binutils for ubuntu) 2.34 compiler64-linux-gnu/9/:/usr/lib/gcc/x8664-linux-gnu/:/usr/lib/gcc/x8664-linux-gnu/ library_pat",question,question
1590,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1590,Host website http://posefs1.perception.cs.cmu.edu/ is down,"issue summary attempting to download any models or dependencies (opencv, caffe, etc.) from the urls in the install scripts reveals that the website is down: the build process fails because the url is inaccessible. executed command (if any) `%wgetfolder%` and other download instructions in setup/installation batch scripts. errors (if any) attempting to access the the noted url, as of the time of posting this issue, yields a ** error. type of issue - compilation/installation error",Error,deployment
1009,https://github.com/dusty-nv/jetson-inference/issues/1009,Retraining for TFLite,"hi dusty, after having achieved pretty good object detection results with jetson nano, deepstream sdk, resnet and peoplenet (both 4 classes models), i'm now going to achieve the same performance on another platform (raspberrypi+coral tpu). i know it is not your party, but i'm really baffled by the quality of your work, tutorials and documentation and i thought you could provide me some input from your experience, if you like. since i was not able to follow successfully the google tutorial ( for transfer learning on object detection, i thought probably this could work: 1) the aim is to re-train a mobilenet ssd v1 or v2 model for 90 classes to be fit for 4 classes only (person, car, roadsign, bicycle) and run it on a coral tpu with the same performance i get from a jetson nano with deepstream. right now i'm achieving 30 fps inference rate at 640x480 for three usb cameras simultaneously on the nano, 30 fps for each cam. i'm only able to achieve 10 fps per cam with the rpi/coral tuple. i would like to improve that at least by factor 3. i'm sure, using a class-reduced model would definitely help. 2) so i thought i could follow one of your tutorials first and either re-train an existing model with a sub-dataset or create a new model from scratch with own imagery and annotations. you have provided very nice videos and documentations for that, it shouldn't be too problematic to come to first results with that. that would allow me to run the training on the jetson gpu and - as far as i hope - i could also use the resulting model together with the deepstream sdk (?) on the nano for comparison and tests. 3) then it would be awesome to make this model ""google tpu ready"". as far as i understood you are in a first step converting torch to onxx, while i would need a tflite model from the torch output, which then would need to be compiled by the edgetpu compiler to an edgetpu.tflite model. i thought i could use maybe this repo in order to make the conversion from torch to tflite i have not started any of these adventures. i was just checking, if this path is promising. may i ask you for your opinion on that? i hope it is not too blasphemic to ask for google on a jetson project. kind regards",other,other
1057,https://github.com/dusty-nv/jetson-inference/issues/1057,Is possibile to train the model with an input size greater than 512x512?,"i read a lot of issues about training the model with an input of 512x512 to detect far objects. does someone know if it is possible to train the model with bigger inputs? probably the answer is still no, but maybe something has changed since the last time someone said that.",question,question
52,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/52,"Does the ""disable_blending"" images can reflect the scores?","issue summary i want to use openpose to do some action recognition. but could you tell me if the ""disableblending images to do a action classification work. and i know i can save the images as .json , but i just want to use the images to train a cnn net) executed command (if any) openpose output (if any) type of issue you might select multiple topics, delete the rest: - compilation/installation error - execution error - help wanted - question - enhancement / offering possible extensions / pull request / etc - other (type your own type) your system configuration **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. compiler (`gcc --version` on ubuntu):",other,question
693,https://github.com/dusty-nv/jetson-inference/issues/693,Failed TLS packet,"sorry to be a bother but ive been searching everywhere to find a solution since thursday, ive checked all open and closed issues but did'nt find my problem. when i try to clone or --recursive clone the /dusty-nv/jetson-inference.git it always bugs out at 68% and gives.... error: rpc failed; curl 56 gnutls recv error (-9): a tls packet with unexpected length was received. fatal: the remote end hung up unexpectedly fatal: early eof fatal: index-pack failed i have however succeeded in cloning tokks repo but tht is 4yrs old now. i have tried installing to a clean install of the latest development kit too. also tried looking at logs but could'nt find the specific log in the /var/logs directory. any idea what is causing this?",question,other
650,https://github.com/dusty-nv/jetson-inference/issues/650,detectnet python script example usage not working,i'd like to test the detectnet python script on a single image. the usage example on the documenation page for the object detection inference says to use the following command: $ ./detectnet.py --network=ssd-mobilenet-v2 images/peds_0.jpg output.jpg but you get the error: **,question,question
1109,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1109,i succeed run configure and generate. but got make -j'nproc' error,i succeed installed caffe cuda8.0 opencv error like this: cmakefiles/openposelib-stamp/openposelib-stamp/openposelib.dir/all' failed make[1]: ****** [cmtc_2a7e1/fast] error 2,question,question
293,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/293,cv::Mat problem,can i process a mat image( from memory) directly? the mat format image was obtained by my own method， not webcam or a image path. ubuntu 14.04 opencv3,question,question
41,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/41,VideoCapture (video) could not be opened for path: 'examples/media/video.avi'.,issue summary videocapture (video) could not be opened for path: 'examples/media/video.avi'. executed command (if any) ./build/examples/openpose/openpose.bin --video examples/media/video.avi openpose output (if any) starting pose estimation demo. unable to stop the stream: inappropriate ioctl for device terminate called after throwing an instance of 'std::runtimerelease -a` on ubuntu): ubuntu16.04 **: installed with `apt-get install libopencv-dev` or opencv 2.x or opencv 3.x. 3.0 compiler (`gcc --version` on ubuntu):,question,question
1203,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1203,Solution for Foot-only Keypoint Detection,"hi, thank for your wonderful work. foot estimation is really helpful. however, i find the foot part will lose, even if the foot is obvious in the center of the image(not a complete person). i guess oposepose focuses on the person first, so is there any solution for foot-only keypoint detection?",question,other
622,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/622,"How the coefficient ""scale_number"" and ""scale_gap"" be used exactly?","for example, --netnumber 4 --scaleresolution for processing except for the initial scale -1x368 ? how you process 4 outputs ? just average the 4 heatmaps of 4 different net_resolution? please help me figure this out thank you so much!",question,question
763,https://github.com/dusty-nv/jetson-inference/issues/763,I have a question during the tutorial.,"i am currently viewing the image recognition tutorial. i bought it because recommended system requirements said i needed training pcs, but do i have to run all the tutorials i'm looking at on the jetson board? is the docker installed on the jetson? i was wondering if the pc i bought from the training in the back was used. ? thank you for your regard",other,question
94,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/94,The poseKeyPoints is 3D or 2D data?,"i read the example code, tutorial_pose, and seen the code like this const auto posekeypoints = poseextractorcaffe.getposekeypoints(); so, please tell me the the posekeypoints is 3d or 2d data? if i want to get the 3d of human pose, how can i do it? thanks for your replay!",question,question
760,https://github.com/dusty-nv/jetson-inference/issues/760,"Object detection, one box instead of several boxes overlapping?","hi, great tutorial. i have finished the custom object detection. and i am working on using two servos to track the object. when an object is detected, it is often to have 2-3 green boxes instead of one. is there anyway to see just one box? here is what i am doing for the servo tracking. in the code of detectnet.py for detection in detections: i use detection.area to find the smallest area and use the centre point to track the object. not sure why it is jumpy... i need to set the pixel tolerance(object centre point - window centre point) to 100 in order to make the servo stop shaking... thanks.",question,question
95,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/95,Add a tracker of the pose data for multi-person tracking?,"issue summary is there any plan to add a tracker of pose data for multi-person tracking? my understanding of this feature can be done using a kd-tree to track the latest model of each person using the pose 18 point data, and then we can get classify each person when we get a new pose detection. if there is no plan of such feature, i will do it and when i finished, i can contribute the code into the example directory. executed command (if any) n/a openpose output (if any) n/a type of issue - enhancement / offering possible extensions your system configuration n/a",other,other
1425,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1425,A link in OpenPose-Prerequisites in nonvalid!,"hello, my sever doesn't support camke-gui, so i can only use camke command line for configuration. however the link ""cmake command line configuration (ubuntu only)"" in openpose-prerequisites is nonvalid. it links to the ""openpose-prerequisites"" page. thanks for your checking it. i really want to konw how to cmake configuration by command line not cmake-gui. thanks very much!",Error,other
128,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/128,Change the pose rendering graphics / style,"hi where can i find the piece of code which generates the pose colorful lines / dots? i like those but i would need to connect the ears to the rest of the body (when possible). not sure in which source files this piece of code is? also, are you planning to use your pose algo to ease a body segmentation type of work? or somehow to generate a shape that is even closer to the real surface used by the body on the frame, rather than the dots+lines only?",other,question
665,https://github.com/dusty-nv/jetson-inference/issues/665,Processing images with the .pth model rather than .onnx model,"hi, i followed the tutorial and managed to get it working very well on the jetson nano, but i would like to know if it is possible to process the images with the .pth model instead of the .onnx on my computer, in order to compare the performance between my computer and the jetson nano.",question,other
714,https://github.com/dusty-nv/jetson-inference/issues/714,How can I build the inference and utils modules for pyenv,"hi, i am using pyenv to manage my python versions (and also manually built opencv with cuda support). i would like to ask how i can correctly build the jetson inference and utils modules for those environments?",deployment,question
495,https://github.com/dusty-nv/jetson-inference/issues/495,How to modify my-detection.py for my own dataset?,"i have done the ""collecting your own datasets"" and it can classify the object. cool. may i know how to change the function call of my-detection.py? load the object detection model net = jetson.inference.detectnet(""ssd-mobilenet-v2"", threshold=0.5) changing ssd-mobilenet-v2 to my onnx file? thx",question,question
748,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/748,"python layer not made despite setting flag in  CMakeCache.txt  (...and caffe/src/openpose_caffe-build/CMakeCache.txt, and CMakeFiles/openpose_caffe.dir/build.make)","issue summary python api not built despite changing entry at cmakecache.txt , when that didnt work tried cmakefiles/openposecaffe-build/cmakecache.txt, when that didnt work tried caffe/tmp/openposepython ""build openpose python."" on) executed command (if any) python 1pose.py openpose output (if any) also, from make: - compilation/installation error your system configuration 1. ** api:",question,question
1425,https://github.com/dusty-nv/jetson-inference/issues/1425,Problem with SPI and live detection,"i’m trying to use the spi1 bus on a jetson nano configured using jetpack 4.3. i followed the instruction from to enable the spi1. i want to use the spi to send some info from live detection from jetson-inference ./detectnet csi://0. the problem before setting the 40-pin configurations the fps was 24-25 … after setting it up, the fps dropped to 3-4 when using simple_camera.py from csi-camera repo it lags and it’s really affected. i know that the problem from the configurations because once i did it i tested it because i reset my nano because of it.",other,question
309,https://github.com/dusty-nv/jetson-inference/issues/309,make build error,"hi all i am trying to setup jetson-inference on jetson nano. however, i am facing an error to build after making the repository without any error: [44%] linking cxx shared library ../aarch64/lib/libjetson-utils.so /usr/bin/ld: cannot find -igl collect2: errir: ld returned 1 exit status can anyone help me to fix this problem? thanks.",question,question
971,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/971,Maximum accuracy for MPI model,"issue summary hello, the configuration for achieving maximum accuracy is well explained in for coco model. hence, i am wondering which configuration for maximum accuracy is best for mpi dataset model?any experience on that? type of issue - help wanted - question your system configuration 2. ** issue:",question,question
1119,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/1119,OPENPOSE-PYTHON:How to use Stereo camera to read 3D data in pyopenpose,"could pyopenpose run the 3d module now?i i didn't find 3d-pyopenpose-readmd i'm searching... issue summary my system:ubuntu18.04 i use the zed camera. i have got the rgb and depth data (numpy) .but i wondered: 1、how to make the pyopenpose recognize the 3d data from zed stereo camera? 2、could i input 2d data to pyopenpose and add the depth data(from stereo camera) into outputdata then? 3、due to the neural networks from openpose,some key-points were kept out but still displayed in image. dose it affect the 3d model? i thought it would affects . type of issue - help wanted - question your system configuration **: code: error: auto-detecting all available gpus... detected 1 gpu(s), using 1 of them starting at gpu 0. error: only 1 camera detected. the 3-d reconstruction module can only be used with > 1 cameras simultaneously. e.g., using flir stereo cameras (`--flir_camera`). where could i append the camera name? i used zed stereo camera does openpose recognize every stereo camera now? if doesn't, what should i do ? thanks!!!!very much!!!!!!!!!!!!!!!!!!!!!!!! maybe i should read the readme more carefully.lol",question,question
310,https://github.com/dusty-nv/jetson-inference/issues/310,CaffeParser: Could not open file      CaffeParser: Could not parse model file,"./detectnet-console dogdog1.jpg coco-dog detectnet-console args (4): 0 [./detectnet-console] 1 [dogdog1.jpg] 3 [coco-dog] [trt] tensorrt version 5.0.6 [trt] detected model format - caffe (extension '.caffemodel') [trt] desired precision specified for gpu: fastest [trt] requested fasted precision for device gpu without providing valid calibrator, disabling int8 [trt] native precisions detected for gpu: fp32, fp16 [trt] selecting fastest native precision for gpu: fp16 [trt] attempting to open engine cache file .2.1.gpu.fp16.engine [trt] cache file not found, profiling network model on device gpu [trt] device gpu, loading [trt] caffeparser: could not open file [trt] caffeparser: could not parse model file [trt] device gpu, failed to parse caffe network device gpu, failed to load detectnet -- failed to initialize. detectnet-console: failed to initialize detectnet",question,other
734,https://github.com/dusty-nv/jetson-inference/issues/734,I have a question for IMAGENET.,"i want to know about information for imagenet data that is trained this repo's classification model. i found a lot of imagenet, so i ask for you about version of imagenet.",question,question
543,https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/543,Help wanted: Isolation of keypoints within specific coordinates in image,"hi there, i would like to extract keypoints data within specific bounds from an image using the openpose api . does anyone have any advice as to ho this could be done? many thanks. jetson tx2 - ubuntu 16.04 **: default from openpose compiler (`gcc --version` in ubuntu): 5.4.0",question,other
13,https://github.com/dusty-nv/jetson-inference/issues/13,CUDNN Problem installing NVCaffe,"@dusty-nv i would like to use tensorrt and accompanying software that was recently made public by nvidia. i am working on installing your repo on my tx1, but first i must install the fp16 branch of nvcaffe. i seem to be getting a cudnn error with the version that comes with the most recent jetpack install. i can run `make all`, `make test`, and `make pycaffe` all with no problems. then i try to run `make runtest cudavisible=0` and get the following error about cudnn. f1003 02:38:10.825604 1032 cudnnlayer.cpp:157] check failed: status == cudnnsuccess (9 vs. 0) cudnnnot_supported * check failure stack trace: ** [runtest] aborted do i need to use a different version of cudnn than is present with the jetpack?",deployment,question
667,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/667,CycleGAN: Is there a way to get the score map of patchGAN?,"hi there, thanks for the great work. just wondering is there a way that i can get the score map produced by patchgan in cyclegan?",question,question
21,https://github.com/JaidedAI/EasyOCR/issues/21,Link refiner from CRAFT,"first of all, thanks for making this tool! it's a great combination of powerful methods for text detection/recognition tasks. i was wondering if the is intentionally omitted from the code. otherwise, i'd be happy to make a pr for adding it.",other,other
532,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/532,val directory in pix2pix,i want to run pix2pix code on my own dataset. and i found this sentence in tips: my question is: what kind of image is in 'val' folder?,question,question
353,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/353,pix2pix  questions in subtitle generate?,i just want to use pix2pix to generate the subtitle，but the result is not very good.what can i do to improve the result?looks forward to hearing from you @junyanz,question,question
166,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/166,Which file shoud I run to get a speaker embedding of a speaker.,i have downloaded the weights for encoder.also i have some audio file of a particular person.,question,question
181,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/181,Just a single inference,whats the best way to make just a inputimage inference? the actual inferencing process in this repository is so nested that i figured i just ask.,question,question
104,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/104,Which part of channels in ConvTransposed2d comes from the bottleneck in Unet model?,as the channels are concatenated before the convtransposed2d. does the first half channels receive the data from the bottleneck or th latter half?,question,question
261,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/261,No GPU - can we use Google Colab?,noob here without a gpu. how would someone use this with google colab,question,question
1117,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1117,How do I get the generated jpg images？,"after i trained a model, i got fake images with png format, when i tested my model. but i want to get some fake images with jpg format. can you tell me how to set to change the format of fake imsges? and what is the impact of the different image format? thanks a lot.",Performance,question
611,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/611,Quality of the voice,"hi, i am trying to clone the voice of famous people like abdul kalam, modi. i collected their speeches from youtube videos. but the quality of the voice is very low. there is no similarity between the voice generated by the model and the target's voice. i am attaching the generated audio file(.wav) and the audio file which i used for training(.mp3)",question,question
461,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/461,Update librosa and numba versions in requirements.txt,"librosa 0.8.0 is now available, and that fixes the issue which required us to pin numba to 0.48.0 (#373). we should require `librosa>=0.8` in requirements.txt and allow any `numba` that is compatible. anyone reading this is invited to submit a pull request. you can do this!",other,other
726,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/726,How To upload input image at webpage demo?,"output image could be saved, however, input image can't be uploaded. my model requires complex input. using line tool is really difficult. so is it possible to upload image file as input? i understand a little of python, but nothing of javascript. maybe i shouldn't put this issue here, but i can't find a better place. thanks!",question,question
223,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/223,Google Colab output file has no sound,google colab notebook: the output.wav is can't be played on the notebook and if downloaded it doent't have any sound.,other,other
476,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/476,Model files not found but I've copied it to root directory,"2020-08-08 13:16:05.767958: w tensorflow/streamloader.cc:55] could not load dynamic library 'cudart64100.dll not found 2020-08-08 13:16:05.768070: i tensorflow/streamstub.cc:29] ignore above cudart dlerror if you do not have a gpu set up on your machine. c:\users\андрей\desktop\real-time-voice-cloning-master\encoder\audio.py:13: userwarning: unable to import 'webrtcvad'. this package enables noise removal and is recommended. warn(""unable to import 'webrtcvad'. this package enables noise removal and is recommended."") arguments: running a test of your configuration... using cpu for inference. error: model files not found. if needed, download them here:",question,question
410,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/410,"Audioread ""no backend"" issue","this comes up very frequently for windows users. `pip install -r requirements.txt` does not install a backend for audioread (a dependency of librosa). this causes an exception when loading any file that requires conversion, such as .mp3. can we make use of soundfile to load and convert audio files, instead of librosa.load? we already have soundfile in requirements.txt and it automatically installs all prerequisites on windows and macos. linux users need to install `libsndfile1`. this will prevent users from encountering the ""no backend"" error. alternatively we can add a step to readme.md asking the user to install ffmpeg before running the toolbox. but this seems more elegant if it works.",deployment,other
337,https://github.com/JaidedAI/EasyOCR/issues/337,Dataset used for training,"hello. i'm curious about the dataset you used for training. i know you used synthetic data. when creating a dictionary(especially ko.txt(korean dict)), i wonder if it was created in syllable units or included words or sentences. also, i fine-tuned your model using my synthetic dataset(word & sentence). validation and test results were good(98%), but the result of using my fine-tuned model in easyocr was very terrible! in conclusion, my fine-tuned model did not recognize a single letter. omg...:( if you know the reason, please let me know. i waiting for your answer.",question,question
903,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/903,Synthesizer fine-tuning ruined the output,"we were trying to fine-tune the existing model on some of our own data. we were faced with this error. we went into the synthesizer/train.py file and commented out the line 192(which was an if condition) after that, the model started training but after training(for 2k steps), we were getting no output at all. can you please identify what might have caused this? thank you.",question,question
781,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/781,Turn off Visdom,"i am running this code on a remote gpu, and can only access via terminal. due to this my output is filled with visdom error message like no display, etc. i just want to directly print the results after each iteration, and store the log files. is there any way to turn off the visdom stuff? below is a snippet of the error. ``raise connectionerror(e, request=request) requests.exceptions.connectionerror: httpconnectionpool(host='localhost', port=8097): max retries exceeded with url: /events (caused by newconnectionerror(': failed to establish a new connection: [errno 111] connection refused',))``",question,question
322,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/322,"Problem:Run demo_cli.py, Nothing happened!","i already run "" pip3 install -r requirements.txt"" ,it's done. when i run ""democli.py (base) c:\users\administrator\real-time-voice-cloning-master\real-time-voice-cloning-master> does anyone know why? thanks.",question,question
170,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/170,RuntimeError: calling resize_ on a tensor that has non-resizable storage.,"when i tried to execute pix2pix with pre-trained model, an error occurred in the function of setdirection btoa --model pix2pix --name facadespretrained --datasetmodel256 --norm batch --gpuratio: 1.0 batchsize: 1 checkpointsmode: aligned displayport: 8097 displayids: [] howtype: normal inputdatasetlayerslabel2photodropout: false nonc: 3 phase: test resizecrop: resizecrop resultsbatches: false whichepoch: latest whichnetd: basic whichnetg: unetmodel.py"", line 67, in set on a tensor that has non-resizable storage. clone it first or create a new tensor instead.",question,question
22,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/22,audioread.exceptions.NoBackendError,"traceback (most recent call last): file ""d:\sdx\deepfake\voice\real-time-voice-cloning\toolbox\_frompreprocessopen audioread.exceptions.nobackenderror i have installed ffmpeg, but still get this error.",question,other
484,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/484,An alternative approach to the speaker encoder,"> for the encoder, i have a question... if i well understand, the target is just to maximize similarity of 2 audios of the same speaker and then minimize distance between them > so, we could imagin to use another approach to train it no ? based on the « voicemap » project i made a simple siamese network whose target is to minimize distance between 2 audios of n seconds (i tried 2 and 3) and have really good results too (i have 88-90% with binary accuracy) with only 2 or 3 hours of training on my gpu ! > the process is really simple : > 2 inputs (2 sec of raw audio) pass to a same encoder network then the 2 embedded (here 64 dims vectors) pass to an euclidian distance layer and then to a 1 neuron linear with sigmoid (which gives the probability that the 2 audios are of the same speaker) > here i used same length audio but i suppose 2 audios of different length can be good too and the model is only cnn so much faster and easier to train than the actual 3-layer rnn... > here is the tutorial link with code of the original voicemap project, really interesting and many fun applications i made with it > > > now i plan to convert the encoder of this repo and see his loss and try to compare it with my encoer loss to see if results are similar or not (because i don’t know how to use binary accuracy with this encoder) _originally posted by @ananas120 in",question,other
449,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/449,Training a new model based on LibriTTS,"> @blue-fish, would it be useful if i was to offer a gpu (2080 ti) for contributing on training a new model based on libritts ? > i have yet to train any models and would gladly exchange gpu time for an opportunity to learn. > i wonder how long it would take on a single 2080 ti. _originally posted by @mbdash in",other,other
539,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/539,MultiGPU Inefficiency,"this implementation is great, thank you very much. i have a question though regarding the efficiency of training on multiplegpu's. the problem i am having is that when training on 8 gpu's it appears that gpu 0 is doing significantly more work than any other gpu. for example, i am using 8 gpus with 16gb's of memory each and the first gpu is fully utilized but the others only use 11gb's. any thoughts about increasing the efficiency would be appreciated. an example of the utilization can be found below.",deployment,question
1491,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1491,"Could not connect to Visdom server: Colab ""Training"" section errors out","setting up a new session... exception in user code: ------------------------------------------------------------ traceback (most recent call last): file ""/usr/local/lib/python3.7/dist-packages/urllib3/connection.py"", line 159, in conn file ""/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py"", line 80, in createconnection connectionrefusederror: [errno 111] connection refused during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py"", line 600, in urlopen file ""/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py"", line 354, in request file ""/usr/lib/python3.7/http/client.py"", line 1281, in request file ""/usr/lib/python3.7/http/client.py"", line 1327, in request file ""/usr/lib/python3.7/http/client.py"", line 1276, in endheaders file ""/usr/lib/python3.7/http/client.py"", line 1036, in output file ""/usr/lib/python3.7/http/client.py"", line 976, in send file ""/usr/local/lib/python3.7/dist-packages/urllib3/connection.py"", line 181, in connect file ""/usr/local/lib/python3.7/dist-packages/urllib3/connection.py"", line 168, in conn urllib3.exceptions.newconnectionerror: : failed to establish a new connection: [errno 111] connection refused during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/usr/local/lib/python3.7/dist-packages/requests/adapters.py"", line 449, in send file ""/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py"", line 638, in urlopen file ""/usr/local/lib/python3.7/dist-packages/urllib3/util/retry.py"", line 399, in increment urllib3.exceptions.maxretryerror: httpconnectionpool(host='localhost', port=8097): max retries exceeded with url: /env/main (caused by newconnectionerror(': failed to establish a new connection: [errno 111] connection refused')) during handling of the above exception, another exception occurred: traceback (most recent call last): file ""/usr/local/lib/python3.7/dist-packages/visdom/_send file ""/usr/local/lib/python3.7/dist-packages/visdom/_handleclose() takes 1 positional argument but 3 were given [errno 99] cannot assign requested address onclose() takes 1 positional argument but 3 were given visdom python client failed to establish socket to get messages from the server. this feature is optional and can be disabled by initializing visdom with `usesocket=false`, which will prevent waiting for this request to timeout. trying to start a server.... command: /usr/bin/python3 -m visdom.server -p 8097 &>/dev/null & wandb: (1) create a w&b account wandb: (2) use an existing w&b account wandb: (3) don't visualize my results wandb: enter your choice: [errno 99] cannot assign requested address on_close() takes 1 positional argument but 3 were given [errno 99] cannot assign requested address",Error,question
692,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/692,Running demo_cli.py fails on new installation,"hi, i am getting this error. summarizing: ** missing key(s) in statethreshold"", ""encoder.embedding.weight"", ................................ it seems like the synthesizer statecli.py pydev debugger: process 52660 is connecting connected to pydev debugger (build 193.6015.41) arguments: /usr/local/lib/python3.7/site-packages/librosa/core/audio.py:162: userwarning: pysoundfile failed. trying audioread instead. warnings.warn(""pysoundfile failed. trying audioread instead."") running a test of your configuration... using cpu for inference. preparing the encoder, the synthesizer and the vocoder... loaded encoder ""pretrained.pt"" trained to step 1564501 synthesizer using device: cpu building wave-rnn trainable parameters: 4.481m loading model weights at vocoder/savedexec file ""/applications/pycharm.app/contents/plugins/python/helpers/pydev/imps/execfile.py"", line 18, in execfile file ""/users/7004470/realtimevoice/real-time-voice-cloning-master/demospectrograms file ""/users/7004470/realtimevoice/real-time-voice-cloning-master/synthesizer/inference.py"", line 64, in load file ""/users/7004470/realtimevoice/real-time-voice-cloning-master/synthesizer/models/tacotron.py"", line 497, in load file ""/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1224, in loaddict runtimeerror: error(s) in loading statedict: ""stopnet.fc1.weight"", ""encoder.prenet.fc2.weight"", ""encoder.prebank.0.conv.weight"", ""encoder.cbhg.conv1dbank.0.bnorm.bias"", ""encoder.cbhg.conv1dmean"", ""encoder.cbhg.conv1dvar"", ""encoder.cbhg.conv1dbank.1.bnorm.weight"", ""encoder.cbhg.conv1dbank.1.bnorm.runningbank.1.bnorm.runningbank.2.conv.weight"", ""encoder.cbhg.conv1dbank.2.bnorm.bias"", ""encoder.cbhg.conv1dmean"", ""encoder.cbhg.conv1dvar"", ""encoder.cbhg.conv1dbank.3.bnorm.weight"", ""encoder.cbhg.conv1dbank.3.bnorm.runningbank.3.bnorm.runningbank.4.conv.weight"", ""encoder.cbhg.conv1dbank.4.bnorm.bias"", ""encoder.cbhg.conv1dmean"", ""encoder.cbhg.conv1dvar"", ""encoder.cbhg.convproject1.bnorm.weight"", ""encoder.cbhg.convproject1.bnorm.runningproject1.bnorm.runningproject2.conv.weight"", ""encoder.cbhg.convproject2.bnorm.bias"", ""encoder.cbhg.convmean"", ""encoder.cbhg.convvar"", ""encoder.cbhg.highways.0.w1.weight"", ""encoder.cbhg.highways.0.w1.bias"", ""encoder.cbhg.highways.0.w2.weight"", ""encoder.cbhg.highways.0.w2.bias"", ""encoder.cbhg.highways.1.w1.weight"", ""encoder.cbhg.highways.1.w1.bias"", ""encoder.cbhg.highways.1.w2.weight"", ""encoder.cbhg.highways.1.w2.bias"", ""encoder.cbhg.highways.2.w1.weight"", ""encoder.cbhg.highways.2.w1.bias"", ""encoder.cbhg.highways.2.w2.weight"", ""encoder.cbhg.highways.2.w2.bias"", ""encoder.cbhg.highways.3.w1.weight"", ""encoder.cbhg.highways.3.w1.bias"", ""encoder.cbhg.highways.3.w2.weight"", ""encoder.cbhg.highways.3.w2.bias"", ""encoder.cbhg.rnn.weightl0"", ""encoder.cbhg.rnn.weightl0"", ""encoder.cbhg.rnn.biasl0"", ""encoder.cbhg.rnn.biasl0"", ""encoder.cbhg.rnn.weightl0hhreverse"", ""encoder.cbhg.rnn.biasl0hhreverse"", ""encodernet.conv.weight"", ""decoder.attnnet.l.weight"", ""decoder.attnnet.w.bias"", ""decoder.attnrnn.weightrnn.weightrnn.biasrnn.biasinput.weight"", ""decoder.rnnrnn1.weightrnn1.weightrnn1.biasrnn1.biasrnn2.weightrnn2.weightrnn2.biasrnn2.biasproj.weight"", ""decoder.stopproj.bias"", ""postnet.conv1dbank.0.bnorm.weight"", ""postnet.conv1dbank.0.bnorm.runningbank.0.bnorm.runningbank.1.conv.weight"", ""postnet.conv1dbank.1.bnorm.bias"", ""postnet.conv1dmean"", ""postnet.conv1dvar"", ""postnet.conv1dbank.2.bnorm.weight"", ""postnet.conv1dbank.2.bnorm.runningbank.2.bnorm.runningbank.3.conv.weight"", ""postnet.conv1dbank.3.bnorm.bias"", ""postnet.conv1dmean"", ""postnet.conv1dvar"", ""postnet.conv1dbank.4.bnorm.weight"", ""postnet.conv1dbank.4.bnorm.runningbank.4.bnorm.runningproject1.conv.weight"", ""postnet.convproject1.bnorm.bias"", ""postnet.convmean"", ""postnet.convvar"", ""postnet.convproject2.bnorm.weight"", ""postnet.convproject2.bnorm.runningproject2.bnorm.runninghighway.weight"", ""postnet.highways.0.w1.weight"", ""postnet.highways.0.w1.bias"", ""postnet.highways.0.w2.weight"", ""postnet.highways.0.w2.bias"", ""postnet.highways.1.w1.weight"", ""postnet.highways.1.w1.bias"", ""postnet.highways.1.w2.weight"", ""postnet.highways.1.w2.bias"", ""postnet.highways.2.w1.weight"", ""postnet.highways.2.w1.bias"", ""postnet.highways.2.w2.weight"", ""postnet.highways.2.w2.bias"", ""postnet.highways.3.w1.weight"", ""postnet.highways.3.w1.bias"", ""postnet.highways.3.w2.weight"", ""postnet.highways.3.w2.bias"", ""postnet.rnn.weightl0"", ""postnet.rnn.weightl0"", ""postnet.rnn.biasl0"", ""postnet.rnn.biasl0"", ""postnet.rnn.weightl0hhreverse"", ""postnet.rnn.biasl0hhreverse"", ""postdict: ""upsample.resnet.convnorm.weight"", ""upsample.resnet.batchnorm.runningnorm.runningnorm.numtracked"", ""upsample.resnet.layers.0.conv1.weight"", ""upsample.resnet.layers.0.conv2.weight"", ""upsample.resnet.layers.0.batchnorm1.bias"", ""upsample.resnet.layers.0.batchmean"", ""upsample.resnet.layers.0.batchvar"", ""upsample.resnet.layers.0.batchbatchesnorm2.weight"", ""upsample.resnet.layers.0.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.1.conv1.weight"", ""upsample.resnet.layers.1.conv2.weight"", ""upsample.resnet.layers.1.batchnorm1.bias"", ""upsample.resnet.layers.1.batchmean"", ""upsample.resnet.layers.1.batchvar"", ""upsample.resnet.layers.1.batchbatchesnorm2.weight"", ""upsample.resnet.layers.1.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.2.conv1.weight"", ""upsample.resnet.layers.2.conv2.weight"", ""upsample.resnet.layers.2.batchnorm1.bias"", ""upsample.resnet.layers.2.batchmean"", ""upsample.resnet.layers.2.batchvar"", ""upsample.resnet.layers.2.batchbatchesnorm2.weight"", ""upsample.resnet.layers.2.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.3.conv1.weight"", ""upsample.resnet.layers.3.conv2.weight"", ""upsample.resnet.layers.3.batchnorm1.bias"", ""upsample.resnet.layers.3.batchmean"", ""upsample.resnet.layers.3.batchvar"", ""upsample.resnet.layers.3.batchbatchesnorm2.weight"", ""upsample.resnet.layers.3.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.4.conv1.weight"", ""upsample.resnet.layers.4.conv2.weight"", ""upsample.resnet.layers.4.batchnorm1.bias"", ""upsample.resnet.layers.4.batchmean"", ""upsample.resnet.layers.4.batchvar"", ""upsample.resnet.layers.4.batchbatchesnorm2.weight"", ""upsample.resnet.layers.4.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.5.conv1.weight"", ""upsample.resnet.layers.5.conv2.weight"", ""upsample.resnet.layers.5.batchnorm1.bias"", ""upsample.resnet.layers.5.batchmean"", ""upsample.resnet.layers.5.batchvar"", ""upsample.resnet.layers.5.batchbatchesnorm2.weight"", ""upsample.resnet.layers.5.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.6.conv1.weight"", ""upsample.resnet.layers.6.conv2.weight"", ""upsample.resnet.layers.6.batchnorm1.bias"", ""upsample.resnet.layers.6.batchmean"", ""upsample.resnet.layers.6.batchvar"", ""upsample.resnet.layers.6.batchbatchesnorm2.weight"", ""upsample.resnet.layers.6.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.7.conv1.weight"", ""upsample.resnet.layers.7.conv2.weight"", ""upsample.resnet.layers.7.batchnorm1.bias"", ""upsample.resnet.layers.7.batchmean"", ""upsample.resnet.layers.7.batchvar"", ""upsample.resnet.layers.7.batchbatchesnorm2.weight"", ""upsample.resnet.layers.7.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.8.conv1.weight"", ""upsample.resnet.layers.8.conv2.weight"", ""upsample.resnet.layers.8.batchnorm1.bias"", ""upsample.resnet.layers.8.batchmean"", ""upsample.resnet.layers.8.batchvar"", ""upsample.resnet.layers.8.batchbatchesnorm2.weight"", ""upsample.resnet.layers.8.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.layers.9.conv1.weight"", ""upsample.resnet.layers.9.conv2.weight"", ""upsample.resnet.layers.9.batchnorm1.bias"", ""upsample.resnet.layers.9.batchmean"", ""upsample.resnet.layers.9.batchvar"", ""upsample.resnet.layers.9.batchbatchesnorm2.weight"", ""upsample.resnet.layers.9.batchnorm2.runningnorm2.runningnorm2.numtracked"", ""upsample.resnet.convout.bias"", ""upsample.uplayers.3.weight"", ""upsample.upihhhihhhihhhihhh_l0"", ""fc1.weight"", ""fc1.bias"", ""fc2.weight"", ""fc2.bias"", ""fc3.weight"", ""fc3.bias"".",question,question
891,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/891,Advice on bad performance,"i trained the synthesizer and the vocoder for the italian language, but i kept the pretrined english encoder. while training, the generated mels were very good (for the synthesizer) and also the wavs for the vocoder where very good. however whn i try to do some inference the performance is really bad! do yo have any idea why? maybe i didn't train enought the synthesizer (82k epochs) .... but still the training results were great...",question,question
893,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/893,"Should I correct the warning ""creating a tensor from a list of numpy.ndarray is extremely slow"" ?","hi, during the training, i get the following warning constantly : `/content/real-time-voice-cloning/synthesizer/synthesizernew.cpp:201.)` should i follow the advice and change line 84 to `embeds = torch.tensor(np.array(embeds))` ? or this was done on purpose and must not be changed ?",other,question
237,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/237,Generator modifies image size when not using already implemented cropping,"hello ! for my project, i crop the images according to a ground truth mask, so i don't use the already implemented cropping functions. for some reason, the generator function now modifies the size of the images: for example, a 362x529 image becomes a 364x332 image after going through netg_a. anyone has an idea why? thanks",question,question
60,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/60,WaveRNN modifications?,is this repo uses vanilla version of wavernn ( or architecture was modified and model was retrained? it's quite fast relative to my benchmark of tacotron2 + wavernn from here but (maybe) have less natural voice. btw i don't tried new `universal vocoder` version.,question,other
450,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/450,some questions about the dataset of the horse2zebra,"thanks to your code about the cyclegan. i achieve my own cyclegan code. however,i can not get the effect of the horse to zebra. when i use the dataset of the maps,the effect is good.i am confused about the question.i want to know about the details of the train the dataset of horese2zebra.",question,question
181,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/181,Where is generated wave file?,"i am running this code from remote server by vnc, i can not hear voice, how can i download generated wave file? from demo_toolbox i can see the log is ok, waveform is generated, but there is nothing in the data directory.",question,question
245,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/245,Tensorflow DLL error (what is the best combo to use of TF & Cuda/?),"i've squashed so many errors and now i'm stuck on this one. anyone have some advise of the exact version of tf to use along with cuda and cudnn? thanks a bunch!!! (deepvoicec) c:\deepvoicec\real-time-voice-cloning>python demotensorflow.py"", line 58, in file ""c:\users\bev\anaconda3\envs\deepvoicec\lib\site-packages\tensorflow\python\pywrapinternal.py"", line 28, in file ""c:\users\bev\anaconda3\envs\deepvoicec\lib\site-packages\tensorflow\python\pywrapinternal.py"", line 24, in swighelper file ""c:\users\bev\anaconda3\envs\deepvoicec\lib\imp.py"", line 242, in loaddynamic importerror: dll load failed: a dynamic link library (dll) initialization routine failed. during handling of the above exception, another exception occurred: traceback (most recent call last): file ""demotensorflow.py"", line 74, in importerror: traceback (most recent call last): file ""c:\users\bev\anaconda3\envs\deepvoicec\lib\site-packages\tensorflow\python\pywraptensorflowtensorflowimportmodule file ""c:\users\bev\anaconda3\envs\deepvoicec\lib\imp.py"", line 342, in load_dynamic importerror: dll load failed: a dynamic link library (dll) initialization routine failed. see",question,question
1360,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1360,'Visualizer' object has no attribute 'ncols',"hi, mr. author i've met a question: i use to latest code and i want to use wandb and avoid visdom.server, so the command is : ""python train.py --dataroot ./datasets/maps --name mapsgan --loadsize 400 --nepochswandb --displaynpnpnpnpnpresource = np.dtype([(""resource"", np.ubyte, 1)]) d:\anaconda3\envs\py37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:542: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future versi on of numpy, it will be understood as (type, (1,)) / '(1,)type'. quint8 = np.dtype([(""quint8"", np.uint8, 1)]) d:\anaconda3\envs\py37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:544: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future versi on of numpy, it will be understood as (type, (1,)) / '(1,)type'. quint16 = np.dtype([(""quint16"", np.uint16, 1)]) d:\anaconda3\envs\py37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:550: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future versi on of numpy, it will be understood as (type, (1,)) / '(1,)type'. npcyclegan wandb: view project at wandb: view run at wandb: run data is saved locally in d:\lzf\cyclegan-fenbianlv2\wandb\run-20211229cyclegan\web... d:\anaconda3\envs\py37\lib\site-packages\torch\optim\lrscheduler.step()` before `optimizer.step()`. in pytorch 1.1.0 and later, you sho uld call them in the opposite order: `optimizer.step()` before `lra: 0.271 ga: 2.136 idtb: 0.838 gb: 1.258 idta: 0.261 ga: 1.849 idtb: 0.282 gb: 1.418 idta: 0.264 ga: 2.066 idtb: 0.251 gb: 1.724 idtcurrentid 0"", it runs fun.",Error,other
693,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/693,Reproducing training - versus new training?  Which python scripts?,"hi all. goal: use custom sound files to create voices to train with. obstacles: the training guide - mentions: ""you're expected to run all python scripts in their alphabetical order."" are these scripts in this wiki or are we supposed to figure that out on our own? for those new to python, is there somewhere we can find the scripts necessary to train our new voices? are we sol unless we have a good command of python? what would be our best resources for training voice recordings so that the output is better and closest to the actual voice? thank you.",question,question
208,https://github.com/JaidedAI/EasyOCR/issues/208,"OCR Can not recognize simple text, solutions?",[deleted],other,other
220,https://github.com/JaidedAI/EasyOCR/issues/220,Support Caucasian group of languages(10) that use Cyrillic,this group of languages: are using a combination of the russian alphabet in addition to one letter how can this be supported? (i could provide you with a dictionary with one of the languages that has this additional letter),question,other
1111,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1111,"During  testing,can I just use one picture?",i successfully trained my own pix2pix model. however it seems that that the test script test.py only accepts a prepared dataset as input.can i send a picture(ndarray or pilimage) to it and do not sacrifice prepairing a a-b dataset?,question,question
921,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/921,"Save custom metric in ""--save_epoch_freq"".",i read the resource about metric of image segmentation. i use the `pix2pix` model to do my task. the input is an input and target like blow. and i already get the prediction result following `!python test.py` and get the result. the prediction is look like blow. thanks.,question,question
228,https://github.com/JaidedAI/EasyOCR/issues/228,Train my own model,thanks for your project. i want to ask how to train my own model.,question,other
204,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/204,training error,"when i run the ""python test.py --dataroot ./datasets/maps --name mapsgan --phase test --no_dropout"" command fowllowing the guidance, some error occured as follows: runtimeerror:",question,question
619,https://github.com/JaidedAI/EasyOCR/issues/619,Doesn't it support macos?,"my system is macos monterey, and the computer is mbp 2019 16inch. i tried to install it using both `pip install easyocr` and `pip install git+git://github.com/jaidedai/easyocr.git`.but just when i run `import easyocr`,zsh says ""segmentation fault"". i wonder if it supports macos and whether i need to install it manually?",question,question
430,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/430,demo_clie.py error,ım open directory and use python demo_cli.py and it result like this,question,question
339,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/339,Problem with demo_cli.py,"c:\users\test\desktop\real-time-voice-cloning-master>python demomem traceback (most recent call last): file ""demo_cli.py"", line 2, in file ""c:\users\test\desktop\real-time-voice-cloning-master\utils\argutils.py"", line 2, in modulenotfounderror: no module named 'numpy' what am i doing wrong?",question,question
103,https://github.com/JaidedAI/EasyOCR/issues/103,[Question] How many words in words file ?,"i plan to rework the french word list (there are many english words in the current one, ane dome are badly written). i plan on pulling a list from wikipedia (fr.wiktionary.org), but this is huge. what is best ? 50.000 most used words ? 170.000 common words ? 268.000 words ? as in the examples there are road signs, which often contains people names in street names, should i also add nouns ?",question,question
91,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/91,We do we need Librispeech alignments when training synthesizer and the vocoder?,"hi, i'm confused. for training synthesizer (tacotron2) we should only need transcript and mel-spectrogram. for training vocoder we should only need the mel-spectrogram and the raw wave form. why do we need to generate force alignments of librispeech for training synthesizer and the vocoder?",question,question
967,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/967,Loss plots from a model that has finished running,"i finished running my cyclegan model for the full 200 epochs, and terminated the visdom server as well. are the final loss plots stored somewhere or can they be regenerated somehow?",question,question
481,https://github.com/JaidedAI/EasyOCR/issues/481,BadZipFile error,"hi guy thanks for your great work! recently, i am using your program to make a tookit with a snap and ouput gui interface. but see the attached image please. i can run the whole program at home, but i can't do it in my office. i don't know what happened, could you please help me out? thanks in advance.",question,other
449,https://github.com/JaidedAI/EasyOCR/issues/449,"In the case of  identity card recognition, sometimes it infers the last 'x' to '8'","thanks for your great work, it performs high-accuracy, but i found in the case of identity card recognition, sometimes it infers the last 'x' to '8' . how can i increse the accuracy of digit recognition?",Performance,question
208,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/208,RuntimeError: inconsistent tensor sizes,"when i run the command: `python train.py --dataroot ./datasets/facades --name facades --model pix2pix --whichnetg unetdirection atob --lambdamode aligned --nosize 0 --inputnc 1 --finesize 80 --loadsize 80` i get the error: `traceback (most recent call last): file ""/home/shani/pycharmprojects/pix2pix/train.py"", line 26, in file ""/home/shani/pycharmprojects/pix2pix/models/pix2pixparameters file ""/home/shani/pycharmprojects/pix2pix/models/pix2pixparallel.py"", line 102, in datafunctions/tensor.py"", line 317, in forward runtimeerror: inconsistent tensor sizes at /opt/conda/conda-bld/pytorch_1511315759884/work/torch/lib/thc/generic/thctensormath.cu:141` it also happens when i use another dataset with 80x80 images. it doesn't happen if i use the default parameters (256). it also doesn't happen with cyclegan, only when using pix2pix.",question,question
935,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/935,Can't execute,"(base) ps d:\real-time-voice-cloning> python demotoolbox.py"", line 2, in file ""d:\real-time-voice-cloning\toolbox\__.py"", line 1, in file ""d:\real-time-voice-cloning\toolbox\ui.py"", line 6, in modulenotfounderror: no module named 'encoder.inference' what am i doing wrong?",question,question
290,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/290,which utterence is chosen for embedding,"if we wanna synthesis speech sounds like speaker a,there are so many utterences in datasets and we can get lots of embedding-speakera-*.npy.does that mean we choose anyone of them for synthesis speech?",question,question
356,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/356,"lack of documentation, setup failure","multiple errors, not worth mentioning them. if someone got it to work on windows please share python version, requirments versions and additional installed software. project seems outdated",deployment,other
289,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/289,How do I use my own mp3?,"i'm playing with the demo, and i only have an option to record, how do i import an audio file? tnx.",question,question
514,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/514,Is modify_commandline_options ever called?,i get the idea of letting models setting the defaults that are specific to them; like the cyclegan using lsgan by default or the pix2pix using vanilla gan by default; but is the modifyoptions ever called? i did a quick search through the repo for 'modifyoptions' and didn't find a single call. is this a bug?,question,question
388,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/388,"In the function backward_D_basic, why operate detach() for fake???","def backwardbasic(self, netd, real, fake): real q1. what is detach() in pytorch? q2. why do detach() for fake only? i'm very new in this field, so kind explanation will be greatly appreciated. thank you.",question,question
543,https://github.com/JaidedAI/EasyOCR/issues/543,Missing chars from latin model,"hi! there are missing characters in the latin model, as i cannot see the `ő` and `ő` characters, that are otherwise available in hungarian. can you add them and update your latin model? off: the hungarian language file is incorrect, so i will provide a language update in a pull request later.",Error,question
708,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/708,How to add more time/delay to the next line?,"hi, i'd like to know where in the code i should change something(time/duration) to add more time/delay to the next line? thanks",question,question
36,https://github.com/JaidedAI/EasyOCR/issues/36,Uyghur Language,i would like to help with the addition of the uyghur language ... what is needed to be done ?,other,other
590,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/590,Why is the voice cloned on the Resemble.AI. platform better than this project?,"when i cloned my voice on the resemble.ai. platform, it was very similar to the real voice. but when i clone my voice with this project, the similarity is very low. do they use the same model? if the same model is used, is the training data different? or the platform finetune the modle to inference?",question,question
817,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/817,telling me to install pretrained models even tho i already have,i've extracted both versions of the pretrained .zip multiple times but it keeps telling me they arent there,other,question
1120,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1120,Victoria,love,other,other
142,https://github.com/JaidedAI/EasyOCR/issues/142,Detail error,"i tried to keep detail is 0 but get an error saying unexpected keyword detail code: reader = easyocr.reader(['en'],gpu = false) reader.readtext(np.asarray(image),detail = 0) typeerror: readtext() got an unexpected keyword argument 'detail'",question,Error
440,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/440,"line 32 at options/base_options.py, type of --load_iter should be ""int"" instead of ""str""","dear author, i tried to apply the pretrained model(pix2pix) and i got a error saying that i guessed that line 32 at options/baseiter should be ""int"" instead of ""str"". after fixing that, my experiment went well.",Error,question
176,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/176,ImportError: Failed to import any qt binding,"`python demotoolbox.py` it throws an error as follows: traceback (most recent call last): file "".\demoqt5agg.py"", line 11, in file ""c:\users\dexbyte\appdata\local\programs\python\python37\lib\site-packages\matplotlib\backends\backendeditor\figureoptions.py"", line 13, in file ""c:\users\dexbyte\appdata\local\programs\python\python37\lib\site-packages\matplotlib\backends\qt_compat.py"", line 158, in importerror: failed to import any qt binding",question,question
1158,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1158,Save output fake images into .tiff,"hi, i really enjoyed implementing cyclegan with my own dataset, this is a great work. could you please let me know how can i change the format of output fake images after testing the model? for example, from default .png to .tiff. thanks",question,question
809,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/809,Can't synthesize and vocode (CUDA out of memory),runtimeerror: cuda out of memory. tried to allocate 20.00 mib (gpu 0; 2.00 gib total capacity; 939.96 mib already allocated; 0 bytes free; 962.00 mib reserved in total by pytorch) this error occurs when i'm trying synthesize and vocode.,question,question
1101,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1101,Exception: Error(s) in loading state_dict for Tocotron,"hi, i'm using the version of rtvc and when i execute sv2tts toolbox i get the following error: did someone receive the same error?",question,question
278,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/278,Can the image-to-image translation by the models trained from cyclegan be real-time?,"dear professor，can the image-to-image translation by the models trained from cyclegan be real-time? i try some experiment, but found it is so slow with low fps.",question,question
129,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/129,Failed to establish a new connection:[Errno 111] connection refused,"when i training the network, the title shows in every epoch. would it be affect something? or i just can't see the training process through web browser?",question,question
493,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/493,Why do I get this error,"traceback (most recent call last): file "".\demo_.py"", line 81, in importerror: dll load failed: the specified module could not be found.",question,question
152,https://github.com/JaidedAI/EasyOCR/issues/152,Ram full. ,detecting 100+ images in loop and end up with ram full error after few images. any solution? runtimeerror: [enforce fail at ..\c10\core\cpuallocator.cpp:72] data. defaultcpuallocator: not enough memory: you tried to allocate 996147200 bytes. buy new ram!,Error,question
875,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/875,Does the weight initialization has effect on the distribution if the images?,"i have a dataset that consists of 9000 synthetic aperture radar images normalized to [0,1]. one of them is visualized in the histogram below. all of the images follow the same structure where most of the pixels are around 0 (e.g. 0.0235411) and only a few are approaching the maximum value (here 1). when i try to train cyclegan, i see that the network is performing very bad and i guess this is because of the way the weights are initialized? when i then remove the 2% percentile in the upper and lower from the image, the network performs much better. i assume the reason for this is because of the way the weights are initialized. can any help me understand why the network trains better when the images seem to be normally distributed? and what do the initialization assume before training? i use instancenormalization if that helps.",question,question
518,https://github.com/JaidedAI/EasyOCR/issues/518,How to merge overlapping bounding boxes,i have been trying to use the parameters provided to merge the overlapping bounding boxes but i didn't reach a solution yet. any ideas on how to do this? thanks!,question,question
198,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/198,Is there any reasons for using batch_size = 1 in CycleGAN?,"hi, i am a big fan of cyclegan and its authors. what a great work. however, i have a question regarding batch size. is there any theoretic reasons for using batch_size = 1 in cyclegan? thanks.",question,question
832,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/832,Report on Single Voice Training Results,"hello @blue-fish and all, i am running the demotoolbox.py) works fine on this setup. my project is to use the toolbox to clone 15 voices from a computer simulation (to be able to add additional voice material (.wav files) in those voices back into the sim), one voice at a time, using the single voice method described in issue #437 ** i also attempted vocoder preprocessing of the single voice pretrained synthesizer, and ran into several issues, which i will open in a new issues thread. i could not get the vocoder_preprocess.py to work properly. overall, after learning how to properly do single-voice training, i was pleased with the output. i can be a lot better, but should be fine for purposes of my project. any recommendations on improving the voice quality (especially of v13m) would be appreciated. regards, tomcattwo",other,other
94,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/94,How to extract/save the loss function,"sorry for my stupid question. i am wondering in the training and testing process, is there any way to extract the loss values for each input and save them somewhere?",question,question
580,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/580,Getting a type error while Training,i am trying to train the model but getting the following error. ` self.dataset = dataset_() missing 2 required positional arguments: 'bases' and 'namespace'`,deployment,question
885,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/885,generated audio file with maximum length of 25 seconds,hello! today i was surprised that the audio file for a long text was cut exactly after 25 seconds. i could reproduce this with other long text block too. is this a known issue/restriction? i am using the griffin synthesizer and a python api. best regards marc,question,question
718,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/718,Order of training networks in CycleGANModel,"hey! i'm slightly confused about the way the parameters of the network are updated in `cycleganmodel`. 1. the `algorithm 1` suggests optimizing over `d` first followed by `g`. however, in the current implementation `g` seems to be optimized before `d`. 2. the more important question is that shouldn't `g` be optimized over the latest `d`'s predictions? i believe we'll be solving the mini-max game only then. in summary, what i believe should happen in `cycleganmodel.optimize_parameters` is:",other,question
10,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/10,Pretrained models,do you have any cyclegan pretrained models available that use pytorch?,other,other
654,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/654,Stuck on Successfully opened dynamic library cudnn64_7.dll,"i did all the installations and when i record myself and press on synthetize only i get stuck on this line in the cli: `2021-02-10 00:45:37.068088: i tensorflow/streamloader.cc:44] successfully opened dynamic library cudnn64pretrained\tacotronexecutor/platform/default/dso100.dll arguments: warning: you did not pass a root directory for datasets as argument. the recognized datasets are: feel free to add your own. you can still use the toolbox by recording samples yourself.` the cli since the loading checkpoint: `loading checkpoint: synthesizer\savedpretrained\tacotronfeatureexecutor/platform/default/dsoruntime/gpu/gpuexecutor/platform/default/dso100.dll 2021-02-10 00:44:03.835372: i tensorflow/streamloader.cc:44] successfully opened dynamic library cublas64executor/platform/default/dso100.dll 2021-02-10 00:44:03.839463: i tensorflow/streamloader.cc:44] successfully opened dynamic library curand64executor/platform/default/dso100.dll 2021-02-10 00:44:03.845400: i tensorflow/streamloader.cc:44] successfully opened dynamic library cusparse64executor/platform/default/dso7.dll 2021-02-10 00:44:03.852557: i tensorflow/core/commondevice.cc:1746] adding visible gpu devices: 0 2021-02-10 00:44:58.939104: i tensorflow/core/commondevice.cc:1159] device interconnect streamexecutor with strength 1 edge matrix: 2021-02-10 00:44:58.939228: i tensorflow/core/commondevice.cc:1165] 0 2021-02-10 00:44:58.941133: i tensorflow/core/commondevice.cc:1178] 0: n 2021-02-10 00:44:58.941796: i tensorflow/core/commondevice.cc:1304] created tensorflow device (/job:localhost/replica:0/task:0/device:gpu:0 with 8551 mb memory) -> physical gpu (device: 0, name: geforce rtx 3080, pci bus id: 0000:21:00.0, compute capability: 8.6) 2021-02-10 00:44:59.515354: i tensorflow/streamloader.cc:44] successfully opened dynamic library cublas64executor/platform/default/dso7.dll` am i missing something? thx",deployment,other
538,https://github.com/JaidedAI/EasyOCR/issues/538,About output confident level,"really thanks for this awesome project!! i am now using easyocr to solve our problem. however, i have question about confident level. according to your project code, confident level will be generated by `custom_mean` function . is there any reference or literature about this formula ?",other,question
228,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/228,integer division or modulo by zero ,when i try to load a dataset the program says this anyone know how to fix this?,question,question
24,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/24,How to inform the dataset?,"hi, i downloaded the dataset librispeech/train-clean-100 and i have it in the project root folder. but when i try to execute the demo_toolbox.py it tells me that 'you do not have any of the recognized datasets in librispeech' maybe i'm passing the path wrong? python demo-toolbox.py -d librispeech do i need to do something different?",question,question
1134,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1134,is exact image  registration for image pairs compulsory to get good results in pix2pix?,"sir ,",question,question
1155,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1155,pix2pix test input with only sketch?,do i need to get a photo with sketch and the sketch model? could i just input a sketch?,question,question
1320,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1320,Rectangle Images,"hi, i noticed that we should crop rectangle image to square patches during training. but why rectangle image is supported during test? thank you in advance for any helps!",question,question
601,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/601,"My voices always come out weird which doesnt happen to others, and it says I'm missing a noise removal component.","if you need me to send any logs or screenshare myself running tests let me know,",question,other
682,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/682,Issues with pretrained model,"i downloaded and installed the pretrained models, but when i try to run the toolbox it says that there are no models.",other,question
251,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/251,Error in python: double free or corruption,"i never had issues running train.py or test.py, and then randomly i got this error: ======= backtrace: ========= /lib/x8664-linux-gnu/libc.so.6(+0x8037a)[0x7f182102a37a] /lib/x8664-linux-gnu/libcuda.so.1(+0x2edd7c)[0x7f18074fdd7c] /usr/lib/x8664-linux-gnu/libcuda.so.1(+0x2ee064)[0x7f18074fe064] /usr/lib/x8664-linux-gnu/libcuda.so.1(+0x1baabc)[0x7f18073caabc] /usr/lib/x8664-linux-gnu/libpthread.so.0(+0xea99)[0x7f1821382a99] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61(+0x4e919)[0x7f18039a9919] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61(+0x1800a)[0x7f180397300a] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61(+0x1bceb)[0x7f1803976ceb] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61(cudasetdevice+0x47)[0x7f180399c1a7] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/64-linux-gnu.so(setdevicei+0x9)[0x7f18048f48c9] /home/alessandro/anaconda/lib/python3.6/site-packages/torch/64-linux-gnu.so(setdeviceobjects0pycfunctionpyevalpyevalpyevalevalcodeex+0x329)[0x56383a2d1529] python(pyevalfileexflags+0xa1)[0x56383a34eef1] python(pyrunmain+0x648)[0x56383a352c28] python(main+0xee)[0x56383a21a71e] /lib/x86start_main+0xf0)[0x7f1820fca830] python(+0x1c7c98)[0x56383a301c98] now i get it everytime i try to run again the code. have you seen this before? thank you!",question,question
244,https://github.com/JaidedAI/EasyOCR/issues/244,Information about training dataset for latin characters,"hi there, thanks for your great work! althrough i know you will open the training dataset in phase 2, could you share more information about your training dataset for latin characters recognition for now? i know possible dataset includes: 1. mjsynth dataset 2. synthtext in the wild dataset 3. text generated by textrecognitiondatagenerator do you use the aboved datasets train your latin model? thank you very much!",other,question
1185,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1185,Set epoch using --n_epoch,i am use argument --nepoch is setting max epoch to what i want or is other command?,question,question
346,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/346,Argument for datasets_root,please help. i don't know how to fix the following argument. can someone help me get the datasets as an argument? thanks in advance.,question,question
144,https://github.com/JaidedAI/EasyOCR/issues/144,installation issues on several platforms,"i was unable to install the package with ""pip install easyocr"" on following platforms: - notebook with linux mint 19: `cannot paste what's missing, because i tried installing missing dependencies and now have more errors than before` - raspberry pi 0w: it would be beneficial to clearly specify needed dependencies and add this to installation instructions, g",deployment,deployment
49,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/49,Can't install requirements on OSX,"i used the network install for cuda before trying this and have pyenv local set to 3.7, anyone know why i'm getting this error? thanks! osx version: 10.12.6 (16g2016) here's the stack trace:",question,deployment
263,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/263,Demo Error,when i try to run the demo i get the following error: modulenotfounderror: no module named 'unidecode any ideas?,question,question
388,https://github.com/JaidedAI/EasyOCR/issues/388,How change to my ready custom network ,"hello. i have trained a none-resnet-bilstm-ctc network with the classes 0123456789abcdefghijklmnopqrstuvwxyzñ- in the original repo, using the data generator mentioned in this repository. how could i use my own .pth it in this repository? i am working on it but i cant get it",question,question
428,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/428,Windows 10 - DLL Error,"i've been stuck on this step for 2 days and i know people have posted this problem before, but the answers have not been too helpful to me considering i am retarded, technologically speaking. anyway, when i type: > python democore\python\pywraptensorflowcore\python\pywrapinternal.py"", line 28, in > tensorflowimportcore\python\pywrapinternal.py"", line 24, in swighelper > module('tensorflowmodule > return loaddynamic > return cli.py"", line 4, in > from synthesizer.inference import synthesizer > file ""c:\users\hugow\downloads\real-time-voice-cloning-master\real-time-voice-cloning-master\synthesizer\inference.py"", line 1, in > from synthesizer.tacotron2 import tacotron2 > file ""c:\users\hugow\downloads\real-time-voice-cloning-master\real-time-voice-cloning-master\synthesizer\tacotron2.py"", line 3, in > from synthesizer.models import createcore import > file ""c:\users\hugow\appdata\local\programs\python\python37\lib\site-packages\tensorflowtensorflowpywrapinternal = swighelper() > file ""c:\users\hugow\appdata\local\programs\python\python37\lib\site-packages\tensorflowtensorflowimportmod = imp.loadpywrapinternal', fp, pathname, description) > file ""c:\users\hugow\appdata\local\programs\python\python37\lib\imp.py"", line 242, in loaddynamic(name, filename, file) > file ""c:\users\hugow\appdata\local\programs\python\python37\lib\imp.py"", line 342, in loadload(spec) > importerror: dll load failed: a dynamic link library (dll) initialization routine failed. > > > failed to load the native tensorflow runtime. > > see > > for some common reasons and solutions. include the entire stack trace > above this error message when asking for help. > please, if you have anything to help, can you literally spell it out for me because i cannot stress enough that i have no idea what i'm doing. thanks.",question,question
787,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/787,LibriTTS & older models,"hey - i've been playing with the tensorflow version of this repo with a libritts model for some time now & have had some good results from it. the model i've been using was from here & had a partial train of a libritts dataset which was really good for punctuation etc. just upgraded my gpu to a 3080ti (rare i know!) & tried to get the repo working with this gpu.. the cuda sdk doesn't appear to work unfortunately (10.1) & every time i try to load an audio file to clone the toolkit just hangs.. so i've ended up pulling the latest repo & pretrained models, but noticed there's no punctuation - which is a real shame... would appreciate any thoughts on either getting the older repo working or how to get libritts into the mix? ()also if needed i can provide a 5950x and gpu to train models if it would help restore some of the functionality around this)",other,other
119,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/119,encoder training error and stop,"when i training encoder without change anything of your code the program will stop in thousands step，the error is dataloader stop. when the training start，i discover that the memory used raise about 100m every step and at last my machine‘s 200g memory will be all used. at first, i think that may be the reason of dataloader of your code. so i run the encoder train with a new dataloader coder write by myself, but that memory raise up still happen and training will stop too. so i think whether the loss of your code cause the memory leak?",other,question
645,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/645,Compatibility update with newer librosa version in collab,"hi, recently the program stopped working in collab due to librosa 0.8.0 update, giving error ""module 'librosa' has no attribute 'output'"", please solve this problem",deployment,other
283,https://github.com/JaidedAI/EasyOCR/issues/283,How to generate Uighur text images for training？,"i used the pil to generate uighur text images, but the letters were separated. could you give me some suggestions ? thank you.",question,question
508,https://github.com/JaidedAI/EasyOCR/issues/508,Heatmaps,"hello, is it possible to get heatmaps from easyocr? thank you in advance.",question,other
624,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/624,Feature request: SSIM loss,"for tensorflow, has success using structural similarity (ssim) loss for cycle consistency loss. it shows the very impressive results do you have the plan to consider the loss in the project? thanks",other,other
401,https://github.com/JaidedAI/EasyOCR/issues/401,Lower accuracy when selecting english and kannada togehter,hey! firstly great project! i was trying easyocr in two modes- 1. only english - the accuracy is good but also attempts to predict kannada 2. kannada and english- the accuracy drops and the prediction for english too is very bad the results are for the same image. any ideas on how i can resolve this issue?,question,question
438,https://github.com/JaidedAI/EasyOCR/issues/438,How to recognize negative number？,i used this model to recognize negative number like '-264.27' with cpu only. but i get the list ['264.27'] without the negative sign.its kinda weird. what's wrong with my code?any suggestions? thanks a lot!,Performance,question
659,https://github.com/JaidedAI/EasyOCR/issues/659,Request for German recognition model,"a gentle request for recognition model for german language thanks a lot for this nice library. can you please add a `german_g2` recognition model at your releases? note: i have searched but could not find, if it already exists, please let me know",other,other
1475,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1475,Using evaluate.py on Cityscapes,"hello. what kind of folder structure does /pytorch-cyclegan-and-pix2pix/scripts/evaloutput_images 1 was set but yet the folder is empty and the results file is also empty can you please provide some clarification, or if someone has managed to get this evaluation script to run let me know the folder layout of your images. thanks.",Error,question
771,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/771,Style Transfer Parameters,did the pre-trained style transfer models all use the default parameters i.e 0.5 for the lambda_identity?,question,question
206,https://github.com/JaidedAI/EasyOCR/issues/206,Different unicode for the same abkhazian character,"hello, the issue is different unicode for the same abkhazian character: ԥԥ ҧҧ ӷӷ ҕҕ is there an easy way to implement this?",Error,question
98,https://github.com/JaidedAI/EasyOCR/issues/98,Latvian language incorrect,"hey, latvian language is incorrect. does not recognize long marks (macron). output: [([[42, 12], [463, 12], [463, 62], [42, 62]], 'gada ir tikai divas dienas,', 0.057680848985910416), ([[53, 57], [449, 57], [449, 101], [53, 101]], 'kuras mes neko nevaram', 0.41570523381233215), ([[39, 101], [143, 101], [143, 143], [39, 143]], 'iesakt.', 0.9080015420913696), ([[155, 101], [463, 101], [463, 143], [155, 143]], 'viena ir vakardiena', 0.32598042488098145), ([[185, 143], [469, 143], [469, 187], [185, 187]], 'ritdiena. tadejadi', 0.2768159508705139), ([[35, 149], [153, 149], [153, 185], [35, 185]], 'un otra', 0.5262686610221863), ([[66, 184], [434, 184], [434, 232], [66, 232]], 'šodien ir ista diena, lai', 0.17492368817329407), ([[63, 231], [442, 231], [442, 272], [63, 272]], 'miletu, ticetu, daritu un', 0.17923244833946228), ([[68, 266], [267, 266], [267, 320], [68, 320]], 'galvenokart', 0.8032127618789673), ([[295, 275], [431, 275], [431, 313], [295, 313]], 'dzivotu!', 0.7159291505813599)]",Performance,Performance
90,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/90,Using imgs with nc=1,"hey! thanks for sharing this great code! the code workes nicely with nc=3 images but now i want to use the code with imgs that have nc=1. (i checked that the images have 8bit) i use the command: python train.py --dataroot ./datasets/pic2seg2 --name amneogan --nonc 1 --outputcycle\web... traceback (most recent call last): file ""train.py"", line 27, in file ""c:\users\eulsen\documents\programs\python\pytorch-cyclegan-and-pix2pix\models\cyclemodel.py"", line 159, in optimizegang file ""c:\users\eulsen\documents\programs\python\pytorch-cyclegan-and-pix2pix\models\networks.py"", line 175, in forward file ""c:\users\eulsen\anaconda3\lib\site-packages\torch\nn\parallel\dataparallel file ""c:\users\eulsen\anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 206, in _statusparam help would be much appreciated!",other,question
163,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/163,Generate HD images,"hello, is it possible to use this architecture to generate hd images? the default settings give you low-definition squared images. did someone try with rectangular hd pics? what i basically would like to do a training on cityscapes images with their real shape. thanks in advance. cheers, filippo",question,question
872,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/872,training epochs,how many epochs and batches does the default training of the three networks have?,question,question
694,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/694,A typo in the code,"hi junyan, thank you for sharing the code. i found a typo in pytorch-cyclegan-and-pix2pix/models/networks.py file in line 202. the output name string might be %netd instead of %net. jing",Error,Error
724,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/724,"Stuck on ""WARNING:root:Setting up a new session""","i downloaded the `facades` dataset. i then run `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction btoa`, but i'm stuck at `warning:root:setting up a new session` even after a few hours, it says there and doesn't seem to progress. why is this?",question,question
1001,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1001,Error in pix2pix backward_G?,"hi, i have a question about the implementation of the `backwardmodel.py` i see the following def backward_g(self): here we call on `criteriongan` to make a prediction for a fake input, but we set the target value as if it were a `true` input. shouldnt that be set to false instead? of not, why not? btw. i understand the concept that the generator needs to fool the discriminator. but thought this was done by still giving the right information to the discriminator, at all times. now we are just fooling the discriminator by giving wrong information...",question,question
234,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/234,Stop and retrieve training ,"how to continue training from the last saving epoch in pix2pix model? i mean, i want to stop training to assess the generated images and then again start training from the last point. thanks",question,question
994,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/994,How to implement in other script,"i want to somehow use an external script to send texts for it to say. but the following code doesn't work. i have also commented out the show() in the ui.py script as to not show the ui. this is my code (this is in a modified demodict) print(""init #2"") tb.loadbrowser(tb, path(""e:/voicecloner/voices/charlie.flac)) print(""init #3"") tb.synthesize(tb, ""hello. this is a text, and my name is charlie."")` but when i run it it prints: `arguments: init #1 warning: you did not pass a root directory for datasets as argument. the recognized datasets are: feel free to add your own. you can still use the toolbox by recording samples yourself. ` then it jumps down one line, but init#2 never shows up. why? sorry btw if this is the wrong place to ask this.",question,question
117,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/117,ModuleNotFoundError: No module named 'tensorflow.contrib.seq2seq',when running demo_cli.py python = 3.7.4 tensorflow = 2.0 rc cuda = 10.1 cudnn = installed for right cuda version windows = 10,deployment,other
72,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/72,Many issues while running,can someone show complete steps to run this project using colab? i am getting a number of issues with this.,question,question
1217,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1217,How can I make the output image result bigger?,i am in high awe of your work. and i noticed that all the fake_b is 256256. looking forward to your reply.,question,question
138,https://github.com/JaidedAI/EasyOCR/issues/138,RuntimeWarning: divide by zero,thanks for all the effort you put in this project! i encountered the following errors today; it seems divisions are done without error handling:,Error,question
275,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/275,Speaker verification implementation,"i need just the speaker verification part which is the implementation of paper, how i can proceed to get it please?",question,question
912,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/912,can i run this repository using cpu only?,can i do training with gpu and cuda as i dont have these resources right now? i have only cpu. thanks,question,question
260,https://github.com/JaidedAI/EasyOCR/issues/260,EasyOCR tutorials,"hi easyocr team -- i just wanted to take a second and let you know how much i appreciate all the hard work put into this library. it's the most intuitive ocr api i have used (and quite frankly, makes it hard to go back to tesseract 😉 ) my name is adrian, i run the popular computer vision, deep learning, and opencv blog, . my apologies if this is not the correct place to post this, but i wanted to let you know that i just published a tutorial on ""getting started with easyocr for optical character recognition"": i would love to write more tutorials on easyocr in the future, so if there is a common question/use case/application that you see asked here on github but you can't/don't want to include it as an example in the docs, please let me know, and i'll certainly see if i can write a tutorial on it to help the community. have a wonderful day and thank you again for all the incredible hard work that has gone into easyocr 😄",other,other
263,https://github.com/JaidedAI/EasyOCR/issues/263,Pali (language of the Buddhist cannon),"please add this old but used language. it is used extensively in written form in theravada buddhist tradition (sri lanka, myanmar, thailand, cambodia, laos) and all buddhist branches around the word. - all wordlist and character list is in the attachment - the only special characters are: ā ī ū ṃ ṁ ṅ ñ ṭ ḍ ṇ ḷ - the rest of the characters are standard latin --- pali (/ˈpɑːli/) or magadhan,[a] is a middle indo-aryan liturgical language native to the indian subcontinent. it is widely studied because it is the language of the pāli canon or tipiṭaka and is the sacred language of theravāda buddhism. ** latin :----------------------------------------------------------- ------------------------------------------------------------ `pli` ---",other,other
349,https://github.com/JaidedAI/EasyOCR/issues/349,Chinese dataset,would you like to provide the dataset for chinese training? my e-mail is : huangwenlu@bupt.edu.cn. thank you very much.,other,other
378,https://github.com/JaidedAI/EasyOCR/issues/378,LOGGER is not imported!,"after installation via pip install, tried to process an image in pil format, the following error appears: it looks like you forgot to import logger",question,question
606,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/606,help! Window 10 looks not work ,"hi, i m brazilian. i hope this could use portuguese language aswell for help my friend favour. i did the first step as ffmpeg, and installed python but it doesn't put the path in system variable. weird. i tried to open something and it didnt work. i should install anaconda , right? i had that idea i was lost. i need paths for variable to be able to open a file. i still am not master or good at python. i wanna learn aswell. i heard of inteligence artifical in 2014 it is good idea for me to make my robot character as xnessax. i want my baby as python-xnessax.",other,question
188,https://github.com/JaidedAI/EasyOCR/issues/188,Comma channel,suggest having a gitter or slack channel for development chat??,other,other
256,https://github.com/JaidedAI/EasyOCR/issues/256,Alpha-numeric detection ,"hi, thanks for such an amazing ocr solution, i just had one query, as per my analysis it is not capable of handling alphanumeric words if they are longer than 4-5 chars. is there anyway to do that?",question,question
618,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/618,What should I do if the synthetic image in target is similar the real image in source?,"hello, i am using the cyclegan for making a synthetic image from source domain a to target domain b. after training 10.000 epochs from dataset with 10.000 images. i used u-net generator with ngf 64 and discriminator ndf 64. the size of the last layer in d network is 7x7. the problem is that the synthetic image in the target domain similar to the real image in the source domain. what is happening in my problem? thanks so much",question,question
1229,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1229,How to set options to train pix2pix with 128x128 data?,"of course i have searched and found these: #406 #578 however, i didn't get any useful infomation, in #406, there is only one reply by @junyanz: > you can resize your images to 200x200 by setting --displaysize=128, i got this error: > valueerror: expected more than 1 value per channel when training, got input size [1, 512, 1, 1] so anyone can help me? thx.",question,question
668,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/668,Confusion with CycleGAN testing regarding domains,hi i am trying to train a cyclegan such that i can translate images from domain b to domain a. after training the initial model i wanted to test it on some out-of-training-set images from domain b. so i used: `python test.py --dataroot datasets/mycyclegandropout` however when i open the index.html the view the results i am confused. as i understand it realnetb.pth -> latestg.pth` does this have something to do with the --direction flag? or am i still confused about the labeling of the results?,question,question
309,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/309,Which versino of CUDA/CuDNN/Tensorflow? CuDNN fails on initializing,"hi, i am using tensorflow-gpu 1.14.0, cuda 10.0 and cudnn 7.4.1 on windows 10 (python 3.7) but i can't manage to make it work. demotoolbox.py sends an error when i try to synthetize an audio. i keep getting the error : ""failed to get convolution algorithm. this is probably because cudnn failed to initialize"" it seems to be a version problem : ""loaded runtime cudnn library: 7.2.1 but source was compiled with: 7.4.1. cudnn library major and minor version needs to match or have higher minor version in case of cudnn 7.0 or later version. if using a binary install, upgrade your cudnn library. if building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration."" which versions should i be running? i've attached my log",deployment,question
165,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/165,"the traindata too big to out of memory, ",does it support stream for train data in current project?,question,question
1188,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1188,CityScape evaluation,"sorry for raising duplicate issues. i have read instructions and previous issues carefully. i still have some concerns about evaluation. python ./scripts/evaldir /path/to/original/cityscapes/dataset/ --resultdir /path/to/output/directory/ so the cityscapestrainvaltest/leftimg8bit/ or gtfine_trainvaltest/gtfine/? if i use the first one, i can run it but producing all 0 outputs even using real images in results. if i use the second one, error ""no module names labels"" occur. sorry for bothering you at the time approaching the cvpr deadline. many thanks.",question,question
161,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/161,tensor2im() in util ?,"i can not understand this code in the function util.tensor2im() —— why do we need ""transpose"" and ""+1"" ，""/2"" ? look forward to your explanation.",question,question
1479,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1479,Pix2Pix discriminator is too strong even when I use the paper's parameters,"i'm trying to write my version of pix2pix models in pytorch following the tensorflow tutorial . i wrote my version models by hand and followed the tutorial to ensure i didn't make any fatal mistakes. i tried training the models, but the discriminator's loss goes to 0 after the first epoch, and the generator's gan loss only goes up. i started adding anything small that i thought might've been the problem but was still the same. i then copied the unet model and nlaterdiscriminator from this repo to make sure, but i still got the same results. i tried switching the loss from bce to mse, and no improvement at all. i'm sure my code is 1-1 to this repo (on high-level terms, not a line-by-line basis), so i can't think of anything i missed by mistake. here's a link to the notebook: i would appreciate it if you could point out what i've done wrong since my code is based on this repo, so there shouldn't be any issues in theory.",question,question
122,https://github.com/JaidedAI/EasyOCR/issues/122,Error in easyoce.Reader module,"i am facing issue with easyocr.reader module. i have successfully imported easyocr, but face issue on following line. reader = easyocr.reader(['ch_sim', 'en']) error is following. attributeerror: module 'easyocr' has no attribute 'reader'",question,question
883,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/883,TTS outputing different words than the ones typed in,"hi, i am putting my hands on your fun project! actually i am trying to clone a voice in french. i edited a short recording and made 16 extracts (22khz mono 32 pcm microsoft wav ranging from 1 to 5 seconds) out of it that i manually transcripted following the file hierarchy @blue-fish . i also added some characters to . then i launched the training with the command you gave : `python3 synthesizeraudio.py datasetsname libritts --subfolders train-clean-100 --noroot was not inflating. indeed it seems that the command output the model in models/myrungta directory. i know you wrote that it was not necessary to train the vocider, but if i miss mels_gta directory maybe something went wrong during the training. or is everything ok ? is it worth it to continue with editing 12 minutes or more from this voice or i made something wrong in the process ? can you help me out ?",question,question
680,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/680,Generating random outputs after some epochs of Training,i am training a cyclegan on my own data set of 10000 tables. input is a natural table and output should be the skeleton of the table. for the skeleton i'm generating random table skeletons using a python script. the training went ok through first 5-8 epochs and after that it is generating some random tables with random noise. also the gan_a loss is oscillating so much and my batch size is 4. does cycle gan become worse after few epochs of training or am i doing something wrong?. attached the loss function with respect to the number of epochs(25).,question,question
535,https://github.com/JaidedAI/EasyOCR/issues/535,memory keep increases (CPU),i'm running this my memory keeps on increasing didn't find any clue,other,question
52,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/52,"can not download anyone , could you tell me what can i do for that ?","wget not found , i have know that . sorry about that",question,question
729,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/729,Loading multiple models when training a network,"hello, i'm working on an implementation where i am extending an architecture to use a third pre-trained network. i've been looking at the code and i can't think of a non-messy way to load a model in-code without copying the options from the original model and changing properties to then create the third model again. is there a more straightforward way to do this? just as an example, let's say we have our network a, and our other network b. b is trained on some data and then the model is saved. then while training a, i want to load the pre-trained b and do something with it. how would i go about loading b in-code?",question,question
596,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/596,Numpy fails to pass a sanity check due to a bug in the windows runtime,"i am getting this error: ** on entry to dhseqr parameter number 4 had an illegal value traceback (most recent call last): file ""demowincheck runtimeerror: the current numpy installation ('c:\\users\\olive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\__.py') fails to pass a sanity check due to a bug in the windows runtime. see this issue for more information: can someone help please, thank you.",deployment,question
772,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/772,How to automize the input from a text file,"hello, than you for this amazing work. i'm currently working on automising the input of the text to synthesize via a text file without reloading the ui ? is there a specific class or function where i can work to make this. thank you",question,question
274,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/274,Import Helper Issue,"traceback (most recent call last): file ""demo_.py"", line 1, in file ""c:\users\kille\desktop\real-time-voice-cloning-master\synthesizer\models\tacotron.py"", line 4, in file ""c:\users\kille\desktop\real-time-voice-cloning-master\synthesizer\models\helpers.py"", line 3, in ** not too sure what the issue is and why it can't locate it. any help?",question,other
249,https://github.com/JaidedAI/EasyOCR/issues/249,AWS Neo compilation on Raspbery Pi,"hey! i was wondering if it was possible to compile the pytorch craft model with aws neo compilation services. is there currently a way to do that right now, since it was mentioned that an onnx export mode would be coming soon? i exported the model to onnx using the code bellow and used an input tensor shape of (1,3,640,640). is this code correct, or is there anything else i have to do?",question,question
800,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/800,Question about the code.,"hi, junyan: i have two new question about the code. first is `webdir, opt.name, '%siter`. this web_dir is probably not very precise. secondly, i see a pull request about 'spectral normalization' in history. but no implementation in master code. i just want to know are there some wrong in the pull code? thanks! best regards, eric kani",question,question
488,https://github.com/JaidedAI/EasyOCR/issues/488,"how to recognition language like ""Uyghur""","hi, thank you for your work. i have a question about recognition language like ""uyghur"". as we know, uyghur character dic is but how to recognition uyghur words according to the character dic you provide, word may be like",question,question
601,https://github.com/JaidedAI/EasyOCR/issues/601,Downloading detection model too slow,"hi, i when i run a code in windows, it display ""downloading detection model, please wait. this may take several minutes depending upon your network connection."" then it keep downloading for a long time. even though i plug a vpn , its progress is very slow i just use ""pip install easyocr"" to install and the code is here: import easyocr reader = easyocr.reader(['en']) result = reader.readtext('1.jpg') result",question,question
421,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/421,Python Crashing after starting demo_toolbox.py or demo_cli.py,"cli.py c:\users\tomas\anaconda3\envs\onlyone\lib\site-packages\h5py\_builttuple) warning! **** the hdf5 header files used to compile this application do not match the version used by the hdf5 library to which this application is linked. data corruption or segmentation faults may occur if the application continues. this can happen when an application was compiled by one version of hdf5 but linked with a different version of static or shared hdf5 library. you should recompile the application or check your shared library related settings such as 'ldpath'. you can, at your own risk, disable this warning by setting the environment variable 'hdf5version is there a way to fix it?",question,other
310,https://github.com/JaidedAI/EasyOCR/issues/310,experimental lite recognition models,some tokens go missing and some words gets swapped in the lighter recognition models released for kannada and telugu..do you plan to release normal sized models for these languages?,question,other
182,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/182,compared with Melgan?,@corentinj how is your repo compared with melgan?,question,question
745,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/745,UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.,(venv) c:\users\electrobot\desktop\voices\real-time-voice-cloning>python demomp3_support' option to proceed without support for mp3 files. (venv) c:\users\electrobot\desktop\voices\real-time-voice-cloning>pip list package version ----------------- ------------ appdirs 1.4.4 audioread 2.1.9 certifi 2020.12.5 cffi 1.14.5 chardet 4.0.0 cycler 0.10.0 decorator 5.0.7 dill 0.3.3 idna 2.10 inflect 5.3.0 joblib 1.0.1 jsonpatch 1.32 jsonpointer 2.1 kiwisolver 1.3.1 librosa 0.8.0 llvmlite 0.36.0 matplotlib 3.4.1 multiprocess 0.70.11.1 numba 0.53.1 numpy 1.19.3 packaging 20.9 pillow 8.2.0 pip 21.0.1 pooch 1.3.0 pycparser 2.20 pynndescent 0.5.2 pyparsing 2.4.7 pyqt5 5.15.4 pyqt5-qt5 5.15.2 pyqt5-sip 12.8.1 python-dateutil 2.8.1 pyzmq 22.0.3 requests 2.25.1 resampy 0.2.2 scikit-learn 0.24.1 scipy 1.6.2 setuptools 56.0.0 six 1.15.0 sounddevice 0.4.1 soundfile 0.10.3.post1 threadpoolctl 2.1.0 torch 1.8.1+cu102 torchaudio 0.8.1 torchfile 0.1.0 torchvision 0.9.1+cu102 tornado 6.1 tqdm 4.60.0 typing-extensions 3.7.4.3 umap-learn 0.5.1 unidecode 1.2.0 urllib3 1.26.4 visdom 0.1.8.9 websocket-client 0.58.0,question,other
273,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/273,Combine with mobile app,hey. is there a possibility of reducing the vocoder time for same voice but different text?,question,question
1095,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1095,Continue/ resume training: update learning rate,"hello, could you please tell me how to set the right learning rate for the first epoch in continue training? when i want from 151 epoch to go on training (100 epochs for lrlambda * initiallr be 0.0001? any reply will be appreciated.",question,question
380,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/380,Some troubles,"finally,i can run the train.py,but still with some trobles. q1: i use the cpu model,then i set gpudataset file ""c:\users\xiang\downloads\pytorch-cyclegan-and-pix2pix-master\pytorch-cyclegan-and-pix2pix-master\data\unalignedfolder.py"", line 26, in makeid 0 to run.and input 'command python -m visdom.server'.found there was nothing in ' there was only a opt.txt in '\checkpoints\mapscyclegan_dataset.sh maps' mean? and can i write chinese to describe my questions?",question,question
448,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/448,Projections points stuck in 4,"help,my projections points never changes,i add vocoder and synthesizer,but still in 4...i need download something ?",Error,Error
629,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/629,Producing an output image from pre-trained model,"i have a pretrained model for satellite->map, that i've tested and i like the results. now how can i use this model beyond testing? i'd like to be able to provide the file path for a single satellite image at a time. what can i do here, and how can i assure a reconstructed image is produced for comparison?",question,question
608,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/608,"Real_A, Real_B saving error?","hi i have modified your code to add a new mode named gray which translate a(one channel)->b(rgb) (code is at in my `_a, fakeb is strange: fakeb should at least resemble in outline but it isn't. real_a is complete mess. i checked all what i can think out of. i believe in some internal code key 'a"" in the dictionary is assumed to be some 3-dimensional rgb.",question,Error
1186,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1186,How to set epoch when using train.py?,"i couldn't find a direct argument that train.py takes for increasing the number of epochs, so how would i increase the number of epochs that pix2pix trains for?",question,question
914,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/914,Training on RTX 3090. Batch Sizes and other parameters?,"hi, sorry that i have to ask these questions here. if there was a discord or something like that i could ask there. i have access to a rtx 3090, there i want to use it's vram to increase batch sizes. i have learned that higher batch sizes = faster and better progress, at least to some level. if yes: where do i find the parameters for the batch sizes of the three models? are there locations correct: encoder: maybe i can increase it to 64 like it was in the original ge2e synthesizer: the last value in the brackets ""12""? vocoder: what are reasonable batch sizes for rtx 3090? - i have no frame of reference. - also, is it necessary to adjust lossrate, speakerbatch, utterancesspeaker or any other parameter when batch-size gets increased. if no: would also be good to know. are there any other advantages i could take with this monster gpu? how long should decent training take? again i have no frame of reference. thanks a lot in advance to anyone who can get me some understanding to this new and extremely interesting topic!",question,question
360,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/360,How do I fix this error? '_cuda_setDevice',"hello, i have been following the instruction up to the training stage, where i got stuck. seems like this is a popular question, and after looking at the previous answers, i have been trying to install different versions of pytorch, but didn't succeed. i couldn't figure out how to run it on the cpu like the other answer suggests. (`--gpu_ids 0`) running on mac os high sierra 10.13.3 python 2.7 version. installed : pip, anaconda2, pytorch (python2.7, cuda 9.2), visdom, dominate, brew, bash, wget, cuda developer toolkit, cudnn i also installed python3 using brew, but switching the path became problematic, so i stuck to 2.7 versions. i have tried installing pytorch 0.4.0, although the terminal seems to be using 0.4.1 i tried downgrading to pytorch 0.3.1 and the same error appears. (#267)",question,question
699,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/699,How to properly change the sample rate from 16000Hz to 24/48000Hz?,"i just discovered that many of my source audios i am using to train my models are sampled at 48000hz, and that the default sample rate for the repo is 16000hz. i figured that this 'downsampling' was part of the cause of lower audio quality of the audios i was generating; i'd like to mitigate that. a bit of research led me to change the hparams in the synthesizer folder like so: nsize=600 winrate=48000 i've re-preprepared another dataset and embeds, and put them through about 200 steps of training. when i generate audios with them under the hparams set above, i'm given audio that's played super fast, like someone was pressing 'fast-forward' on a cassette tape player. are the params above correct? or should i have also changed sample rates in other param files as well like the vocoder or encoder? thank you!",question,question
613,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/613,Why do not use LeakReLU instead of ReLU in upsampling side?,"in the unet network, you used leakrelu at encoding to maintain the weight less than zero. i am wondering what is happen if we use leakrelu in the up-sampling side (after concatenate and before convtranspose2d)? do you think we got some improve although training time will be slow? second question i want to ask is that what if i add a vae block in the bottleneck of unet? thanks",question,question
647,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/647,Tutorial: Windows installation,"hello, i have an issues with installing to windows (10). please, could you give a guide?",question,question
318,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/318,Fail to use pre-trained model (CycleGAN),"seems the pre-trained model loading failed. but trainning with a k80 machine works fine >> bash pretrainedcycleganmodel.py"", line 43, in setup file ""/media/liao/user/ex/pytorch-cyclegan-and-pix2pix/models/basenetworks file ""/home/liao/anaconda3/lib/python3.6/site-packages/torch-0.4.0-py3.6-linux-x86statedict for resnetgenerator: unexpected key(s) in statebatchesbatchesbatchesblock.2.numtracked"", ""model.10.convbatchesblock.2.numtracked"", ""model.11.convbatchesblock.2.numtracked"", ""model.12.convbatchesblock.2.numtracked"", ""model.13.convbatchesblock.2.numtracked"", ""model.14.convbatchesblock.2.numtracked"", ""model.15.convbatchesblock.2.numtracked"", ""model.16.convbatchesblock.2.numtracked"", ""model.17.convbatchesblock.2.numtracked"", ""model.18.convbatchesbatchesbatches_tracked"".",Error,question
448,https://github.com/JaidedAI/EasyOCR/issues/448,It Do not detect single digits sometimes,"i was trying to read a marksheet, it sometimes do not read single digits like '6','4',etc",Performance,question
265,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/265,demo_cli.py launch error,"i just installed all the libs, but for some reason running `python3 demo_cli.py` just says `illegal instruction (core dumped)`. if anyone can help at least find out where is the problem i will be much more happier",question,question
536,https://github.com/JaidedAI/EasyOCR/issues/536,Add a new language (Shan),excuse me that i'm very new to python language. but i read the documentation and i prepared the training the repo includes 1000 images with its' labels. could someone help me in training this new language? thanks in advance.,other,other
139,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/139,cli_test.py unable to complete on windows,"windows 10 1903 py 3.7.4 i run `python climem` it runs fine up until where it asks me for a path to sample audio, which i provide and it throws `caught exception: typeerror(""argument of type 'windowspath' is not iterable"")` help please",other,question
719,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/719,Replacing synthesizer from Tacotron to Non Attentive Tacotron,working on it!,other,other
319,https://github.com/JaidedAI/EasyOCR/issues/319,Adding performance results?,"for better using the oss, time/accuracy performance on some testing devices should be taken in to consideration. besides, the appropriate application scenarios should be claimed, because lots of developers asked the quesiton about advantages over tesseract. but actually, i think the most important one is time/resource performance. would you provide some results related to these info？",other,other
567,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/567,Number of layers in NLayerDiscriminator,"hi, i noticed that the three-layer discriminator actually has 6 convolutional layers (""sequence"" has 6 conv2d commands). however, i do not understand how this can result in a receptive field of 70x70, as there seems to be an extra layer compared to what @emilwallner mentioned in thank you!",question,other
1125,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1125,Does the code of pytorch1.4 and pytorch1.1 change a lot for this project,does the code of pytorch1.4 and pytorch1.1 change a lot for this project,question,question
126,https://github.com/JaidedAI/EasyOCR/issues/126,Support for Sinhala language,could you please add support for (native language of sri lanka) language as well?,other,other
5,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/5,Unable to get as good result as in torch,"hi, i ran your code on horse2zebra, with loadsize, finesize and whichnetg changed to match torch version. however, i couldn't get good results. your pytorch model here do have some differences to torch, like different padding, and different training strategy. i'm wondering if you see similar problems.",Performance,question
244,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/244,"train one cycleGAN with multiple parallel datasets (ex trainA1->trainB1, trainA2-trainB2, trainA3-trainB3)","sorry for the noob question, but i was wandering if it could be possible to train the same cycle gan with more than one correlation. example: change all the zebras to horse and all the oranges to apples in the same picture. this could be interesting for more precise results (for example you can correlate tree photos to monet trees, real flowers to monet flowers, real houses to painted ones and so on), but i have absolutely no idea how it could be achieved and if it's actual necessary to have different train ""subfolders"" couples for that, or if you can simply put tree, flower and house photos in traina and the painted ones in trainb. awesome project, anyway!",other,other
958,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/958,I have the problem No Module named torch,please send help,question,other
359,https://github.com/JaidedAI/EasyOCR/issues/359,Poor Detection Accuracy on URLs,"i am currently using easyocr on a project that pulls the url from the url bar on a twitch stream. the box is always in the same spot, so i crop the image, but the results are rather poor. it usually misunderstands the period before com, and struggles to identify the slashes. are there any optimizations i could do to improve this? is it possible currently (or in the near future) to specify a font or a regex pattern to detect based on?",Performance,question
142,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/142,Other Native Accents of English -- Received Pronunciation,"first off, i wanted to congratulate you on the amazing results this app can create. the results are especially amazing if you speak american english. with british english, however, you can easily hear an odd pattern. of course, this varies from accent to accent. i'm speaking about received pronunciation (or bbc english) in this ticket specifically. when using a voice with a british accent, the voice itself sounds british, but pronounces words in an especially american way. so, my question is: thanks in advance.",question,other
880,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/880,Question about sets size ratio,"i'm sorry if this is a trivial quesiton. i want to train a cyclegan using my dataset a (about 5000 images) and a public synthetic dataset b (about 5 milions images). how many images should i choose from dataset b? in other words, must the two sets have approximately the same size or one can be much bigger then the other? does it depends on whether i want to train atob or btoa? edit: another question, does the images from the two sets need to have the same size?",question,question
109,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/109,Generated Images become (1-input) in first several iterations,"hi @junyanz it's a nice code. but i wonder about the initialisation. in my experiments, i notice sometime the network will fall in some strange local minimum. the generated image is just like (1-input). but the recover image still seems good. can i draw the conclusion that the initialisation really matters?",question,question
222,https://github.com/JaidedAI/EasyOCR/issues/222,Running on mobile devices,"first of all, thank you for your great work! it would be great if we can enable running easyocr on the mobile phone. currently we are using ml kit from google to do the ocr on device itself. i would love to help if someone can point me where to start digging in order to enable easyocr to work on mobile.",question,other
414,https://github.com/JaidedAI/EasyOCR/issues/414,Columns of text are being merged in OCR result. How to read text from columns without merging bounding boxes?,"hi, i want to perform ocr on the following image: as can be seen, there are 3 columns of the main text. which should be 3 separate bounding boxes. but the output is merging all 3 and reading them as one, for example the output from ``reader.readtext(img, detail=0, paragraph=true, decoder='beamsearch') ``is: > peter; chris and sarah were london (tfl) to extend the extension is thousands of delighted to join harriet bakerloo line from elephant to people travel from; to and i even tried changing the margin parameters to prevent combining the bounding boxes: and what can i do to prevent this merging and consider these as separate bounding boxes? thank you",question,question
512,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/512,Distributing GAN pool over multiple GPUs,"i'm wondering if any new capabilities have been put in place to distribute the gan pool over multiple gpus. i have been working with an april 2018 off shoot, and i'm about to implement this for myself but if something has already been done, it would be good to know. this is important when dealing with large images. (i'm using the code architecture for an entirely different image transformation task.)",other,other
296,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/296,Error(s) in loading state_dict for ResnetGenerator,"today, i want to test my trained model. there are some errors not occur before. traceback (most recent call last): file ""test.py"", line 19, in file ""/home/t-fayan/vision/pytorch-cyclegan-and-pix2pix/models/basemodel.py"", line 130, in loadstatedict for resnetgenerator: what does it mean? also, if i test pretrained model like horse2zebra, these errors occur too. but i didn't encounter these errors before.",Error,question
380,https://github.com/JaidedAI/EasyOCR/issues/380,Question - Handwriten Support,"hi, do you have an estimate of when the handwritten support will be released?",question,other
174,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/174,Your PyTorch installation is not configured to use CUDA.,"hi, i am trying to build this on a win10 machine with nvidia gpu - nvs 310 with 1 gb ram and driver version 392.56 while running `python demomem`, i get the error > your pytorch installation is not configured to use cuda. if you have a gpu ready for deep learning, ensure that the drivers are properly installed, and that your cuda version matches your pytorch installation. cpu-only inference is currently not supported. ** python 3.7.4 cuda 10.0 cudnn v7.6.4 torch 1.2.0 torchfile 0.1.0 torchvision 0.4.0 would appreciate if you can help find and fix the break. thanks, ajay",question,question
597,https://github.com/JaidedAI/EasyOCR/issues/597,Pytorch Tensors,"hi, thanks for the wonderful project. its by far the best ocr that i have encountered. i was wondering if the ** function accepts pytorch tensors as input. i am asking this only because of performance optimization. any possible way that this could be a near-term update?",question,other
291,https://github.com/JaidedAI/EasyOCR/issues/291,Important Error,"hi colleagues: when i use the easyocr system (and i am using the gpu) appears the error below: is very important for me because i need use this system with videos. this problem only occurs with gpu is enable, but it works fine when i use only cpu. thank you in advance. miguel ángel and @mariamartinmarin",question,question
775,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/775,Problem converting to ONNX,"hey, i've been trying to convert the winter2summer checkpoint to an onnx model, so i've tried running test.py with the following additions: `dummyinput, ""./cyclegan.onnx"")` i got the following error: thanks!",question,question
489,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/489,Cannot assign 'torch.cuda.LongTensor' as parameter 'step' (torch.nn.Parameter or None expected),"hi, i am trying to re-train the synthesizer model as discussed in , but i get this error below:",question,question
398,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/398,"For anyone still having trouble, you can check out my fork and run this project using SageMaker  ","shameless plug: i'm told most of the installation pain has been resolved now that there's cpu support. still, you might find this useful in case you want to generate a large number of voice files.",other,other
467,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/467,Could not find Source.Txt file in dataset,"hi in encoder, speaker.py reads source.txt from data set which is not found. when i run the train loop , it is showing me error: filenotfounderror: caught filenotfounderror in dataloader worker process 0. original traceback (most recent call last): file ""/home/shivani/anaconda3/lib/python3.7/site-packages/torch/utils/data/workerutils/fetch.py"", line 47, in fetch file ""/home/shivani/projects/real-time-voice-cloning/encoder/dataverificationobjects/speakerobjects/speakerobjects/speaker.py"", line 34, in randomobjects/speaker.py"", line 14, in utterances file ""/home/shivani/anaconda3/lib/python3.7/pathlib.py"", line 1203, in open file ""/home/shivani/anaconda3/lib/python3.7/pathlib.py"", line 1058, in sources.txt'",question,question
691,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/691,Error running synthesizer_preprocess_audio.py: ValueError: max() arg is an empty sequence,i want to go through the steps of preprocessing and training a synthesizer using the librispeech train-clean-100 dataset i have the latest version of this repo and i'm able to run demopreprocessname librispeech --subfolders train-clean-100 --nopreprocess_audio.py: i tried and it didn't help. i use windows os and i only have 450gb of free space left on my ssd and ram is 8 gb. can't i do something about it?,question,question
572,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/572,Information for working with images of different size using Spatial Pyramid Pooling layer,is there a way to add spatial pyramid pooling layer(or something else) to work with images of different size and so as to not resize them ? thanks gaurav,question,question
752,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/752,stuck at encoder,"when i try to record my voice every thing goes well untill it says ""loading the encoder encoder\savedvoicemodels, how can this be fixed? (says sv2tts is not responding)",question,question
148,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/148,Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:,"trying to setup your voice-cloning tool, i get this error: this raises, when i'm trying to execute the script demomem argument, as well as without. do you know why this happens, or what i can do to avoid this error? it works till loading the checkpoints, than this message raises. all libraries and dependencies seem to be installed, cause pytorch/torch/torchvision are working, as well as the cuda test scripts and the nvidia-cuda drivers. does the non-supported nvidia gtx 760 causes my problems? if it is, it's sad, cause it's the only nvidia gpu available. hopefully, anyone has an answer to me. best regards,",question,question
270,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/270,"My computer halted everytime I input""python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout""","my computer halted everytime i input""python train.py --dataroot ./datasets/maps --name mapsgan --nocyclesgan/web..."" i, a new learner beg for help....",question,question
773,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/773,How to enable dataset in toolbox?,i downloaded the dataset recommended in the readme to test the system and i started with this command: `python demo_toolbox.py -d c:\users\vito\desktop\librispeech\train-clean-100` but i have this result (without blanks in the path):,question,question
978,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/978,AttributeError: 'Namespace' object has no attribute 'resize_and_crop',"file ""d:\pytorch-cyclegan-and-pix2pix-master\data\baseparams attributeerror: 'namespace' object has no attribute 'resizecrop'",other,other
55,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/55,Images of current results are rotated,"when i view the current results, i find that the images are rotated. is it okay for the software?",question,question
970,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/970,Toolbox Won't Run Due to Qt Dependency Issue,"on this platform: `hostnamectl static hostname: debian transient hostname: freeman-uncluttered operating system: debian gnu/linux 10 (buster) the toolbox gives the following error message when run: `seancarter@freeman-uncluttered:~/real-time-voice-cloning$ python demodandandandandandandandandandandandandandandandan_carter/miniconda3/lib/python3.9/site-packages/pyqt5/qt/plugins/platforms/libqxcb.so"" qt.qpa.xcb: could not connect to display qt.qpa.plugin: could not load the qt platform plugin ""xcb"" in """" even though it was found. this application failed to start because no qt platform plugin could be initialized. reinstalling the application may fix this problem. available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb. aborted` what's the problem, and how can i fix it?",question,question
1002,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1002,Chinese can't download Pretrained-models,"in this link, the download source is google, but china has already implemented access to google.",other,other
565,https://github.com/JaidedAI/EasyOCR/issues/565,Arabic text generation is producing garbage images,here's an example by running: actual: ﺍﺩﺭﺍﻙ ﻋﻠﻴﻞ ﻧﺴﺎﻧﺲ ﺩﻧﺎﻭﺓ ﺍﻧﺰﺍﺡ3 image: actual: ﺍﻟﺒﺎﻝ ﺍﺳﺘﻄﻴﺮ ﺍﻋﺪ ﻟﻴﻜﺘﺐ ﺳﻌﺪﺍﺀ_1 image: and so on ... you get the idea,other,other
785,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/785,Saving the trained model,after training on few audios input i want to save the trained model so that i can use it again when i need it whithout having to redo all the application again. how can i save these models ? thank you,question,question
565,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/565,Tiff end-to-end using CycleGan,"hi, first of all congratulations for your paper and for the code. it was a pleasure reading what you did and how you did it. i am running cyclegan with different types of tiffs in traina and trainb. the tiffs are 256x256 pixels in size and have 1 channel per pixel. i am using tiffs to have a wide range of values. i changed the code as you suggest ( and similar), but what i got out during the training in `./checkpoints` are three-channels pngs. do you think it would be possible to change the code so that it goes from 1 channel tiff to 1 channel tiff with no information loss? as far as i understand, at present you are converting the imported files to pngs along the way. in other words: i would like my tensors to be `[256int_range,1]`. thanks for the help!",question,question
630,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/630,Can't load dataset,warning: you did not pass a root directory for datasets as argument. the recognized datasets are: librispeech/dev-clean librispeech/dev-other librispeech/test-clean librispeech/test-other librispeech/train-clean-100 librispeech/train-clean-360 librispeech/train-other-500 libritts/dev-clean libritts/dev-other libritts/test-clean libritts/test-other libritts/train-clean-100 libritts/train-clean-360 libritts/train-other-500 ljspeech-1.1 voxceleb1/wav voxceleb1/test_wav voxceleb2/dev/aac voxceleb2/test/aac vctk-corpus/wav48,question,other
613,https://github.com/JaidedAI/EasyOCR/issues/613,Error trying to import easyocr.,"since yesterday i received the following error, 2 months ago it was working just fine. does anybody know the reason or recognises this problem? i am working on a macbook m1. import easyocr traceback (most recent call last): file """", line 1, in file ""/opt/homebrew/caskroom/miniforge/base/envs/afas19/lib/python3.8/site-packages/easyocr/_imaging.cpython-38-darwin.so, 0x0002): library not loaded: @rpath/libjpeg.9.dylib referenced from: /opt/homebrew/caskroom/miniforge/base/envs/afas19/lib/python3.8/site-packages/pil/_imaging.cpython-38-darwin.so reason: tried: '/opt/homebrew/caskroom/miniforge/base/envs/afas19/lib/libjpeg.9.dylib' (no such file), '/opt/homebrew/caskroom/miniforge/base/envs/afas19/bin/../lib/libjpeg.9.dylib' (no such file), '/opt/homebrew/caskroom/miniforge/base/envs/afas19/lib/libjpeg.9.dylib' (no such file), '/opt/homebrew/caskroom/miniforge/base/envs/afas19/bin/../lib/libjpeg.9.dylib' (no such file), '/usr/local/lib/libjpeg.9.dylib' (no such file), '/usr/lib/libjpeg.9.dylib' (no such file)",question,question
121,https://github.com/JaidedAI/EasyOCR/issues/121,Sort textboxes into lines or paragraphs,"it would be nice if the detected text was ordered in the same way someone would read it. sometimes sentences are scrambled arround and sentences like this ""reduce your risk of coronavirus infection"" becomes something like ""of coronavirus infection reduce your risk"".",other,other
455,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/455,Error - AttributeError: 'Colorbar' object has no attribute 'set_clim',"hello everyone! every time i run i got this following error: i can comment the line and working fine, but i'd be glad to fix this. the specific line is this last one: could you help me? thanks a lot!",question,question
825,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/825,Training works only with gray scale images,"hi, i have used pix2pix model to train my model using custom dataset with pairs. a is the real image while b is the black & white version of a. when i trained the model with 3 channel ie rgb image(--nchannels 1), it learns perfectly. btw, i use resnet9 for g and basic for d. can you please explain the behaviour?",question,question
279,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/279,Torch not compiled with CUDA enabled,hello guys i already install cloning toolbox and everything is okay but when i try to insert any voice to the toolbox it say torch not compiled with cuda enabled and i don't have cuda drive i only use cpu and i instaled pytorch for only pc.. second it say datasets_root: none and i can't use anything from my computer to use it on toolbox thanks.,question,question
4,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/4,can't install requirements,"hi guys, i tried to install this tool, following next steps: 1) install python 3.73 2) install pip 3) run from command line pip install -r requirements.txt but i've got the following error: (see below). could you help me with some suggestion. i don't know python well, but i will very gratefull if your suggestion help me. thank you in advance. c:\users\admin\desktop\real-time-voice-cloning-master\real-time-voice-cloning-ma ster>pip install -r requirements.txt requirement already satisfied: tensorflow-gpu=1.10.0 in c:\users\admin \appdata\local\programs\python\python37\lib\site-packages (from -r requirements. txt (line 1)) (1.13.1) requirement already satisfied: umap-learn in c:\users\admin\appdata\local\progra ms\python\python37\lib\site-packages (from -r requirements.txt (line 2)) (0.3.9) requirement already satisfied: visdom in c:\users\admin\appdata\local\programs\p ython\python37\lib\site-packages (from -r requirements.txt (line 3)) (0.1.8.8) collecting webrtcvad (from -r requirements.txt (line 4)) using cached a56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz collecting librosa>=0.5.1 (from -r requirements.txt (line 5)) collecting matplotlib>=2.0.2 (from -r requirements.txt (line 6)) using cached 027fe1945c3c78bdb465b4736903d0904b7f595ad/matplotlib-3.1.0-cp37-cp37m-winamd64.w hl requirement already satisfied: protobuf>=3.6.1 in c:\users\admin\appdata\local\p rograms\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0- >-r requirements.txt (line 1)) (3.8.0) requirement already satisfied: wheel>=0.26 in c:\users\admin\appdata\local\progr ams\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (0.33.4) requirement already satisfied: tensorboard=1.13.0 in c:\users\admin\app data\local\programs\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (1.13.1) requirement already satisfied: six>=1.10.0 in c:\users\admin\appdata\local\progr ams\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (1.12.0) requirement already satisfied: astor>=0.6.0 in c:\users\admin\appdata\local\prog rams\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (0.8.0) requirement already satisfied: termcolor>=1.1.0 in c:\users\admin\appdata\local\ programs\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0 ->-r requirements.txt (line 1)) (1.1.0) requirement already satisfied: keras-preprocessing>=1.0.5 in c:\users\admin\appd ata\local\programs\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (1.1.0) requirement already satisfied: grpcio>=1.8.6 in c:\users\admin\appdata\local\pro grams\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->- r requirements.txt (line 1)) (1.21.1) requirement already satisfied: keras-applications>=1.0.6 in c:\users\admin\appda ta\local\programs\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (1.0.8) requirement already satisfied: tensorflow-estimator=1.13.0 in c:\use rs\admin\appdata\local\programs\python\python37\lib\site-packages (from tensorfl ow-gpu=1.10.0->-r requirements.txt (line 1)) (1.13.0) requirement already satisfied: absl-py>=0.1.6 in c:\users\admin\appdata\local\pr ograms\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0-> -r requirements.txt (line 1)) (0.7.1) requirement already satisfied: gast>=0.2.0 in c:\users\admin\appdata\local\progr ams\python\python37\lib\site-packages (from tensorflow-gpu=1.10.0->-r requirements.txt (line 1)) (0.2.2) requirement already satisfied: scikit-learn>=0.16 in c:\users\admin\appdata\loca l\programs\python\python37\lib\site-packages (from umap-learn->-r requirements.t xt (line 2)) (0.21.2) requirement already satisfied: numba>=0.37 in c:\users\admin\appdata\local\progr ams\python\python37\lib\site-packages (from umap-learn->-r requirements.txt (lin e 2)) (0.44.0) requirement already satisfied: tornado in c:\users\admin\appdata\local\programs\ python\python37\lib\site-packages (from visdom->-r requirements.txt (line 3)) (6 .0.2) requirement already satisfied: requests in c:\users\admin\appdata\local\programs \python\python37\lib\site-packages (from visdom->-r requirements.txt (line 3)) ( 2.22.0) requirement already satisfied: websocket-client in c:\users\admin\appdata\local\ programs\python\python37\lib\site-packages (from visdom->-r requirements.txt (li ne 3)) (0.56.0) requirement already satisfied: torchfile in c:\users\admin\appdata\local\program s\python\python37\lib\site-packages (from visdom->-r requirements.txt (line 3)) (0.1.0) requirement already satisfied: pillow in c:\users\admin\appdata\local\programs\p ython\python37\lib\site-packages (from visdom->-r requirements.txt (line 3)) (6. 0.0) requirement already satisfied: pyzmq in c:\users\admin\appdata\local\programs\py thon\python37\lib\site-packages (from visdom->-r requirements.txt (line 3)) (18. 0.1) collecting decorator>=3.0.0 (from librosa>=0.5.1->-r requirements.txt (line 5)) using cached dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl collecting resampy>=0.2.0 (from librosa>=0.5.1->-r requirements.txt (line 5)) requirement already satisfied: joblib>=0.12 in c:\users\admin\appdata\local\prog rams\python\python37\lib\site-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (0.13.2) collecting audioread>=2.0.0 (from librosa>=0.5.1->-r requirements.txt (line 5)) collecting python-dateutil>=2.1 (from matplotlib>=2.0.2->-r requirements.txt (li ne 6)) using cached f3844689e3a78bae1f403648a6afb1d0866d87fbb/pythonamd64.w hl collecting cffi>=1.0 (from sounddevice->-r requirements.txt (line 10)) using cached e57b47f41d1049b5fb0ab79caf0ab11407945c1a7/cffi-1.12.3-cp37-cp37m-winwheel -d 'c:\users\admin\appdata\local\temp\pip-wheel-7ewjy9xx' --python -tag cp37: error: running bdistpy creating build creating build\lib.win-amd64-3.7 copying webrtcvad.py -> build\lib.win-amd64-3.7 running buildwebrtcvad' extension error: microsoft visual c++ 14.0 is required. get it with ""microsoft visual c++ build tools"": ---------------------------------------- error: failed building wheel for webrtcvad running setup.py clean for webrtcvad failed to build webrtcvad installing collected packages: webrtcvad, decorator, resampy, audioread, librosa , python-dateutil, cycler, pyparsing, kiwisolver, matplotlib, tqdm, pycparser, c ffi, sounddevice running setup.py install for webrtcvad ... error ython\python37\python.exe' -u -c 'import setuptools, tokenize;__, '""'""'exec'""'""'))' install --record 'c: \users\admin\appdata\local\temp\pip-record-t598pe1f\install-record.txt' --single -version-externally-managed --compile"" failed with error code 1 in c:\users\admi n\appdata\local\temp\pip-install-iqkryxja\webrtcvad\",deployment,question
86,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/86,getting massive error when getting webrtcvad,"error: command errored out with exit status 1: error: command errored out with exit status 1: 'c:\users\name\appdata\local\programs\python\python37\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\name\\appdata\\local\\temp\\pip-install-sd3z4nr3\\webrtcvad\\setup.py'""'""'; __, '""'""'exec'""'""'))' install --record 'c:\users\name\appdata\local\temp\pip-record-405orggd\install-record.txt' --single-version-externally-managed --compile check the logs for full command output.",other,question
595,https://github.com/JaidedAI/EasyOCR/issues/595,combine Arabic with English in same image,@rkcosmos i face some problems with i combine arabic with english like this image the result is : ['؟٦ ٨٧٨٥٥٣ ٥٣٨٥٣ ٧٥٧٣'] when i remove ar from reader function the output is right i need tar and english because some lines have two languages. do u have any solution for that thanks,question,question
854,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/854,make the model work in french,"hello everyone, i have the toolbox, i don't know how to make it work so that it clones voices in french, would someone have the solution please, i'm not a computer expert , thank you very much",question,question
36,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/36,multiple forward passes but one backward call for updating G?,"i saw the following code in the cyclemodel `backwarda and gg` would be wrong and one should instead do backward pass thrice, each immediately following their involving forward pass. i assume this is somehow taken care of in pytorch. but how does the model know w.r.t which input data it should compute the gradients?",question,question
754,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/754,Can the AMD graphics card work?,"excuse me, can amd 6000 series graphics cards run this project?",question,other
355,https://github.com/JaidedAI/EasyOCR/issues/355,Wrong detection.,"i am using ""en"" language, when ever it detecting 1(one) it always gives i else . original image = 21071056635 result = [['20705663']] import matplotlib.pyplot as plt import cv2 from pylab import rcparams from ipython.display import image from google.colab import files import matplotlib.pyplot as plt import easyocr rcparams['figure.figsize'] = 8, 16 l = ['en'] result = [] for i in l: reader = easyocr.reader([i], gpu = false) result.append(reader.readtext('image.jpg', detail = 0)) print(result) [['20705663']]",other,question
87,https://github.com/JaidedAI/EasyOCR/issues/87,size mismatch for Prediction.bias,"hi, trying to run the 3 lines demo, using the git version, and it dies with this error: using python 2.7.12 (default, apr 15 2020, 17:07:12), on a container running ubuntu 16.04.6 lts and host running rhel 7.7 with kernel 3.10.0-1062.18.1.el7.x86_64.",question,Error
1042,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1042,Pix2pix single data test mode error,"here is my train code !python3 train.py --dataroot data1/ --model pix2pix --name realm3 --direction btoa --inputnc 1 and here is my test code !python3 test.py --dataroot test/ts1 --name realm3 --model pix2pix --netg unetmode single --norm batch --inputnc 1 but it occured error traceback (most recent call last): file ""test.py"", line 62, in file ""/home/young0719/jm/p2p/pytorch-cyclegan-and-pix2pix-master/pytorch-cyclegan-and-pix2pix/models/pix2pixinput keyerror: 'b' my files already in that directory,, thank you..",question,question
878,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/878,Can a self supervised technique be used for generating paired image?,"hi, i have used your pix2pix gan model with my custom dataset. but the process of generating image pairs takes a lot of time. i have very less data around 100 image pairs. even when i tried cycle gan the results were not great. i came across the term self-supervised learning. my question is whether can i use self-supervision to image translation tasks since i have very little data.",question,question
730,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/730,Looking for performance metric for cyclegan,"hi, we often apply cyclegan for unpaired data. so, some of the performance metric will be not applied - ssim - psnr for my dataset, i would like to use cyclegan to mapping an image from winter session to spring session and they have no pair data for each image. could you tell me how can i evaluate the cyclegan performance (i.e how to know the output is close to a realistic image...)",question,question
115,https://github.com/JaidedAI/EasyOCR/issues/115,Question,"if multiple languages are passed fin for detection ['a', 'b', 'c'] is it possible to get the detected language type returned in the the readtext response?",question,question
613,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/613,"is there a way to configure it so that, rather than differentiate voices by speakers, it creates a composite voice trained on a few different speakers?",ive spent the last week or so having a ton of fun with this project and ive noticed that if you continuously train the encoder on one speaker it will tend to make any future attempts at voice cloning sound closer to the original voice it was trained on. im a brainless monkey though and have no idea what im doing so im wondering if anyone else here has attempted this.,question,question
702,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/702,Query Regarding Pre-trained Weights,"hello, the pretrained weights of the model that you provided are trained on which datasets?",question,question
1154,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1154,Question about using other optimizers,"if i want to try out other optimizers like sgd with momentum would i just need to change line 68 & 69 in pix2pixg = torch.optim.adam(self.netg.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))` ` self.optimizerg = torch.optim.sgd(self.netg.parameters(), lr=opt.lr, momentum=0.9)` ` self.optimizer_d = torch.optim.sgd(self.netd.parameters(), lr=opt.lr, momentum=0.9)` thank you!",question,question
917,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/917,Clonador de voz ,python,other,other
645,https://github.com/JaidedAI/EasyOCR/issues/645,Increase easyOCR model performance on cpu,hi how i can increase number of workers of cpu when i use aesyocr model ??? i look-up to increase the speed of processes without using gpu thanks,question,question
357,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/357,class ZoneoutLSTMCell(tf.nn.rnn_cell.RNNCell):  AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',class zoneoutlstmcell(tf.nn.rnncell',deployment,other
880,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/880,Two errors Importerror and ignoring 2 things,"hi, there i'm having issues regarding installing and running the real-time voice cloning master tool from github. everytime i try to install and run i get this error importerror: dll load failed: the specified module could not found and i also have an error to do with ""ignoring numpy/webrtcvad:- don't match your environment more details below",question,question
282,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/282,"Some problems when I try to ""bash"" the datasets and pretrained models:","after i type the command""bash ./datasets/downloaddataset.sh facades"" the terminal warns me: warning: timetamping does nothing in combination with -0. see the manual for details. and connection timed out how to fix this problem?",question,question
189,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/189,Unable to start the toolbox or the demo,"good day. i am trying to launch the toolbox, but i ran into an issue that i don't really know what to do about. i installed all the files as instructed and installed all the requirements. but when i do the ""python demotoolbox.py"": again, it doesn't do anything after that. i left the windows for hours, they didn't change. nothing launches. i don't know what to do about it. i have to add that i never worked with python before, this is my first time doing this, so i am learning as i go. i am probably doing something completely wrong. but i just don't know what.",other,question
88,https://github.com/JaidedAI/EasyOCR/issues/88,Process Time,"great work, just a simple question for a normal image, it takes around 6-7 seconds of time to give output with gpu, any method to make it faster.",Performance,other
609,https://github.com/JaidedAI/EasyOCR/issues/609,Citation of EasyOCR,"dear authors, in case of we want to cite your work in our paper: - how can we cite? - what is bibliography we have to use? hope to see your respond soon. regards,",other,question
503,https://github.com/JaidedAI/EasyOCR/issues/503,How to recognize numbers only?,"my text contains just numbers.in order to improve accuracy of recognition,how to recognize numbers only?",question,question
465,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/465,The box with clusters isn't working.,"hi, when i load different or same pretrained datasets the box where points appear and form clusters are not working. just nothing happens there. how can i solve this? also the audio often seems ""doubled"" or with a lot of noise. how can i improve that? thank you",question,question
650,https://github.com/JaidedAI/EasyOCR/issues/650,Current PyTorch install does not support RTX 3070.,"hi there, when i updated my gfx card i started getting this error when running `easyocr.reader()`. is this a problem with how i'm installing my requirements maybe, or is this something that would need to be updated in the repo?",deployment,question
134,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/134,dependencies pack for non-programmers,"hello, for non-programmers, it would be useful to create some installer pack, with all dependencies required by project like this (or are there any available somewhere?), so that the project is easy to use and to migrate between computers. for example, right now - since yeasterday i'm searchoing for answers and installing things one after another, and i still don't know how this will end, and whether it will work. so from sqa perspective, this is a major weak spot. next step would be to create exact (repeatable) step-by-step framework, how to expand such project for other languages (i'm interested in pl). also, i would suggest some compiled version of the project, with all dependencies needed (.exe for windows users, like me).",other,other
397,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/397,GPU / CPU Error,"hello everyone, i have this error. it works fine when the model is over cpu but it doesn't when put on gpu. do you have any ideas ? file ""/models/cyclemodel.py"", line 168, in optimizegantrim/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in _trim/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in _trim/lib/python3.5/site-packages/torch/nn/modules/container.py"", line 91, in forward file ""/home/{}/.pyenv/versions/3.5.0/envs/nettrim/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 308, in forward runtimeerror: input type (torch.cuda.floattensor) and weight type (torch.floattensor) should be the same",question,question
759,https://github.com/JaidedAI/EasyOCR/issues/759,TypeError: 'NoneType' object is not iterable in recognize method,"i am using the recognize method provided with my own preset horizontalagg list. the documentation says that free_list by default is none and that i only need either one of horizontal or free list, yet i get this error as seen below:",other,Error
500,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/500,Could not load the Qt platform plugin,"in case you got this error during training the encoder, run these commands could resolve the problem. sudo apt-get install libxkbcommon-x11-0 sudo apt-get install libxcb-xinerama0",other,other
51,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/51,about “the three networks training order”,"the encoder and synthesiser network training has a sequential relationship, and the vocoder network is not dependent on these two networks, can be trained at any time?",question,question
715,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/715,No Visualizations appear at visdom while training,i started the server then i started training in another process but no visualizations appear while training as in the training instructions,other,question
253,https://github.com/JaidedAI/EasyOCR/issues/253,OCR of Seven Segment Display,"the ocr results on the ""7 segment display"" images are average. could you please update this feature in future models?",other,question
346,https://github.com/JaidedAI/EasyOCR/issues/346,rotation_info --> ZeroDivision Error,"for example, adding rotation_info=[180] to my readtext method gives me ""zerodivisionerror: integer division or modulo by zero""",Error,other
800,https://github.com/JaidedAI/EasyOCR/issues/800,when load arabic model as a custom model to compare that the character_list is correct in order,"i tried to load the arabic.pth model yours model but i got error mismatch when loading statemodel.py doesn't match with the arabic.pth model and this is the error runtimeerror: error(s) in loading statedict: ""featureextraction.convnet.0.weight"", ""featureextraction.convnet.0.bias"", ""featureextraction.convnet.3.weight"", ""featureextraction.convnet.3.bias"", ""featureextraction.convnet.6.weight"", ""featureextraction.convnet.6.bias"", ""featureextraction.convnet.8.weight"", ""featureextraction.convnet.8.bias"", ""featureextraction.convnet.11.weight"", ""featureextraction.convnet.12.weight"", ""featureextraction.convnet.12.bias"", ""featureextraction.convnet.12.runningvar"", ""featureextraction.convnet.14.weight"", ""featureextraction.convnet.15.weight"", ""featureextraction.convnet.15.bias"", ""featureextraction.convnet.15.runningvar"", ""featureextraction.convnet.18.weight"", ""featureextraction.convnet.18.bias"". unexpected key(s) in state1.weight"", ""featureextraction.convnet.bn01.bias"", ""featureextraction.convnet.bn0mean"", ""featureextraction.convnet.bn0var"", ""featureextraction.convnet.bn0batches2.weight"", ""featureextraction.convnet.bn02.bias"", ""featureextraction.convnet.bn0mean"", ""featureextraction.convnet.bn0var"", ""featureextraction.convnet.bn0batchesmean"", ""featureextraction.convnet.layer1.0.bn1.runningbatchesmean"", ""featureextraction.convnet.layer1.0.bn2.runningbatchesmean"", ""featureextraction.convnet.layer1.0.dow...",question,Error
384,https://github.com/JaidedAI/EasyOCR/issues/384,Special Characters,"hi, is there a way to also recognize special characters like @ or #?",question,question
1159,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1159,identity loss for cyclegan,"hi, i purposely increased the identity loss for my own project and the results got worse after the change. i took a look at the code carefully and then realized the identity loss may be not correct. minimizing gb(a) - a will just make the two generators have no effect to the input as much as possible. it's supposed to be `identity loss (optional): lambdaa(a) - b lambda_a) ` instead if i understand properly. please let me know if it is an issue. thank you.",question,question
389,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/389,Training with CPU: AttributeError: module 'torch' has no attribute 'cpu',"the torch.cpu is giving me ""attributeerror: module 'torch' has no attribute 'cpu'"" _originally posted by @jokeryan in",deployment,question
65,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/65,Continuing training always start from epoch 1 again,"hi, when i halt my training at some point (i.e. epoch 40) and want to continue it later, i give `--continueepoch 40` arguments to the train.py but when it starts training, it begins from epoch 1 again. because of that, output html files are being overwritten. you can check the attached `loss_log.txt` file.",Error,question
759,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/759,ModuleNotFoundError: No module named 'matplotlib',"i've installed all the required files and programs to execute it, but i can't seem to make it run here is my error: `c:\users\owner>python` documents\real-time-voice-cloning-master\demotoolbox.py"", line 2, in file ""c:\users\owner\documents\real-time-voice-cloning-master\toolbox\__.py"", line 1, in file ""c:\users\owner\documents\real-time-voice-cloning-master\toolbox\ui.py"", line 1, in modulenotfounderror: no module named 'matplotlib'` and since it's been a source of my issues for a while, here is my path directory: the version of pytorch i installed is: pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio===0.8.1 -f",other,question
571,https://github.com/JaidedAI/EasyOCR/issues/571,can't recognize this  very simple and clear picture with a number,python3.8 with easyocr==1.4.1 model `english_g2` this stdout all the other numbers can be correctly identified,question,Error
1200,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1200,Bw2Color with Pix2Pix not working,"hi i'm implementing the pix2pix architecture with keras following this on tensorflow site for the colorization task. i created a dataset from imagenet by randomly choosing 25 classes(mainly animals) and for each class i downloaded 1000 images so to have a dataset with 25k images. i trained the model for 150 epochs with 256x256 images, learning rate of 0.00002 , batch size of 4 and then i tried to colorize some images but the results weren't so good. for example 1. the dataset is too small ? 2. should i train the model more ? 3. or change the hyper parameters(learning rate, batch size, loss function. ecc) ? thanks",question,question
164,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/164,"out of memory, gtx1080, 8gb, batch_size=32","its' perform right, when i use batch_size=16, but!, it's down very slowly, have anybody know that any h param sets, to reslovtion is problem?",question,question
779,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/779,error in loading state_dict,"when doing a fine tuning for spanish, when executing the toolbox, there is a mismatch with the weights. what parameters must be changed to solve this error. try to look for information for a few days but i can't solve it.",question,question
113,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/113,Where is the edge post-process code？ ,"i want to create my own edge image, so where is the edge post-process code?",question,question
419,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/419,Getting an exception when browsing for files,"for some reason, importing mp3 files is not working. anyone got an idea on why this might be the case.?",question,question
377,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/377, global name 'ConnectionError' is not defined in display_current_results,"i recently updated/rebuild my pytorch-cyclegan-and-pix2pix docker image but now i have an error i did not get before: end of epoch 5 / 200 time taken: 619 sec learning rate = 0.0002000 traceback (most recent call last): file ""train.py"", line 36, in **",Error,Error
124,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/124,PyTorch installation is not configured to use CUDA,"upon cloning of the repo and installation of all dependencies, i am running `python demowrapper.py:119] from /users/yev/sites/voice/synthesizer/models/modules.py:91: the name tf.nn.rnncell.rnncell instead. > arguments: > > encfpath: encoder/savedmodelmodels/logs-pretrained > vocfpath: vocoder/savedmem: false > no_sound: false > > running a test of your configuration... > > your pytorch installation is not configured to use cuda. if you have a gpu ready for deep learning, ensure that the drivers are properly installed, and that your cuda version matches your pytorch installation. cpu-only inference is currently not supported. i believe the main takeaway is `your pytorch installation is not configured to use cuda` i have both pytorch and cuda installed but not sure how to configure pytorch to use cuda. i am trying to run this repo on a macbook pro running macos mojave 10.14.5 this machine does not comes with an nvidia graphics card. the graphics card installed is the following: radeon pro 560 4gb i dont think you can run cuda without an nvidia graphics card. is it only possible to run this application on a machine with an nvidia graphics card?",question,question
585,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/585,Cutting off the ending of the sentence ,"i've noticed that ending of the last word in the text is often cut off. for example, in the output for ""this work reflects a quest for lost identity, a recuperation of an unknown past."" past is pronounced as pa or pas. i thought that this could be happening because ""st"" and ""t"" sounds at the end are registered as noise and trimmed in the post processing step. upon listening to the outputs without applying the postprocessing operation, i realized that my guess wasn't correct because the ""st"" and ""t"" sounds were still missing. i'm using the 002 vocoder from the of available pretrained models for this project. i've used the original synthesizer, synthesizer 004 and synthesizer 006 from the aforementioned list and noticed the same thing happening with all three. does anyone has any idea why this could be happening or has any advice on how i can further investigate this issue? thanks!",question,question
997,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/997,I get that error while trying to install. What to do?,"c:\users\pc\desktop\real-time-voice-cloning-master>pip install -r requirements.txt collecting inflect==5.3.0 using cached inflect-5.3.0-py3-none-any.whl (32 kb) collecting librosa==0.8.1 using cached librosa-0.8.1-py3-none-any.whl (203 kb) collecting matplotlib==3.5.1 using cached matplotlib-3.5.1-cp37-cp37m-winamd64.whl (6.7 mb) requirement already satisfied: scikit-learn==1.0.2 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from -r requirements.txt (line 7)) (1.0.2) requirement already satisfied: scipy==1.7.3 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from -r requirements.txt (line 8)) (1.7.3) collecting sounddevice==0.4.3 using cached sounddevice-0.4.3-py3-none-winlearn-0.5.2-py3-none-any.whl collecting unidecode==1.3.2 using cached unidecode-1.3.2-py3-none-any.whl (235 kb) requirement already satisfied: urllib3==1.26.7 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from -r requirements.txt (line 14)) (1.26.7) collecting visdom==0.1.8.9 using cached visdom-0.1.8.9-py3-none-any.whl collecting webrtcvad==2.0.10 using cached webrtcvad-2.0.10.tar.gz (66 kb) preparing metadata (setup.py) ... done requirement already satisfied: packaging>=20.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (21.3) requirement already satisfied: joblib>=0.14 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (1.1.0) requirement already satisfied: pooch>=1.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (1.6.0) requirement already satisfied: numba>=0.43.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (0.55.1) requirement already satisfied: audioread>=2.0.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (2.1.9) requirement already satisfied: resampy>=0.2.2 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (0.2.2) requirement already satisfied: decorator>=3.0.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from librosa==0.8.1->-r requirements.txt (line 2)) (5.1.1) requirement already satisfied: python-dateutil>=2.7 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 3)) (2.8.2) requirement already satisfied: kiwisolver>=1.0.1 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 3)) (1.3.2) requirement already satisfied: cycler>=0.10 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 3)) (0.11.0) requirement already satisfied: fonttools>=4.22.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 3)) (4.29.0) requirement already satisfied: pyparsing>=2.2.1 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 3)) (3.0.7) requirement already satisfied: pyqt5-sip=12.8 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from pyqt5==5.15.6->-r requirements.txt (line 6)) (12.9.0) requirement already satisfied: pyqt5-qt5>=5.15.2 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from pyqt5==5.15.6->-r requirements.txt (line 6)) (5.15.2) requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 7)) (3.0.0) requirement already satisfied: cffi>=1.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from sounddevice==0.4.3->-r requirements.txt (line 9)) (1.15.0) requirement already satisfied: colorama in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from tqdm==4.62.3->-r requirements.txt (line 11)) (0.4.4) requirement already satisfied: pynndescent>=0.5 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from umap-learn==0.5.2->-r requirements.txt (line 12)) (0.5.6) requirement already satisfied: six in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (1.16.0) requirement already satisfied: tornado in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (6.1) requirement already satisfied: pyzmq in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (22.3.0) requirement already satisfied: torchfile in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (0.1.0) requirement already satisfied: jsonpatch in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (1.32) requirement already satisfied: websocket-client in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (1.2.3) requirement already satisfied: requests in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from visdom==0.1.8.9->-r requirements.txt (line 15)) (2.27.1) requirement already satisfied: pycparser in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from cffi>=1.0->sounddevice==0.4.3->-r requirements.txt (line 9)) (2.21) requirement already satisfied: llvmlite=0.38.0rc1 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from numba>=0.43.0->librosa==0.8.1->-r requirements.txt (line 2)) (0.38.0) requirement already satisfied: setuptools in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from numba>=0.43.0->librosa==0.8.1->-r requirements.txt (line 2)) (47.1.0) requirement already satisfied: appdirs>=1.3.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from pooch>=1.0->librosa==0.8.1->-r requirements.txt (line 2)) (1.4.4) requirement already satisfied: idna=2.5 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from requests->visdom==0.1.8.9->-r requirements.txt (line 15)) (3.3) requirement already satisfied: charset-normalizer~=2.0.0 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from requests->visdom==0.1.8.9->-r requirements.txt (line 15)) (2.0.10) requirement already satisfied: certifi>=2017.4.17 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from requests->visdom==0.1.8.9->-r requirements.txt (line 15)) (2021.10.8) requirement already satisfied: jsonpointer>=1.9 in c:\users\pc\appdata\local\programs\python\python37\lib\site-packages (from jsonpatch->visdom==0.1.8.9->-r requirements.txt (line 15)) (2.2) building wheels for collected packages: webrtcvad building wheel for webrtcvad (setup.py) ... error error: command errored out with exit status 1: command: 'c:\users\pc\appdata\local\programs\python\python37\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'c:\\users\\pc\\appdata\\local\\temp\\pip-install-i5z7dkyd\\webrtcvadf90e9d7aafb5416faa2eb06f9cfa4074\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(_wheel -d 'c:\users\pc\appdata\local\temp\pip-wheel-dwheel running build running buildext building 'f90e9d7aafb5416faa2eb06f9cfa4074\\setup.py'""'""'; _f90e9d7aafb5416faa2eb06f9cfa4074\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__, '""'""'exec'""'""'))' install --record 'c:\users\pc\appdata\local\temp\pip-record-epy40fs2\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\users\pc\appdata\local\programs\python\python37\include\webrtcvad' check the logs for full command output.",question,question
728,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/728,Error while running the app,i get this error `'tensor' object has no attribute 'repeat_interleave'`. i have installed pytorch and here is the command `conda install pytorch==1.0.1 torchvision==0.2.2 cudatoolkit=10.0 -c pytorch`,deployment,question
1070,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1070,How to use NVIDIA/apex?,"i just fix the function which defined in network.py and then add function: call netsa, netga, netdg, optimizera, netga, netdg, optimizeroriginally posted by @anxingle in",question,question
94,https://github.com/JaidedAI/EasyOCR/issues/94,Benchmarking with SOTA approaches,thanks for sharing this amazing tool. did you guys already benchmarked it with other state-of-the-art approaches (e.g. tesseract)?,question,other
278,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/278,Requirement PyQt5 is broken,there seems to be a bug with the latest pyqt5 version and it fails to install when i try to install from requirements.txt. i did however manage to install pyqt5==5.14 so i think that specific version (or anything better that does work) should be specified in requirements.txt. i'm not sure if that's the only problem with reqs since at present i can't manage to make this software work (because of issue #179 ).,deployment,deployment
294,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/294,encoder_train.py and --low_mem,"voxceleb2 dataset doesn't work with encodermeta.csv, encodermem flag to work with encoder_train.py.",question,Error
614,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/614,Is there a specific GPU model is more suitable for training cyclegan (or GAN in genreal)?,as the title suggests.,other,other
445,https://github.com/JaidedAI/EasyOCR/issues/445,EasyOCR for python 2.7?,"hi! i would like to use easyocr with python 2.7. i have installed easyocr using pip2, and i can import it using import easyocr. however, i am running into many issues trying to even run `reader = easyocr.reader(['en'])`. i am assuming this is due to some python 2 / 3 differences. one big issue is that downloading the models results in files that only contain the string `error code 1010` (paraphrased). i found the urls that the files are supposed to be downloaded from but they return errors, too (inserting them in the url bar in firefox). is there an easier way to use easyocr in python 2.7? i know 2.7 is no longer maintained but at my lab we are still working with ros melodic which is running on python 2.7 (or c++).",question,question
541,https://github.com/JaidedAI/EasyOCR/issues/541,why cannot  EasyOCR detect all the text in an image spanning multiple lines,"i have tried using easyocr and tesseract on text documents. while the page segmentation modes of tesseract are able to adjust and extract all the text available on an image, easyocr just extracts the first line and that's it. can you explain why this happens and how can we change it so that we can detect the whole text? thanks in advance.",question,question
1281,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1281,How can I save more experiment result in each epoch? ,dear sir: thank you very much,question,question
429,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/429,How does a gpu with 6G memory set gpu_ids?,"when i set gpuids,default = '0,1,2' or default = '0,2',the result is assertionerror: invalid device id",question,question
868,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/868,Is this model implemented identity mapping loss?,i want to translate painting to photo.,question,question
864,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/864,the parameter of the torch.cat(),"why is the parameter of the cancat operation 1, which means that if you put two pictures together in a column, shouldn't it be set to 2? reala, self.real_b), 1)",question,question
212,https://github.com/JaidedAI/EasyOCR/issues/212,Add more Devanagari languages (Bihari),"the devanagari model that's present can also work with the following additional languages: languageiso 639-3 ------------------- biharibh maithilimai angikaang bhojpuribho magahimah nagpurisck newarinew goan konkanigom please add the language codes to map to devanagari. or if you feel it's clumsy with many languages, just add a langlist.",other,other
258,https://github.com/JaidedAI/EasyOCR/issues/258,Unable to recognize Numbers in the picture,link: 658299 cannot be identified in the picture. result:,Performance,other
438,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/438,Query regarding the cgan paper,"in the paper ""** in experiments, its given that > the day to night training set consists of only 91 unique webcams (see results in figure 14). i would like to know how cgan was trained on such a small number of data points. was it like the data points were increased to 20k (or more) using augmentation and then the model was trained? and also how many epochs was the dataset trained on? i am trying to train the cgan model for colorization using 170 images. training the model for 500 epochs overfits the model but even when i use augmentation and increase the dataset to 10k, the model still doesnt seem to learn (things like skin color etc.). can you please help me with that? but the difference in my case is that i am using binary input images as opposed to grayscale/color images used in the paper.",other,question
260,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/260,"Confused with ""self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)","dear, thanks for the awesome work! in the file ""cyclemodel.py"", function 'backwardga(self.fakeb(self.fakegb(self.fake_a), true) have i misunderstood anything else? could you please explain it? thank you!",question,question
363,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/363,Zebrafication,i want to zebrafication my face. what should i do?,other,question
364,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/364,Regarding 3D volume generator,"hello @junyanz , @taesung89 , thank you for nice tutorial + nice codes. i ran several datasets with the cyclegan but i eventually need 3d cyclegan. here, 3d is not video (x, y, t) input but some from medical image volume (x, y, z). i don't think cyclegan support 3d input volume but just want to make sure that part. thank you!",question,other
644,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/644,cuda runtime error (30) : unknown error at torch/csrc/cuda/Module.cpp:32,"hi i am student. i was successful when i ran this code last week. however, when i ran again today, there were some errors, and i was confused. looking forward to your reply file ""/home/chy/pytorch-cyclegan-and-pix2pix-master/train.py"", line 28, in file ""/home/chy/pytorch-cyclegan-and-pix2pix-master/options/basedevice runtimeerror: cuda runtime error (30) : unknown error at torch/csrc/cuda/module.cpp:32",question,question
523,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/523,No matching distribution found for tensorflow==1.15,"i can't install this. pip install tensorflow==1.15 result: error: could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0) error: no matching distribution found for tensorflow==1.15 the latest version lauch a error like: file ""c:\users\...\python\real-time-voice-cloning-master\synthesizer\models\helpers.py"", line 3, in modulenotfounderror: no module named 'tensorflow.contrib'",question,other
503,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/503,idk what to do ,giving me this error and i dont know what to do importerror: dll load failed: the specified module could not be found,question,question
840,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/840,How to retrain model using saved weights?,"hi , 1-i have trained model on 512 x 512 visual images to generate infrared images using paired images. 2- how can i continue the training from last stored weights? do you think the performance will increase if i train this model more ? i.e for 1000 epoch. 3-please see few attached input and output examples generated with trained model. 3- any other suggestions to increase the quality of output?",question,question
659,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/659,SV2TTS doesn't open when i enter python demo_toolbox.py,"everytime i enter python demo_toolbox.py, even with dataset, it just doesn't open the sv2tts at all. i tried everything required. i just don't know why it didn't open. can you help me to make sv2tts open and run?",question,question
469,https://github.com/JaidedAI/EasyOCR/issues/469,Model Combination,"hi easyocr, thank you for the great repository! you guys are doing an amazing project! for my project scenario, i need to detect all the french and arabic in the input image. i noticed that all language detection models are the same, using the craft-mlt model. the difference for multi-languages is in the recognition model. so i am considering can i combine the arabic recognition model and the latin recognition model? to solve the problem, there are two plans in my mind: 1. use some technic to make the arabic recognition model and the latin recognition model into one model. 2. after detection, use the detection results first through the arabic recognition model and then through the latin recognition model. which plan you think is likely to solve the problem? and any suggestions? regards, kevin",question,question
1032,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1032,Data used for pretraining speaker encoder?,"a similar question was asked in #78 but it was closed without an answer. so, on which data is the provided speaker encoder pretrained? i looked through the wiki and issues but couldn't find an answer. was it pretrained on a combination of librispeech and voxceleb 1 & 2, as mentioned in the thesis? @corentinj in our case, we are taking the pretrained encoder (encoder.pt) and looking to fine-tune its last linear layer and similarity scaling parameters with a dataset of our interest. knowing on which data the encoder was pretrained would be of much help.",question,question
835,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/835,GPU Error,i have a built-in gpu on my laptop how i can use it for this tool.,question,question
530,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/530,does it stucks or takes too long?,its not finishing? im using asus rog strix scar 3 and i did installed the python version 3.7.9? and the requested packages? what more is needed?,question,question
576,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/576,able to do Speech-To-Speech Synthesis,"can it perform speech to speech synthesis, instead of text to speech",question,other
256,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/256,Paper question in pix2pix,"when i read pix2pix, i meed a problem. it‘s −da(gb(v,z0))−db(ga(u,z)) in formula 3, can one task's discriminator used in another task's generator.",other,question
612,https://github.com/JaidedAI/EasyOCR/issues/612,turning to exe file,"hi, i'm from japan! i made a project that recognize the number from a screenshot with easyocr. i wanted to make exe file to give this project to my friend, but whenever i change it into exe file and move it the next error comes up. filenotfounserror: [errno 2] no such file or directory: c:\\users\\username\\appdata\\local\\temp\\char.txt i did it with pyinstaller. does it usually work or not?",question,question
792,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/792,Datasets_Root,in the instructions to run the preprocess. with the datasets how to i figure where the dataset root is. what is the command line needed to do so. i'm using windows 10 btw. it launches and works without any dataset_root listed but i want to make it work better. and without the datasets i can't,question,question
284,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/284,alignment.txt file is not present in the data,"while running the sythesizeraudio.py file it looking for *.alignment.txt file which is not present in the dataset. as a result no audio, mel file remain empty. please answer if anyone faced this issue.",other,Error
550,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/550,using embedding from a specific wav file sounds nothing like it in output,im trying to recreate the scout from tf2s voice but the output sounds absolutely nothing like him,other,other
4,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/4,undefined symbol: PySlice_Unpack,"cool that you got it all to pytorch! when running python train.py --dataroot ./datasets/facades --name facadesgan i get: traceback (most recent call last): file ""train.py"", line 2, in file ""/home/ubuntu/pytorch-cyclegan-and-pix2pix/options/trainoptions.py"", line 3, in file ""/home/ubuntu/pytorch-cyclegan-and-pix2pix/util/util.py"", line 4, in file ""/home/ubuntu/miniconda3/lib/python3.6/site-packages/pil/image.py"", line 56, in importerror: /home/ubuntu/miniconda3/lib/python3.6/site-packages/pil/64-linux-gnu.so: undefined symbol: pyslice_unpack what versions are you using?",deployment,question
295,https://github.com/JaidedAI/EasyOCR/issues/295,About the DataParallel and the assignment to anonther device of the model,"i tried to build the model on another gpu device instead of `cuda:0`. i tried the `with` statement in torch and tried to pass the device to the `gpu` parameter of `reader` as well. in both cases, the init is ok. however, it will reports an exception like this seems to be caused by missing specifying the `devicedetector` function in `detection.py` and the 157th line in `get_recognizer` function in `recognition.py`. i modified the original code such as to and the exception disappeared. maybe you could make a little modification to fix this minor problem. thanks very much!",question,question
987,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/987,Unable to access monet2photohires images,unable to access the above images in returns permission denied.,other,other
874,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/874,loss-curves and training collapsed,"@junyanz , thank you for sharing this repo. when i train cyclegan on my own dataset, i found that the db loss decrease and converge, but the d loss is close to 0. is this normal or did my system collapse? my traina data contains 5000 images with size 160x120, and my trainb data contains 899 images with size 301x451. i use --netg resnetlayerslayersb, --direction atob), maybe it could be better. so, is the loss curve an indicator of evaluation? or what parameters do i need to adjust to keep the system from collapsing?",question,question
560,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/560,batch_hed.py Unable to create correctly shaped tuple error,"after much work, i got hed working. but the batchhed.py"", line 63, in ` ` in, ((border, border), (border, border), (0, 0)), 'reflect')` ` file ""/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.py"", line 1301, in pad` ` padvalidatewidth)` ` file ""/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.py"", line 1080, in lengths` ` normshp = shape(narray, numbernormalize_shape` ` raise valueerror(fmt % (shape,))` `valueerror: unable to create correctly shaped tuple from ((128, 128), (128, 128), (0, 0))`",question,Error
630,https://github.com/JaidedAI/EasyOCR/issues/630,readtext doesn't work with latest opencv version,"windows 10 64bit easyocr 1.4.1 opencv-python 4.5.5.62 problem when using reader.readtext i was getting an error message from opencv solution spent most of my day trying to fix whatever was wrong with connectedcomponentswithstats.... issue didn't go away no matter what i did until i switched from opencv-python version 4.5.5.62 to version 4.5.4.60 . i know this is a opencv problem so you can't do much about, but probably should limit the opencv version until they fix",question,other
73,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/73,"what‘s wrong cant run,cuda problem","i got stuck for this step,i am ubutun on windows,win10 x64 gtx970 with the lastest driver and cuda installed,anybody can help,thanks.!!! root@sc-201705310804:/mnt/e/github/pytorch-cyclegan-and-pix2pix# python test.py --dataroot ./datasets/maps --name mapsgan --phase test --no256 --whichoptions.py"", line 60, in parse file ""/usr/local/lib/python2.7/dist-packages/torch/cuda/_device runtimeerror: cuda runtime error (35) : cuda driver version is insufficient for cuda runtime version at torch/csrc/cuda/module.cpp:87 software have: root@sc-201705310804:/mnt/e/github/pytorch-cyclegan-and-pix2pix# pip freeze dominate==2.3.1 numpy==1.13.1 olefile==0.44 pillow==4.2.1 pyyaml==3.12 six==1.10.0 torch==0.2.0.post1 torchvision==0.1.9",question,question
722,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/722,It's for sale?,"are you aware of ? i found it in the search bar, and it surprised me very much",other,question
1464,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1464,Training High Resolution + Rectangular images,"hi, i intend to training images which are both rectangular and high resolution. which training flags should be used in this case? i checked the training/test tips link. however, it mentions either high-resolution or rectangular, which seems not possible to combine.",question,question
15,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/15,ntrain is ignored,"couldn't find ntrain anywhere else in the code, so i guess it's not taken into account.",Error,other
695,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/695,PixelDiscriminator should not use bias in Convolution layer with BatchNorm layer,"in `pixeldiscriminator` in `network.py`, the code about `use_bias` is incorrect. the correct code should be same as the one in `nlayerdiscriminator`.",Error,Error
199,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/199,Input for Cityscapes,"most datasets used in cyclegan are image to image datasets, but cityscapes is label to image. i'm wondering what the input is. is it the color map or the label map? if it's the label map, then do you change the generator's last output to be a n-class softmax?",question,question
824,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/824,How to improve voice cloning?,"to improve voice cloning, what is the recommended avenue? i mainly want to clone 3rd party voices, not fine-tune the model for a single speaker. i see that the latter has already been done in #437, and i also see that libritts is a better dataset than librispeech. would the recommended avenue be to then train the synthesizer only on libritts, and then replace the pretrained model (the one trained on librispeech + fine-tuned on libritts) with this new one? has this already been done? are there any other recommendations for high roi ways to improve generalization quality? thanks!",question,question
579,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/579,CycleGAN() takes no arguments,i keep getting this error: typeerror: cyclegan() takes no arguments,question,question
1021,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1021,The image is blurry and has a lot of noise (pix2pix),the image of the test result has a lot of noise and blur(pix2pix),question,other
552,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/552,It is hard to generate cars?,"when i am using pix2pix to generate scene images, i find everything is ok except cars, as shown in the following figures:",Performance,question
670,https://github.com/JaidedAI/EasyOCR/issues/670,Windows 10 + python 3.9.6 returns empty while Linux/Mac py 3.9.6 works fine,this is sample image i tried and py code,question,other
1025,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1025,Increasing batch size is not reducing no. of iterations in an epoch,"hi, i am new to cyclegan and am just getting started with the code. i am trying to run the dataset maps through it, just as outlined in the 'getting started' section. i have kept all the parameters same, except for --displaysize 4. should this not reduce the no. of iterations per epoch? whether i use --norm batch (batch normalization) or just let it be (instance normalization), the no. of iterations / epoch keeps going to 1000 (approx.) [no. of training images in maps = 1096]. am i missing something?",question,question
185,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/185,"error when running ""python demo_cli.py""","i'm getting error when i run ""python democli.py c:\python37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:542: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. quint8 = np.dtype([(""quint8"", np.uint8, 1)]) c:\python37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:544: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. quint16 = np.dtype([(""quint16"", np.uint16, 1)]) c:\python37\lib\site-packages\tensorboard\compat\tensorflownpstub\dtypes.py:550: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. npcli.py"", line 3, in file ""d:\deepfakelab\real-time-voice-cloning-master\synthesizer\inference.py"", line 1, in file ""d:\deepfakelab\real-time-voice-cloning-master\synthesizer\tacotron2.py"", line 3, in file ""d:\deepfakelab\real-time-voice-cloning-master\synthesizer\models\_rnn\_rnn\python\layers\_rnn\python\layers\cudnnrnn\python\ops\cudnnops.py"", line 22, in file ""c:\python37\lib\site-packages\tensorflow\contrib\rnn\_ops.py"", line 298, in file ""c:\python37\lib\site-packages\tensorflowcore\python\framework\registry.py"", line 61, in register keyerror: ""registering two gradient with name 'blocklstm'! (previous registration was in register c:\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\registry.py:66)""",question,question
171,https://github.com/JaidedAI/EasyOCR/issues/171,About training dataset,"hi, jaidedai i want to practice the training process by myself. can you share your (chinese traditional) training and testing dataset to me(images and label.txt)? according to my test, your chi_tra recognize model's predict accuracy is really good. i want to use your dataset and add additional images and labels for my project to strengthen my performance. can you help me? thx!!",other,question
477,https://github.com/JaidedAI/EasyOCR/issues/477,Tutorial to fine-tune /train Arabic EasyOCR model,"i test the arabic model, it performs well on some images, but worse on most images with different backgrounds. can i use the pre-trained model of easy_ocr and fine-tuning it on bunch of these images (with different background that has wrong prediction)",question,other
10,https://github.com/JaidedAI/EasyOCR/issues/10,Add a way to choose witch GPU to use,"i'm have a pc with 2 gpu, the first is pretty bad and has very little memory and isn't very powerful, but the second one doesn't have that problem. but when i use easyocr, it will always use gpu0 (the first one), and i couldn't find a way to change that currently i can't even run easyocr since my first gpu doesn't have enough memory `runtimeerror: cuda out of memory. tried to allocate 736.00 mib (gpu 0; 2.00 gib total capacity; 1.04 gib already allocated; 206.91 mib free; 1.07 gib reserved in total by pytorch)` so would it please be possible to add a way to change witch gpu gets used?",question,question
38,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/38,"ConnectionError[Errno 111], during run 'train.py' of maps","when i run : $ python train.py --dataroot ./datasets/maps --name mapsgan the error code : connectionerror: httpconnectionpool(host='localhost', port=8097): max retries exceeded with url: /events (caused by newconnectionerror(': failed to establish a new connection: [errno 111] connection refused',)) how can i solve this?? full script of error --------------------------------------------------------------------------------------------------------------------------- traceback (most recent call last): file ""/home/khryang/.local/lib/python2.7/site-packages/visdom/_send file ""/home/khryang/.local/lib/python2.7/site-packages/requests/api.py"", line 112, in post file ""/home/khryang/.local/lib/python2.7/site-packages/requests/api.py"", line 58, in request file ""/home/khryang/.local/lib/python2.7/site-packages/requests/sessions.py"", line 513, in request file ""/home/khryang/.local/lib/python2.7/site-packages/requests/sessions.py"", line 623, in send file ""/home/khryang/.local/lib/python2.7/site-packages/requests/adapters.py"", line 504, in send connectionerror: httpconnectionpool(host='localhost', port=8097): max retries exceeded with url: /events (caused by newconnectionerror(': failed to establish a new connection: [errno 111] connection refused',)) (epoch: 1, iters: 700, time: 0.456) da: 0.561 cycb: 0.163 gb: 0.794",question,question
571,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/571,pix2pix model acc becomes lower after hundeds  epochs,"i find a confusing problem: the pix2pix model acc goes high within several epochs (for example, 150 epochs), and after that , the acc goes down and down... and from the faked imaged i can see the model already get lost. i can understand why the acc does't go up anymore after many epochs, but i really can not figure out why it goes down and down. any hints here?",Performance,Performance
50,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/50,Is it possible only work with cpu ,"my imac doesn't have a nvidia gpu, i wondering is there a way train with cpu only, can anyone help me, thanks",question,question
303,https://github.com/JaidedAI/EasyOCR/issues/303,How to get the coordinates of each word?,i want to get the coordinates of each character. is it possible to locate each chinese character?,question,question
992,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/992,Error in loading state_dict for SpeakerEncoder: size mismatch,"hi! i trained a synthesizer a month ago and i could synthesize my voice, too mechanical though, but now i got this error. how can i fix this?",question,question
797,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/797,I got a nan loss on vocoder train,"hi folks, i've already made changes to my learning rate, i've already changed the batch size and it didn't. `found 117946 samples +------------+--------+--------------+ batch size lr sequence len +------------+--------+--------------+ +------------+--------+--------------+ { epoch: 1 (989/1180) loss: nan 2.3 steps/s step: 0k } ` does anyone know what this might be?",question,question
888,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/888,Picking a checkpoint model to use,is it possible to pick a specific model in the checkpoints folder to use for image generation? i renamed one of the files to latestg.pth and latestd.pth but i receive the following error: attributeerror: 'unetskipconnectionblock' object has no attribute '0',question,question
229,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/229,Viewing testing losses,"is there an option for the 'test.py' script that prints losses for each model? i've looked through the various arguments, but i can't seem to find anything. (should i just make my own?)",other,question
555,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/555,Training results are good but testing results are very blurred.,here are some training results: my commands are:,question,question
849,https://github.com/JaidedAI/EasyOCR/issues/849,Label the new data for recognition model,"good morning, everyone, thank you for this wonderful tool so highly performing. i wanted to ask how it was possible to go about creating a dataset on which to do the train and test part for calibrating the new weights if the images available are not yet labelled. is this something already explained within git or do you have to rely on other packages for this step. thank you very much",question,question
224,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/224,The Loss of the Encoder,"the loss of the encoder is computed as crossentropyloss of sim_matrix and the groudtruth(0~63 ids). why the loss designed like this? why not use the loss described in the paper ""generalized end-to-end loss for speaker verification"".",question,question
603,https://github.com/JaidedAI/EasyOCR/issues/603,Combining paragraph=TRUE and keeping confidence score,"hi, thanks for building this library! i had a couple of questions. i am trying to use easyocr and extract language from gsv images, but i am having issues with quality and performance (if anyone had any suggestions for gsv extraction, would be great). but when i use paragraph=true, it gives me a better representation of the text box i'm extracting, but i lose the confidence score -- any chance to keep it? also, i am still a little convinced but adding multiple languages into one model. if i want to extract all the arabic, swedish, and english, for instance, how do i know which text box is english or arabic? or is it better to run the models individually and use the confidence score to understand if its closer to arabic to english etc? thank you, tom",question,question
215,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/215,EXACT working environment,i've been trying to get voice cloning to work for several hours with not success. can someone be so kind as to share an ** functional working environment. packages have changed quite a bit and a mish mash between conda and pip seems unavoidable:,question,other
252,https://github.com/JaidedAI/EasyOCR/issues/252,[Android] Convert in TF-lite is possible?,"i want to use this ocr in android and 1) can i convert this to tf-lite? if possibble, the input and output is same as tf in python? 2) it seems(i'm using flutter) i made my flutter pipeline and connect the multiple tfs. am i right?",question,question
88,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/88,'aligned_data mode' means the data is paired?,"if the dataset_mode == aligned then, does the data have to be paired? i don't know exectly what it means, 'aligned', 'unaligned', and 'single'",question,question
247,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/247,AttributeError: 'ResnetGenerator' object has no attribute 'module',"i have downloaded a pretrained model and a dataset. then when i tried to generate the results using the command line provided in the readme, the following error occurred. i guess the recent code reformattings caused the problem. maybe the pretrained models did not get updated. traceback (most recent call last): file ""test.py"", line 18, in file ""/home/zyh/xxx/pytorch-cyclegan-and-pix2pix/models/_model file ""/home/zyh/xxx/pytorch-cyclegan-and-pix2pix/models/testmodel.py"", line 81, in load_ attributeerror: 'resnetgenerator' object has no attribute 'module'",Error,Error
580,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/580,PaErrorCode -9997,"demo_cli.py works fine and crates a .wav file, but when trying to play something with the toolbox i get an error (exception: error opening outputstream: invalid sample rate [paerrorcode -9997]) and i don't know how to access the results. i followed the steps as closely as possible.",question,question
1160,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1160,pix2pix What if I train data with sketch in left side?,"what if i train data with sketch in left side and face in right side? if i do so, can i still test with only sketch photo?",question,question
372,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/372,About load_network,"when i wanted to load pretrained model by using loadcyclegan/50dcyclegan/50dmodel file ""f:\ylp\pytorch-cyclegan-and-pix2pix\models\cyclemodel.py"", line 81, in initialize attributeerror: 'nonetype' object has no attribute 'parameters' i found that returned '**' ,so i got above error.",question,Error
242,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/242,From [here](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf#page=12):,"from : > a particularity of the sv2tts framework is that all models can be trained separately and on distinct datasets. for the encoder, one seeks to have a model that is robust to noise and able to capture the many characteristics of the human voice. therefore, a large corpus of many different speakers would be preferable to train the encoder, without any strong requirement on the noise level of the audios. additionally, the encoder is trained with the ge2e loss which requires no labels other than the speaker identity. (...) for the datasets of the synthesizer and the vocoder, transcripts are required and the quality of the generated audio can only be as good as that of the data. higher quality and annotated datasets are thus required, which often means they are smaller in size. you'll need two datasets: the second one needs audio transcripts and high quality audio. here, finetuning won't be as effective as for the encoder, but you can get away with less data (300-500 hours). you will likely not have the alignments for that dataset, so you'll have to adapt the preprocessing procedure of the synthesizer to not split audio on silences. see the code and you'll understand what i mean. don't start training the encoder if you don't have a dataset for the synthesizer/vocoder, you won't be able to do anything then. _originally posted by @corentinj in",other,other
547,https://github.com/JaidedAI/EasyOCR/issues/547,minor bug when migrating from 1.4 to 1.4.1. ,"as stated in the 'what's new' section, in version 1.4.1: > extend rotationinfo = [270]` i get all the text right but 'barreaux europeens'. when using `rotationinfo it gives the right results for the horizontal text as well. but the thing is that i need both the 90 and 270 rotation and when the elements are 2+ in rotation_info i only get the results for the last element which was not an issue in version 1.4. for now i am just going to use version 1.4, but needed to inform of that minor bug.",Error,Error
501,https://github.com/JaidedAI/EasyOCR/issues/501,When will easyocr support text direction?,"as i know, you are implementing this feature, aren't you? thank you!",other,other
1311,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1311,about image size,"i noticed that cyclegan resizes both domain images to the same size, if i have two images with very different sizes and i don't want to resize them to the same size to avoid missing a lot of details, how should i deal with this problem, looking forward to your advice, thanks a lot.",question,question
776,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/776,Encoder converging extremely fast ,"hi there, i used voxceleb1 &2 dataset to train an encoder with adam and init_lr set to 1e-4, but the algorithm converges way too fast, loss decreased below 1 in under 1000 steps. i used a batch size of 64x10. is there something i am missing or should i just further decrease the lr.",question,question
857,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/857,Transfer Learning with pix2pix,"hello, thank you for your great work! i would like to ask about the possibility of applying transfer learning to the discriminator in pix2pix. the source and target images are concatenated together at the channel level, hence the resulting channels is 6 instead of 3. however, most if not all transfer learning models are trained with and expect images in 3 channels, so how can i use these pre-trained models in the discriminator?",question,question
925,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/925,Is one of the Generators always supposed to be doing worse than the other?,"i have gb, gb (real to synthetic). is this normal? any thoughts on how to make g_a's job easier?",question,question
561,https://github.com/JaidedAI/EasyOCR/issues/561,how to provider http restfull api in dock deploy?,i didn't found any documents about http web api interface.,question,question
820,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/820,dataset  problem,"i have downloaded the dataset provided in your git but after opening toolbox its giving warning that""you do not have any of thr =e recoganized dataset in'path' """,question,question
1325,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1325,how to set lambda properly?,"i see the paper set identity loss as 0.5lambda and cycle-consistent loss as lambda to control the relative importance. what if gan loss just about 0.5-1.0 around but idt loss and cycle loss up to at least 10 at the first. will it be ignoring the adversarial loss while training? thanks alot. : ) by the way... when i change the framework to 3d, my gpu memory ran out and i exchange to torch.cuda.amp mode, with gradscaler(), will it make the generator performance worse? i use it as medical image generating and my mae just around 90... bad performance... : (",question,question
818,https://github.com/JaidedAI/EasyOCR/issues/818,Custom model fine-tuning on initial weights,"thanks for the awesome package. i'm trying to fine-tune easyocr and create a custom model, however i don't necessarily want to create an entirely new model. i'd like to fine-tune it to recognise more specific fonts for my use case. easyocr performs the best for my use case out of other packages but could be improved i think. i have generated data to create my own custom model, but find the results are worse. i've generated enough data and can only assume the recognition is trained from scratch when i create my own custom model, but haven't been able to determine if this is actually the case from the docs here. is there a way to use the original weights from easyocr or simply fine-tune the model with new, additional data? or should i look at generating more data which is broader than my use case to try and replicate the same best performance of easyocr?",question,question
248,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/248,"VoxCeleb1: found 0 anglophone speakers on the disk, 1123 missing (this is normal)","preprocessing librispeech_other couldn't find voxceleb1\librispeech\train-other-500, skipping this dataset. preprocessing voxceleb1 voxceleb1: using samples from 1123 (presumed anglophone) speakers out of 1251. voxceleb1: found 0 anglophone speakers on the disk, 1123 missing (this is normal ). voxceleb1: preprocessing data for 0 speakers. voxceleb1: 0speakers [00:00, ?speakers/s] done preprocessing voxceleb1. preprocessing voxceleb2 couldn't find voxceleb1\voxceleb2, skipping this dataset. trying to start train with voxceleb1. program do not see speakers on disk. but i have dataset with id10001, id10002... perhaps something missed ?",question,question
253,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/253,Error training after saving the `latest` model.,"i think the latest commit cause the problem. a quick way to solve swap these two lines of code however, this does not sovle multigpu support error.",Error,Error
517,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/517,CycleGAN visdom 4th image in each row is switched,it seems like in visdom the 4th image in each row (labeled idtb respectively) is reversed. the top right most image is the identity loss for (realb) and the bottom right image is the identity loss for (reala).,other,Error
1062,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1062,The direction of picture would be reversed sometimes,"i found that during the training step, the direction of the input data would be reversed at times. that is the left side of the input picture would become the right side of the output picture in the temporary print stage. if the picture is colored, the color of the pictures would also be reversed at times. how can i solve such problem?",question,question
760,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/760,RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd,"last year i had a working copy of this software running in archlinux, but after various system updates, trying to run this software again, i see this error:",question,question
521,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/521,Adding custom loss function to discrimator  ,"i am trying to add a custom loss function which takes real and fake images to perform a custom parametric matching. i have found the ** on > pix2pix_model.py script as but however, my custom loss function will take the real and fake images as arguments and return a score. i am confused how to access the real and fake images that is taken on account in the equation above. i have found a line where the images are found on the same script as",question,question
853,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/853,Why cycle loss use L1 loss？,"i wonder why cycleloss use l1 lossfunction. i'm new in cv. i think maybe people almost like to use mse or something else. did you try to change the cycle loss to other loss function? if you try, is it the best? just want to know why,thank you",question,question
1382,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1382,"Warning: wandb package cannot be found. The option ""--use_wandb"" will result in error.","warning: wandb package cannot be found. the option ""--use_wandb"" will result in error. how can i solve the problem? thanks a lot.",question,question
1226,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1226,Loss trend validation ,"hello. i hope everyone are doing well. i have trained the cyclegan on set a and set b below: a--> fakea --> my loss trends is as below, **? the discriminator losses sit around 0.2, 0.3 in the plot below:",question,question
160,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/160,error demo_cli.py,"when runninf `python democli.py"", line 76, in file ""/home/kubutsushimuzan/bureau/real-time-voice-cloning/encoder/inference.py"", line 145, in embedtospectrogram file ""/home/kubutsushimuzan/.local/lib/python3.7/site-packages/librosa/feature/spectral.py"", line 1391, in melspectrogram file ""/home/kubutsushimuzan/.local/lib/python3.7/site-packages/librosa/filters.py"", line 247, in mel valueerror: operands could not be broadcast together with shapes (1,201) (0,) `",question,Error
177,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/177,"numba.errors.TypingError: Failed at nopython (Preprocessing for parfors) Invalid usage of Function(<function argsort at 0x7fc2a69a9ae8>) with parameters (array(float32, 1d, C), const('quicksort'))","traceback (most recent call last): file ""/home/ubuntu/real-time-voice-cloning-master/toolbox/_fromrealumaplearn-0.3.10-py3.6.egg/umap/umaptransform file ""/home/ubuntu/.local/lib/python3.6/site-packages/umap.py"", line 1625, in fit file ""/home/ubuntu/.local/lib/python3.6/site-packages/umap.py"", line 559, in fuzzyset file ""/home/ubuntu/.local/lib/python3.6/site-packages/umap.py"", line 232, in nearestp36/lib/python3.6/site-packages/numba/dispatcher.py"", line 344, in forp36/lib/python3.6/site-packages/numba/six.py"", line 658, in reraise numba.errors.typingerror: failed at nopython (preprocessing for parfors) invalid usage of function() with parameters (array(float32, 1d, c), const('quicksort')) * parameterized in definition 0: this is not usually a problem with numba itself but instead often caused by the use of unsupported features or an issue in resolving types. to see python/numpy features supported by the latest release of numba visit: and for more information about typing errors and how to debug them visit: if you think your code should work with numba, please report the error message and traceback, along with a minimal reproducer at:",other,other
602,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/602,The training script gets stuck at this.,i have succesfully installed the requirements. im training it on horse2zebra dataset. the training script seems to get stuck at this. any idea why this is happening? im using macbook pro with no gpu support. the script i'm running is this - python train.py --dataroot ./datasets/horse2zebra --name horse2zebraids -1,question,question
416,https://github.com/JaidedAI/EasyOCR/issues/416,Arguments and vertical Japanese text,"i have used easyocr successfully a few times now, however i'm running into issues. firstly, a guide recommended using a ""paragraph"" argument. putting ""--paragraph=1"" into my script caused an error called ""unrecognized arguments"" and that error persisted on every variation i tried. typing ""easyocr -h"" tells me that the options i can use are language, input file, detail of output and if i want to use gpu. these options seem very bare bones and in contrast to guides. also, japanese vertical text does not work. it only grabs the uppermost character of each column and puts it on its own line in the output (which is very annoying even when it does manage to grab the text correctly). can that be fixed? i tried downloading the bigger japanese.pth and replacing the one in the install directory, but the program deleted it. is it not compatible with the latest git download? did i get the wrong version? is there even a way to check version? the standard ""--version"" did absolutely nothing. one last thing, how do i make it stop pestering me about my two different gpus? to clarify, i'm using windows 10's cmd to run easyocr.",question,question
234,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/234,Error running demo_cli.py,"i am currently on a fresh install of manjaro, and i have followed all of the instructions to attempt to get it to work. whenever i run `python3 ./demo_cli.py` i get the following. sorry in advance if my issue is really easy to resolve as i am relatively new to python, and typically use java.",question,question
314,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/314,"My voice is hoarse, like electric sound",can anyone answer my question,Performance,question
853,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/853,Mobile Text to Speech engine,is there a way to make a (mobile/android) text to speech engine using this? something like google tts or svox tts engine (pico) ?,question,other
833,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/833,"ValueError: num_samples should be a positive integer value, but got num_samples=0","hello everyone, i've created my own custom dataset for pix2pix and also ran `python datasets/combineanda /path/to/data/a --foldab /path/to/data`. the command executed nicely and generated the paired {a, b} images as promised. however, when i try to run the train command `python train.py --dataroot path/to/abdata --name petnc 1 --outputsamples should be a positive integer value, but got numpath).convert('rgb')` in data/alignedpath)`. could that be contributing to the problem? this is the full error message: > dataset [aligneddataset] was created > traceback (most recent call last): > file ""train.py"", line 29, in > dataset = createmode and other options > file ""/content/pytorch-cyclegan-and-pix2pix/data/_dataset > dataworkers=int(opt.numsamples={}"".format(self.numsamples should be a positive integer value, but got num_samples=0",Error,question
594,https://github.com/JaidedAI/EasyOCR/issues/594,Not predicting new images,"i trained the easyocr from scratch on my own data. when doing ocr on the test data i realized very strange behavior. for example for the following image it predicts ""instantaneous tripping unit"", but the image contains only rst . i checked and the training data there is instantaneous tripping unit but not rst . if i look at the training data, it seems that an image that contains rst, similar to the attached image, has been wrongly labelled as "" instantaneous tripping unit"". so it seems that what happens is that the model just maps the given image to the most similar image in the training set and uses it's linked text as the output of ocr ( like classification), which has nothing to do with ocring. this is my training config: - transformation: 'none' - featureextraction: 'resnet' - sequencemodeling: 'bilstm' - prediction: 'attn' - numchannel: 1 - outputsize: 128 - decode: 'greedy' - newfeaturefxtraction: false - freeze_sequencemodeling: false what i am doing wrong here?",question,question
20,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/20,NaNs during CycleGAN training,"i have run approx. a dozen test runs (using train.py) on 2 datasets (maps and my own custom dataset trying to convert synthia to cityscapes). every run so far is giving nans after a couple of epochs, sometimes after more than 70 epochs, sometimes after only a handful of epochs. until i am getting only nans actual learning seems to really happen as e.g. evidenced by looking at transformed images over epoch number. i have also played with various learning rates, but even at pretty low lr nans seem to eventually occur. **: is this something others have also observed? second: in case this is ""normal"" and e.g. due to the difficulties of training gans (min-max), what would be critical params to vary to eventually avoid training to break down?",question,question
1162,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1162,train.py: error: unrecognized arguments: --epoch_count ,"hi, i'm trying to continue training with this options: --continuecount 200 but i'm getting ""train.py: error: unrecognized arguments: --epoch_count"" is there anything wrong with that? thanks",question,question
185,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/185,Training with both unpaired and paired datasets,"hi, i am using your code for some time (the cyclegan for unpaired training), and i am wondering how difficult it would be to use a mix of unpaired+paired training. in my problem, i have small paired datasets, and huge number of unpaired images. i am intending to train using unpaired data, while use paired ones to 'guide' the training. how hard would it be to change your code to support so? sorry for the silly question, but i am new to pytorch :p",question,question
218,https://github.com/JaidedAI/EasyOCR/issues/218,Data Requirement,"i want to train this model for handwritten digits only. what should be the ideal data requirement for training the network for numbers only? this is the kind of data, i want to train the model on.",question,question
113,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/113,Toolbox Save File,"is there a way to save the files generated in toolbox? its giving me a name like ""recordings10646"" but this might be just a temp name because i can't find that file. thanks!",question,question
826,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/826,Module not found: No module named 'numba.core',when i try to run the demo_toolbox this error prevents me from opening it. does anyone know what is going on?,question,question
233,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/233,How to use chekpoint files ?,"i trained cyclegan with my own dataset. and i have ** files in checkpoints/atob/ how can i test with my different dataset ? python test.py --dataroot ...? (i already read readme file, but i don`t have latestg.pth file)",question,question
840,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/840,General question about embedding size,"hey, i am interested in training a one-language model, which also has less accent. i would like to train a good quality model on about 1000 speakers. then i want to fine tune on a single speaker (like #437) (with 5 minutes or even hours of audio) to finally get a good single speaker model. now my question is: does the model benefit from an embedding size (or only hidden size in encoder) of 768 like sberryman did in #126, even if training time and vram usage increases heavily? or is it only interesting for multi-language/multi-accent models and i would definitely waste my time with that? or even get worse results? i also use 48000 as sample_rate, as most of my samples (of commonvoice) are in 48k, maybe this has an impact? thanks in advance :)",question,question
736,https://github.com/JaidedAI/EasyOCR/issues/736,About incorporating CRAFT training code to this repository,"it seems that many people need the craft training code, and i recently uploaded a reimplemented training code with similar performance to the craft paper. would it help if i incorporated my training code into the repository here? @rkcosmos",question,other
932,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/932, How do I organize my data to evaluate unlabeled test data in pix2pix?,"hello, how do i organize my data to test.py unlabeled test data in pix2pix? i configured it like a 'facades' dataset for learning. but there is no label for test data. should i test it by creating a label on an empty image like the train, val data configuration? thank you.",question,question
592,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/592,Are the state of the optimizers being saved?,"are the optimizers state being saved during training so that it can be continued after stopping, with exact same state? it seems only model weights are being saved",question,question
392,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/392,SystemExit:2,"when train.py is executed in jupyter notebook it shows :an exception has occurred, use %tb to see the full traceback. systemexit: 2 why?",question,question
604,https://github.com/JaidedAI/EasyOCR/issues/604,Floating point boxes,"hello, when using simple predict: on this image: note that some of the bounding boxes are in different format: is this format of results a bug or feature?",question,question
689,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/689,CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.96 GiB total capacity; 1.07 GiB already allocated; 12.50 MiB free; 135.23 MiB cached),"hi, please guide me solve this issue. i am just following your github steps to run it successfully.",question,other
848,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/848,Training Time Estimate,i am running a training for the encoder for nepali language with 527 speakers and approximately 200~hours of audio data. all the training parameters used are default parameters. how can i estimate the total training time for this? (forgive me if this is something simple enough to ask. i am new to this whole thing and am enthusiastic to learn further about this.) thank you!,question,question
643,https://github.com/JaidedAI/EasyOCR/issues/643,Language code clarification.,"hi first of all great work. i'm currently trying to fine-tune the latin language model but am confused about all the languages you have used . could you provide a key value mapping or something of the sort, that maps the code to the language it refers to. thanks",other,question
258,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/258,TypeError: Object of type 'Tensor' is not JSON serializable,"hello,i run your train.py with gpu and the newest pytorch >>> torch._send file ""/home/yt/anaconda3/lib/python3.6/json/_a: 0.164 ga: 1.978 idtb: 0.312 gb: 4.014 idtsend file ""/home/yt/anaconda3/lib/python3.6/json/_current_results file ""/media/yt/searchforgraduationdesign/pytorch-cyclegan-and-pix2pix/util/util.py"", line 30, in tensor2im runtimeerror: can't call numpy() on variable that requires grad. use var.detach().numpy() instead.",question,question
59,https://github.com/JaidedAI/EasyOCR/issues/59,ImportError: cannot import name '_validate_lengths' from 'numpy.lib.arraypad',can't `import easyocr` on winpython 3.71 on windows 10. installed without errors though,question,question
269,https://github.com/JaidedAI/EasyOCR/issues/269,AttributeError: module 'torch.jit' has no attribute 'unused',i'm facing one issue related to torch. my torch version is 1.2.0+cu92 and torchvision is 0.5.0 thanks,other,question
534,https://github.com/JaidedAI/EasyOCR/issues/534,Bad Batch OCR Performance,"using a rtx 6000 (24 gb vram), i can perform ~15-20 images(nheight=1919, latinbatched is faster than processing images sequentially and do you know any reasons which could cause this performance loss?",Performance,Performance
678,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/678,ImportError: cannot import name ABC,"hi, i am pulling the docker file () and trying to run the following command after running . python train.py --dataroot ./datasets/--name gangan error: traceback (most recent call last): file ""train.py"", line 22, in file ""/containeroptions.py"", line 1, in file ""/containeroptions.py"", line 5, in file ""/containerfolder/models/base_model.py"", line 4, in ** can you please help? any lead will be appreciated. thank you",question,question
649,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/649,Trian epoch,"i have10000 images both triana and trianb for clothes stlye transfer. like modern to medieval, set niter and niter_decay both 40 is ok ?",question,question
291,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/291,Control pauses between words,"is there any way to make pauses between words bigger, words stick together and pronounced too quick. can i insert extra pauses between words using some punctuation signs (commas, for example) or somethings else? i can do it manually in audio editor later, but it is more complicated than add normal pauses during synthesis.",question,question
621,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/621,What is the training time for a K80,"hey, i'm thinking of implementing cyclegan from scratch for a school project. what is the training time for getting okay results with say the horse2zebra dataset using a k80?",question,question
564,https://github.com/JaidedAI/EasyOCR/issues/564,How to read text without dictionary,"hi, i am trying to read small text snippets that are likely not to be found in any dictionary. for example: other snippets might only contain numbers or 'random' character sequences, so adding everything to the dictionary is not an option. is it possible to use easyocr for such a use case? with the standard configuration the result is completely wrong. see also this screenshot i took from your web tool for the same snippet:",question,question
593,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/593,Encoder's embeds _loss becomes NaN,"hi, i gave a try training the encoder on my dataset (which is much smaller - around 500 speakers) to see how the model behaves. however, at nearly step 25,000, i encountered an error: `` valueerror: input contains nan, infinity or a value too large for dtype('float32'). `` does anyone know what is the error here ? my 2cent is it may encouter the vanishing gradient problem (possibly because the dataset is too small). if this is the case, should i decrease the learning rate ? any help or comments are appreciated :d.",Error,question
260,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/260,"Demo - DLL Load Failed, Toolbox - QT issue","hello! i've managed to iron out most problems, but here's what i'm currently stuck with! all requirements installed, win 10, python 3.7, cuda 10 (but tried 8 and 9 too with relevant cudnn) pip list running demotoolbox.py any insights in to this? many thanks!",question,other
490,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/490,How do I train a model with my own data? Where can I find the instruction?,how do i train a model with my own data? where can i find the instructions on how to do it? need help,question,question
199,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/199,No encoder models found in encoder/saved_models BUT the files are in the directory!,"i have a really weird problem here. i have the pretrained.pt file located under: d:\a\encoder\saved_models but it says that it can't find the model. what am i doing wrong here? edit: and yes, i have downloaded the pretrained models from here:",question,question
420,https://github.com/JaidedAI/EasyOCR/issues/420,I want to use a custom model,"i want to use english as it is and only change the korean model. i put the files(koreang3.pth) in the folder(userg3','en'], gpu=false, recogg3"") some errors have occurred 1) valueerror: koreanlist=koreang3' has no attribute 'model’ 3) size mismatch for sequencemodeling.0.rnn.weightl0: copying a param with shape torch.size([1024, 512]) from checkpoint, the shape in current model is torch.size([1024, 256]). thank you",question,Error
264,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/264,"vocoder training : ""operands could not be broadcast together with shapes""",i've this issue with vocoder training,question,question
712,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/712,286x286 PatchGAN,"hi, what is the correct config for using the whole imagegan in pix2pix i suppose it has to be --netd nlayers_d. any hint would be appreciated. thanks!",question,question
325,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/325,Training on rectangle images,"hello, i am starting using the code to train on my own dataset. i have images of size (800x600), any tips what to do i need to edit inside the code/options, to fit my dataset ?",question,question
592,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/592,Training on custom data,is there a guide for training this using custom voice data we collect on our own?,question,other
76,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/76,ValueError: frames must be specified for non-seekable files,"preprocessing voxceleb2 voxceleb2: preprocessing data for 5994 speakers. voxceleb2: 0% 0/5994 file ""/nasdata/yangyg/real-time-voice-cloning/encoder/preprocess.py"", line 175, in preprocesspreprocessdirs file ""/home/yyg/anaconda3/envs/realtime/lib/python3.6/site-packages/tqdm/speaker file ""/nasdata/yangyg/real-time-voice-cloning/encoder/audio.py"", line 28, in preprocesscheck_frames valueerror: frames must be specified for non-seekable files now ,what can i do to solve it. thanks.",question,question
18,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/18,Out of memory when training my own datasets,"i want to train my own dataset with ~4800 images of training data, the size of each image is 512×512, no matter when i set the --loadsize (and --finesize) to 512, 256, 128, the program run out of memory with nvidia gtx 1080 (8g gpu memory). i'm new to use pytorch, i wonder whether it was caused by pytorch or my gpu memory is not sufficient for your code.",question,question
121,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/121,What are the hardware minimum requirements to work with this software?,i only got super old (10+) laptops with ancient graphic cards in them. maybe i can use a friends machine with a i5 and nvidia card..,other,question
716,https://github.com/JaidedAI/EasyOCR/issues/716,cuDNN version incompatibility,"full error below: `cudnn version incompatibility: pytorch was compiled against (8, 4, 0) but linked against (8, 3, 3)` easyocr version: 4.1.2 pytorch version: 1.11 python version: 3.10.4 output of `nvcc --version` but it is able to be imported.",question,deployment
388,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/388,Request: Alternative Pretrained models for UK accent,"whilst this tool is fascinating and fun to play with, the pretrained models made available for download impart a us accent on uk english speakers. i don't have the compute power to train models, does anyone know of available models for download that would fair better with uk accents?",other,other
687,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/687,5 seconds is not real-time!,5 seconds is not real-time bro! real-time means under milliseconds order! may it be happen by using more cpu/gpu resources or not ??!,question,question
426,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/426,pytorch is failing to install,"using 3.7 python,",question,deployment
1312,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1312,Dataset link not always accessible,it seems like the dataset link is region-limited? i can download it on my computer but it doesn't work on colab. i also asked a couple folks and some of them can access while others cannot. i suspect there is some sort of region limitation for being able to access this dataset? hopefully this can be fixed as soon as possible since many people are likely trying to use these datasets.,question,other
462,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/462,Output file not as clear as audio played within toolbox,"hello, so after a week i finally got the toolbox to work (i have never worked with python). the project i am working on is replicating my deceased father's voice to have his cloned voice read his memoir. the issue i am running into is that the output .wav file that is rendered is of lesser quality than the audible voice that the toolbox plays. i also noticed i am unable to export as a .flac file. it states ""file contains data in an unimplemented format"" which makes me think i don't have something installed right. any help someone can give me would be greatly appreciated. thank you! hal",question,other
294,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/294,"If I paused running it, could I still run it from the breakpoint?","hi,as the title shows",question,question
801,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/801,test problem,"hello, the document of testa is 17125 images, when i run the test code, the generation images in the results document are only 4744. please tell me how to deal with this problem?",question,question
357,https://github.com/JaidedAI/EasyOCR/issues/357,latin.pth recognition model for azerbaijani does not detect characters other than english and german umlauts,"hi, thanks for the great repo. i have a need to ocr different languages such as az - azerbaijani cs - czech da - danish de - deutsch en - english es - spanish, castilian et - estonian fi - finnish fr - french hr - croatian 1) it does not recognize characters other than english and german when i used the azerbaijani recognition model. but, i could not read any special characters like for example, if i read an image with the following azerbaijani text after loading the model for azerbaijan language ('az'): ** s i it gives the output as siket nümayandesi it misreads the characters highlighted in bold. 2) i have a use case where i need to use both cyrillic and latin model together cause i don't know which language type of images i need to process for ocr. when i tried mixing cyrillic and latin languages, it was throwing errors. is there any way where i could combine these two different language models for recognition? thanks in advance :)",question,question
647,https://github.com/JaidedAI/EasyOCR/issues/647,SSL verification error on Mac.,"i don't get this error on my windows machine. this is a fresh venv with modules just installed, tried installing both pip and github, neither worked.",question,deployment
235,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/235,I keep getting TypeError: Invalid file: WindowsPath,"hi guys i keep getting this error when running python demopreprocess_audio.py typeerror: invalid file: windowspath('d:/ai/librispeech/train-clean-100/103/1240/103-1240-0000.flac') any help with this would be fantastic it may be something simple i have only just started with python a few days ago. i am running windows 10 and using anaconda and have downloaded all the files required. i just cant seem to load any voices in the toolbox through voxceleb, librispeech or custom audio files in any format but i can record my own voice in toolbox. thanks guys hopefully someone can help me out. cheers glenn",question,question
746,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/746,CUDA out of memory only in python demo_toolbox.py (python demo_cli.py works),"`python demospectrograms file ""c:\python\real-time-voice-cloning-master\synthesizer\models\tacotron.py"", line 464, in generate file ""c:\python\pyto\lib\site-packages\torch\nn\modules\module.py"", line 889, in impl file ""c:\python\real-time-voice-cloning-master\synthesizer\models\tacotron.py"", line 142, in forward runtimeerror: cuda out of memory. tried to allocate 50.00 mib (gpu 0; 2.00 gib total capacity; 1.07 gib already allocated; 0 bytes free; 1.11 gib reserved in total by pytorch)` i mean that demo_toolbox dont able a % free of memory... someone can help me how i can reserve memory to app?",question,question
802,https://github.com/JaidedAI/EasyOCR/issues/802,Details about recognition model architecture,"hi @rkcosmos , i noticed that there have been two different architectures used for recognition models. what's the motive behind that and why only some selected languages chosen for the bigger and complex one. also, could you provide the python file for the bigger architecture, just like the one you mentioned",question,question
1128,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1128,Does pix2pix is good for face transformation?,"i'm trying to make different face masks like faceapp,snapchat. does pix2pix is good for this?",question,question
63,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/63,"Sorry, you can't view or download this file at this time.","seems google drive have some limitations `too many users have viewed or downloaded this file recently. please try accessing the file again later. if the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. if you still can't access a file after 24 hours, contact your domain administrator.`",other,other
371,https://github.com/JaidedAI/EasyOCR/issues/371,"CUDA out of memory, any SOLUTIONS available are NOT WORKING","i'm using easyocr gpu-enabled to detect numerics from images(448480, 512*512). it runs fine the first time and the second time it gives this error. i tired to clear memory by those torch clear cuda memory methods. torch.cuda.emptygrad, even numba reset device, and to restart spyder kernel too through kill process. but nothing is working out. i'm monitoring gpu and mem usage through gputil, at the end of the process i see gpu is 0% but mem is keep on increasing. please find below a detailed error. any vailable solutions will be appreciated. first run: using tensorflow backend. id gpu mem ------------------ 0 10% 20% before none {0: 'window', 1: 'door', 2: 'bg'} elapsed time for postprocessing of layer2 = 81.84690499305725 ppm detection on closed places: [11.3] [4] ppm detection on closed places: [5.59] [4] elapsed time for calculating ppm is: 6.606334686279297 {0: 'sofa', 1: 'dining table', 2: 'bed2', 3: 'bed2sidetable', 4: 'tvtable', 7: 'tableasarray.py:83: visibledeprecationwarning: creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. if you meant to do this, you must specify 'dtype=object' when creating the ndarray return array(a, dtype, copy=false, order=order) id gpu mem ------------------ 0 0% 36% none id gpu mem ------------------ 0 0% 36% none time taken for the process: 134.68992948532104 json output is saved! 2021-02-08 04:22:01.599227: i tensorflow/streamloader.cc:44] successfully opened dynamic library cudart64code file """", line 1, in file ""c:\program files\jetbrains\pycharm 2020.3.3\plugins\python\helpers\pydev\bundle\pydevpydevpydevmain.py"", line 64, in file ""c:\repos\dedi-server-ai-gpu\inceptionmodules.py"", line 200, in aipreprocessingremovetextgpu.py"", line 61, in removemain file ""c:\repos\dedi-server-ai-gpu\preprocessingmodulestexttextbox file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\easyocr\detection.py"", line 38, in testcallparallel.py"", line 159, in forward file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\module.py"", line 727, in impl file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\easyocr\craft.py"", line 60, in forward file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\module.py"", line 727, in impl file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\easyocr\model\modules.py"", line 61, in forward file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\module.py"", line 727, in impl file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\container.py"", line 117, in forward file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\module.py"", line 727, in impl file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\conv.py"", line 423, in forward file ""c:\programdata\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\conv.py"", line 420, in forward runtimeerror: cuda out of memory. tried to allocate 1.23 gib (gpu 0; 8.00 gib total capacity; 693.49 mib already allocated; 500.57 mib free; 740.00 mib reserved in total by pytorch)",question,question
486,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/486,Try to train Synthesizer,"try to train synthesized on train-clean-100 data, but have the next one issue:",question,other
1393,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1393,Training Results do not match Test Results [Pix2Pix],"i am training a conditional model to generate logos. however, the results outputting during training are much better than when i try to do inference on a single image. for example, real a one thing i noticed that occurs across all test images is that the colors seem to ""bleed"" across the edges. could this have something to do with how my test input image is saved or do i need to do some sort or pre-processing step to make the test results more accurate to the training results. i am also getting this warning when i run test.py. not sure if it is relevant. userwarning: argument interpolation should be of type interpolationmode instead of int. please, use interpolationmode enum. thanks in advance",question,question
504,https://github.com/JaidedAI/EasyOCR/issues/504,Overlapping boxes do not merge - Solution proposal,"when two boxes following each other overlap, they are not merged by easyocr currently. example: currently, to decide if two boxes following each other should be merged, the *np.mean(bmax) < widthheight) - box[5]) < heightheight)) and ((box[0]-xths * value is no longer calculated in the second condition.",other,Error
756,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/756,CycleGAN Fake B looks very similar to real A (vice versa),"hi, i've trained a cyclegan model to do unpaired translation of medical imaging of domain a to domain b (and vice versa). my input data are 3d volumes of the human body, which are transformed into 2d 256x256 (only 1 channel) patches so that they can be passed into the network for training. after preprocessing, i have 160k patches of domain a and 59k of patches of domain b. for training, i used the following flags: `--model cycleids 6,3,0,5 --preprocess none --datasetnc 1 --outputdatasetthreads 24 --batch_size 24` i let it run for 50 epochs and the result is that real a and fake b looks very similar (and so is real b and fake a). this is the result after 50th epoch during training: does anyone have any idea of what went wrong here and the possible remedies? (apart from data augmentation, which i will try out next). any help or feedback is greatly appreciated.",question,question
250,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/250,Train network without scaling or cropping,is there an option to train without resizing the dataset? it seems like i have to select one of the 4 alternatives [resizecropcropscalewidthcrop]. thank you.,question,question
331,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/331,CUP Training Time,i am currently trying to train cyclegan using only my computers cpu. are there any flags that i should run with in order to decrease the training time? i have 36 cpu cores to work with. how long should i expect training to take when i use the cpu? thanks!,question,question
438,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/438,unable to download pretrained model,"hello, thanks for sharing the code of this amazing tool, i've set it all up on my pc but i can't download the pretrained model. the mega link is not valid anymore and the google drive link says the file is temporarily unavailable. any chance the models could be somehow made available? thanks in advance, romain",other,other
858,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/858,D_loss drop fast and G_loss drop slow,"when i train my dataset,i fand that loss of discriminative drop very fast,after 3 epoch ,loss_d is about 0.001,but the l1 loss is about 0.18 and the result of g is bad.",question,Performance
815,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/815,"With net_G: unet_256, how to keep the size of the input?","thank you for your wonderful project. it helps me a lot. recently, i have a pre-trained pix2pix model, in which net256. when i tried to translate a 1920x1088 picture with this option, the output was resized to 256x256. > python test.py --dataroot ./datasets/rgb2ir/ --name rgb2ir256 --direction atob --datasetnc 1 is there a method to keep the size of the input picture? must i retrain the model with net9blocks?",question,question
907,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/907,No pth documents generated in checkpoints,"i used other images to replace the images(horse2zebra),but when i tried to train my model and pth documents. the process showed no pth documents in my checkpoints//. but the train process have no error. i am a freshman in gan field. hope someone can help me to sove this problem",question,question
998,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/998,doesn't recognize speakers,"when i start preprocess, it doesn't recognize speakers and goes for an infinite loop. how to fix this? i have a mozilla common voice dataset and used",question,question
549,https://github.com/JaidedAI/EasyOCR/issues/549,Fail to run sample code due to OMP issues?,"i set the environment variable `kmplib_ok=true`, reboot the system, but it also fails. windows 10 20h2 19042.1237, python 3.8.5, installed packages:",question,deployment
393,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/393,can't open demo_toolbox.py,"whenever i try to open demo_toolbox.py, this happens: please help!",question,question
610,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/610,couldn't communicate with the NVIDIA driver in WSL,"hi guys, i tried to use wsl (windows 10 subsystem for linux) as my environment. i have already installed driver, cuda and cudnn for both windows and linux separately but when i use nvidia-smi to check it says ""nvidia-smi has failed because it couldn't communicate with the nvidia driver. make sure that the latest nvidia driver is installed and running."" anyone knows what is going on?",other,question
677,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/677,How to train new model from Mozilla Common Voice?,"how to train new model from mozilla common voice? i wish to train new models from mozilla commmon voice. i chosed polish. i already changed .tsv files to match librispeach style ""namefile.mp3 description"" (i dont know can sometell me if the big/small letters are important?). but this is for i dont know synthesizer? vocoder? i don't even know what is this... and why do i need three of them??? how can i train three of them, if mozilla gives me only one datasets... what should i do, can someone answer me, how should i train this?",question,question
674,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/674,Real Time Voice on MacBook,"hello, i am a mac user and trying to install your ""software"". but when i launched democli.py"", line 1, in modulenotfounderror: no module named 'encoder.params_model' can you help me?",question,question
777,https://github.com/JaidedAI/EasyOCR/issues/777,Is it possible to obtain coordinate of every bounding box?,"hello, first thanks for the model it is really helpful. but one question is whether it is possible to obtain the coordinate of every bounding box. we are obtaining coordinates of multiple bounding boxes as you can see in the original image and another image is drawn rectangle with green color.",other,question
697,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/697,Network Choice for Paired data,"hi, which network style do think works better in the case of paired data (pix2pix): unet, resnet, or any other network architecture that you can recommend? thank you.",question,question
54,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/54,B to A training?,i'm really enjoying the library. maybe consider having a flag for training from b->a so the dataset doesn't have to be redone.,other,other
288,https://github.com/JaidedAI/EasyOCR/issues/288,how to convert the specified model to standard onnx model,"hi, i have tried converting the provided model in onnx format but getting failed, print(""exported started for recognize model to onnx "") inputchannel = 512 hiddenclass = 6710 model = model(inputchannel, hiddenclass) statesim.pth"", mapstatedict.items(): model.loaddict(newdict) below both lines failed torch.onnx.export(model, torch.randn(1, 3, 128, 128, device='cpu'), ""recognames=(""image"",""text""), outputversion=11) print(summary(model, (torch.zeros((1, 3, 128, 128)),torch.zeros((1, 512,6710))), showexport file ""/home/vijay/.local/lib/python3.6/site-packages/torch/onnx/utils.py"", line 366, in totracegetfromgetgraph file ""/home/vijay/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 722, in impl file ""/home/vijay/.local/lib/python3.6/site-packages/torch/jit/_callslowdetector(""/home/vijay/easyocr/model/craft25k.pth"", ""cpu"") print(summary(dectect, torch.zeros((1, 3, 128, 128)), showtext.onnx"",inputnames=(""preds"", ""confs""), opset_version=11) print(""detection model exported successfully"") please provide solution, how can i export the model to onnx successfully",question,question
794,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/794,Trying to Find Bottleneck When Using Nvidia Jetson Nano,"hi, great work on this! it's amazing to see this working! i am testing this software out on a , and am seeing ~1 minute needed to synthesize a waveform, and am trying to figure out what the bottleneck could be. i originally tried this code on my windows machine (ryzen 7 2700x) and saw about 10 seconds for the waveform to be synthesized. this testing used the cpu for inference. on the jetson, it's using the gpu: `""found 1 gpus available. using gpu 0 (nvidia tegra x1) of compute capability 5.3 with 4.1gb total memory.""` it did seem to be ram-limited at first, but created a swap file to file the gap and did not see the ram changing much during synthesis. i can see it being read during synthesis and the read time of disk slowing everything down, but it looked like one of the four cpu cores was also taking a 100% load to process, making me think that i'm cpu bottlenecked. i figured that since this project uses pytorch, using a 128 cuda core gpu would be faster than an 8 core cpu, but i may be missing some fundamentals, especially when seeing that one of my cpu cores is at 100% usage. is synthesis cpu and gpu constrained or would it rely mostly on gpu? here are images of the program just before it finished synthesizing and just after with jtop monitoring gpu, cpu, and ram. ** - 5.5gb of memory used. 3.4 is ram, 2.089 is swap file on disk - cpu1 at 12% - cpu 2 at 98% - gpu at 0% thank you! voloved",Performance,question
1330,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1330,"Cycle-GAN(Pytorch) Why, when designing the backward fucntion, the generator is defined once and the discriminator is defined twice?","the file is: cyclemodel.py gb are backpropagated together, while db call the backward function twice. i suspect that the two of them should be the same. hope to get your answer. thx!!! ` `",question,question
548,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/548,Encoder training does not use GPU,"hi, i have a titan rtx and cuda 10.2 installed correctly on my system. nevertheless, the encoder training happens on cpu. is there a way to make it use the gpu? i could not find any option.",question,question
114,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/114,Core dumped with pytorch 0.2.0 and multi-gpu,"does anyone has the same problem? it often occurs randomly. by the way, this code works well in previous pytorch versions, but it will take up more memory.",deployment,other
27,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/27,Question: batch size,"the paper indicates that training was done with batch size = 1 is there a reason not to use a slightly larger batch size to more fully occupy the gpus? for example, are the results better with batch size = 1 than with batch sizes larger than 1?",question,question
227,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/227,Short text samples,"it would be awesome to be able to use this to help train a hot word detector. in addition to recording myself saying the hotword, i could create an even larger dataset by adding outputs of this model that used my voice as the reference. the problem with that, however, is that this model seems to only work well on sentences of medium length (+- 20 words according to demo_cli.py). is there anything i can do to make short text samples (e.g. 2 words) sound better?",Performance,other
170,https://github.com/JaidedAI/EasyOCR/issues/170,Vertical Characters support ,"my particular use case would benefit from vertical, character by character detection. so something like the following would work response for the above is ['',''] the following example does work: response for this is perfect, ['triu', '8625018'] any tips on how to get the first example to work? i don't mind if the list returned is a character at a time.",Performance,other
636,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/636,Get different output for the same image?,"we use pix2pix to do a segment task. however, we get different result images for the same input image each time. why?",question,question
768,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/768,How to run test in the aligned mode?,"i try to train and test the cyclegan in the aligned mode. all works fine except of finding resuts of test. a put test images to the dataset\test folder, run test.py, it wrote processing (0000)-th image..., processing (0005)-th image... and exit. and nothing was happend, no any error or exception, and no result images anywhere.",question,question
23,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/23,How to load my own audio files in demo_cli.py?,"i have several audio files from the same person. in gui, i can load them using browser and then synthesize and vocode some text. how can i do the same thing without gui? thanks",question,question
1085,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1085,Can pix2pix fine tune the geometry structure of the image,"i am trying to beautify the teeth using gan. in the upper picture, the teeth are not very straight. i want to use gan to generate the straight teeth just like the teeth in the lower picture. can pix2pix or cyclegan do this job ?",question,question
627,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/627,Trouble with starting toolbox.,"in the past, back in september, i could manage to run this program, but now i am running into the same error. a s s e r t i o n f a i l e d ! p r o g r a m : c : \ u s e r s \ d a n i e \ a n a c o n d a 3 \ e n v s \ t e s t \ p y t h o n . e x e f i l e : s r c / h o s t a p i / w d m k s / p a w d m k s . c , l i n e 1 0 8 1 e x p r e s s i o n : f a l s e i cannot bypass it and i have found no solutions online either.",other,other
559,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/559,Generate a speech which still keep the speaker's speaking rate,"i've gotten some good results with this project. it is really amazing! however, as title, is keeping speaker's speaking rate achievable during generating? i knew that there's an optimum length of input text(too short: the voice will be stretched out with pauses; too long: the voice will be rushed). e.g., there are 5 people a, b, c, d, and e. their speaking rates are different. my input sentences are: >please call stella. ask her to bring these things with her from the store: >six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother bob. >we also need a small plastic snake and a big toy frog for the kids. >she can scoop these things into three red bags, and we will go meet her wednesday at the train station. i expected to get 5 different length of output voice. however, all of the output utterances are 17-19 seconds. that's the reason why i'm curious. is it possible to keep speaker's speaking rate? if it is, could anyone tell me how to make it or give me a hint? should i change the encoder for capturing more speaker's features, or i need to modify on synthesizer? thanks in advance for anyone's reply.",question,question
1299,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1299,Load only G_A to make transformations faster,"hi, i have spend some time now on training a model. however, when i want to test the model it takes approximately 1second to transform one image. i think this is long. when my dataset is approximately 20000 images, it will take 5.5 hours to transform all the images. i am only interested in the g_a transformation. how can i speed up my transformations?",question,question
544,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/544,Please install ffmpeg or add the '--no_mp3_support' option to proceed without support for mp3 files.,"i typed python .\demomp3_support' option to proceed without support for mp3 files."" please guide me!",question,other
1316,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1316,Test results (question),"is it possible to save the full result image, and not only the half of fakeb?",question,question
679,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/679,Ethics in technology like this,"i am raising an issue regarding ethics in technology, specifically, with technology like this. my 90yo grandparents were just scammed using deep-fake audio tools similar to this, into sending $9500 cash to a bail-bondsman because one of their children ""was arrested from a car accident, a baby was dead, and alcohol was detected"". look, putting this source code on the internet for anyone to use without limits is not only an ethical issue, but a security issue. deep-fake audio cloning should be illegal and should be considered identity theft. what does this project intend to do about the use of this software for harmful purposes? not even a disclaimer or hoop to jump through? on the same note, are you intending to write and release malware or viruses with open-source code that could be used to shut down the power grid for a major hospital, or show how you can infiltrate a top security prison to unlock all of the jail cells? it's just iot? are you going to show how you can control lights and locks and call it totally fine? this type of project is not only unethical, but it's wrong. github's community standards suggests there is educational value here. i disagree, but would like to see anything that might help prevent this type of misuse. i would recommend pulling access to this code and forming an ethics in ai group to review what has already been done since nothing can be done to undo this type of thing. ""active malware or exploits being part of a community includes not taking advantage of other members of the community. we do not allow anyone to use our platform for exploit delivery, such as using github as a means to deliver malicious executables, or as attack infrastructure, for example by organizing denial of service attacks or managing command and control servers. note, however, that we do not prohibit the posting of source code which could be used to develop malware or exploits, as the publication and distribution of such source code has educational value and provides a net benefit to the security community.""",other,other
991,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/991,Replot Loss After Train,"hi, i'm just wondering if there is anyway i can replot the loss from the txt file after training finished?",question,question
453,https://github.com/JaidedAI/EasyOCR/issues/453,[Bug] Colab is not working,i don't know if it is a dns problem from my side or not,other,question
493,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/493,Is it possible to specify which model to test ?,"let's say i have multiple checkpoints under the `./checkpoints/.../` folder, 30gnetb.pth 120gnetb.pth 200gnetb.pth latestgnetb.pth etc. it seems that the script `test.py` only test with `latestgnet... .pth` without renaming the file or modifying the code provided ? renaming checkpoints can be a problem if there are 200 checkpoints to test. i didn't find the answer by reading readme or existing issues.",question,question
131,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/131,CycleGan with Paired Images,"hello, i am dealing with the following problem: i have two paired medical image datasets from two different modalities. however, the images cannot be registered well, so pix2pix should not be a good option. cyclegan is a method that works fairly well and was already employed by wolterink et al. for mri/ct datasets in ""deep mr to ct synthesis using unpaired data"" using your dl framework. however, cyclegan seems to be not the perfect choice under these conditions since the information that there indeed exists a corresponding (though not registered) image pair is neglected. i don't have a lot of experience in machine or deep learning, so my thinking might be very naive: in cyclegan i think the idea to use a second generator to go also from b -> a instead of only a-> b is - simply put - great. but this should not only hold for unpaired image data, but also for paired data. wouldn't it make sense to combine the strengthes of pix2pix and cyclegan ? best, florian",other,question
11,https://github.com/JaidedAI/EasyOCR/issues/11,Failed to download pretrained models,"i am having trouble downloading pretrained models from using google chrome, i keep getting error ""failed - file incomplete"". is it caused by my internet connectivity? nope, i have no problem accessing other websites. i think there's an issue with the server.",question,question
117,https://github.com/JaidedAI/EasyOCR/issues/117,Identification of text features,being able to identify text features like the following: typography * bold/italic/strikethrough would make this ocr library really powerful. could it be possible as future work?,other,other
1000,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1000,"train on Multi CPUs/GPUs,  FileExistsError: [Errno 17] File exists: './checkpoints/gan_pix2pix'","i train the pix2pix model on 20 cpus and 2 gpus, and sometimes error occurs while sometimes does not. the error message is: i wonder is this error caused by cpus/gpus? thank you!",question,question
57,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/57,Recommended length of input audio?,"what should be the length of input audio? what are other requirements(can there be silence, etc.)? is it feasible to use several audio files and averaged embedding for better quality?",question,question
301,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/301,"Exception NameError: ""global name 'FileNotFoundError' is not defined""",i change '--resizecrop' option as 'scalea: 0.373 ga: 2.286 idtb: 0.358 gb: 1.072 idta: 0.208 ga: 1.876 idtb: 0.237 gb: 1.517 idta: 0.244 ga: 1.458 idtb: 0.244 gb: 1.726 idtcurrent_results nameerror: global name 'connectionerror' is not defined,other,question
482,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/482,Model files not found.,"while trying to install the real-time-voice-cloning repo, i am unable to run the toolbox, because i always get the following error: i tried this command python `demotoolbox.py -d c:\users\raphael\documents\github\real-time-voice-cloning\models` but nothing works. is there someone that can help me with this issue? :) p.s. i downloaded the ffmpeg libary but i don't know where i should paste it in.",question,question
435,https://github.com/JaidedAI/EasyOCR/issues/435,Rotated image,when image orientation rotates the results are going very bad. how can i solve this issue? the text should extract correctly even the image rotates 90* inversely.,question,question
515,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/515,Problem opening the toolbox,"everytime i try to open the toolbox i get this: traceback (most recent call last): file ""demo_.py"", line 1, in file ""c:\users\myusername\desktop\real-time-voice-cloning-master\toolbox\ui.py"", line 5, in file ""c:\users\myusername\desktop\real-time-voice-cloning-master\encoder\inference.py"", line 2, in file ""c:\users\myusername\desktop\real-time-voice-cloning-master\encoder\model.py"", line 5, in modulenotfounderror: no module named 'torch'",question,other
681,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/681,Bug in inference code,"synthesizer/inference.py: len(inputs) returns you 1 (if demoinputs is not ""batched"" in any sense. batch inputs",Error,Error
245,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/245,error while training with 128X128 images,"while running with 128x128 images i changed finesize=128, loadsize = 143, i get the following error.. this i do not get when i run with 256x256 images.. please help > /home/rohit/torch/install/bin/lua: /home/rohit/torch/install/share/lua/5.2/cudnn/init.lua:162: error in cudnn: cudnnbad_param (cudnngetconvolutionndforwardoutputdim) > stack traceback: > [c]: in function 'error' > /home/rohit/torch/install/share/lua/5.2/cudnn/init.lua:162: in function 'errcheck' > ...torch/install/share/lua/5.2/cudnn/spatialconvolution.lua:140: in function 'createiodescriptors' > ...torch/install/share/lua/5.2/cudnn/spatialconvolution.lua:188: in function /lua/5.2/cudnn/spatialconvolution.lua:186> > (...tail calls...) > /home/rohit/torch/install/share/lua/5.2/nngraph/gmodule.lua:345: in function 'neteval' > /home/rohit/torch/install/share/lua/5.2/nngraph/gmodule.lua:380: in function > (...tail calls...) > train.lua:220: in function 'createrealfake' > train.lua:341: in main chunk > [c]: in function 'dofile' > ...ohit/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk > [c]: in ? >",question,question
73,https://github.com/JaidedAI/EasyOCR/issues/73,Tensorflowjs model,i can't seem to find much online about converting pytorch models over to tensorflowjs. i'm looking for a good scene text detection model primarily for use on screenshots--so the text will be very clear. i'd even be willing to donate a little if the accuracy was sufficiently high.,other,other
107,https://github.com/JaidedAI/EasyOCR/issues/107,Punjabi Language Gurmukhi script,i would like to contribute to punjabi language in the gurmukhi script.,other,other
617,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/617,Bus Error: 10,"i am using python 3.7 with macos 11.1 big sur. mac mid 2015 with 8gb ram. when i try to load the dataset on the qt, i get a bus error : 10. datasets_root: /users//downloads/real-time-voice-cloning loaded encoder ""pretrained.pt"" trained to step 1564501 bus error: 10 can you help me?",question,question
349,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/349,Html visualizations interfere,"hi, thank you for the clean and well documented pytorch implementation. i have one issue with the html visualizations when training multiple models on the same machine. results from one model end up in the 'images' folder and 'index.html' file that corresponds to the other experiment. for example, running two different experiments the 'fake_b' image on epoch 0 in the html visualizations are the same although the parameters of the models are different. somehow the visualizer of one experiment instance overwrites results of the other instance. both experiments are running with different 'opt.name' parameters and therefore different 'checkpoint' folders. how can i solve this issue? thanks in advance, tycho",question,question
664,https://github.com/JaidedAI/EasyOCR/issues/664,failure of simple test while passing others,"hi. i've tested the file in your example folder, english.png and as expected, get the attached file number10.jpg (it's so simple it appears in here in the forum looking like a bolded ** text below ) works in the demo on your website but on my computer, it doesn't. i'm puzzled. could there be some options used in the demo portion of your website? thank you! t",question,question
7,https://github.com/JaidedAI/EasyOCR/issues/7,Error when reader.readtext can't detect text,when you run `reader.readtext ` on an image that has no text you get the following error:,Error,other
976,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/976,Invalid Load Key,"as per the last issue, i figured out it's because the dropbox link was dead. i commented out the line of code where it was calling it and it worked. but now i'm getting an ""invalid load key"" error, even though it's able to find the models just fine. it just can't load them. why is this?",Error,question
3,https://github.com/JaidedAI/EasyOCR/issues/3,I tried to run it and ran into some problems,"warning error `traceback (most recent call last): file ""c:/users/amp/pycharmprojects/untitled3/test.py"", line 2, in file ""c:\users\amp\pycharmprojects\untitled3\venv\lib\site-packages\jaidedread\jaidedread.py"", line 87, in __ unicodedecodeerror: 'gbk' codec can't decode byte 0x81 in position 2: illegal multibyte sequence ` i don't know what to do about it",Error,Error
577,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/577,List python/whatever versions required for this to work,"could you list proper versions of the required components? there seem to be a hundred versions of python, not to mention there's something called ""python 2"" and ""python 3"". even minor versions of these thingies have their own incompatibilities and quirks. i keep getting there's no package for tensorflow for my config, i've tried a lot of combinations with x64 pythons for about an hour. now i'm giving up, it's hopeless...",other,question
806,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/806,[Question] Can i use tool from command line?,"[question] can i use tool from command line? eg. tool -voice ""voicestousagetospeach/example.mp3"" -test ""text to speach"" -o /save_file.ogg or smoethink like that? test to speatch in selected voices and maybe emotion voices?",question,question
497,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/497,Generating large images,"hello everyone, thanks a lot in advance for any help! best, alexey",other,other
641,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/641,the problem of python -m visdom.server,"when i input this code, the error is appeared. how to solve this problem?",question,question
1274,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1274,Why did I lose some of the images when running combine_A_and_B.py?,"i used ! python datasets/combineanda datasets/wbb datasets/wbab datasets/ab here's the output: [foldresize/a [foldresize//b [foldimgs] = 1000000 [usemultiprocessing] = false split = test, use 8/8 images split = test, number of images = 8 split = train, use 30/30 images split = train, number of images = 30 split = val, use 8/8 images split = val, number of images = 8 however there are only 11 combined images in datasets/ab/train and no image in other folders at all",question,question
161,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/161,FileNotFoundError training encoder,"i have gone through the preprocess for the encoder, which output the sv2tts folder with each speaker in the individual folders. all do contain the sources.txt file is not found. `traceback (most recent call last): file ""encoderprocessutils.py"", line 369, in reraise filenotfounderror: caught filenotfounderror in dataloader worker process 0. original traceback (most recent call last): file ""f:\python\venv\lib\site-packages\torch\utils\data\workerutils\fetch.py"", line 47, in fetch file ""f:\python\venv\repo\encoder\dataverificationobjects\speakerobjects\speakerobjects\speaker.py"", line 34, in randomobjects\speaker.py"", line 14, in utterances file ""f:\python\lib\pathlib.py"", line 1165, in open file ""f:\python\lib\pathlib.py"", line 1019, in sources.txt'` i have rerun the preprocessing but still get the same error. do you know what could cause them to not be found?",Error,question
130,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/130,Confusion regarding Dataset and use embedding from. ,is the idea that the more speakers i load or i record the more data this has to train with? or is the librispeech dataset just there to pull speaker voices to clone? and is there a way to improve the voice to sound more accurate with more samples from the same voice? i tried recording my voice several times but it didn't sound any better.,question,question
818,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/818,"  File ""C:\Real-Time-Voice-Cloning-master\venv\lib\site-packages\torch\__init__.py"", line 81, in <module>     from torch._C import * ImportError: DLL load failed: The specified procedure could not be found.  (venv) C:\Real-Time-Voice-Cloning-master>","having an issue with windows installation when attemping to do ""run python demotoolbox.py traceback (most recent call last): file ""demo_.py"", line 81, in importerror: dll load failed: the specified procedure could not be found. (venv) c:\real-time-voice-cloning-master>`",question,question
351,https://github.com/JaidedAI/EasyOCR/issues/351,Optimize for number plate recognition,"how would i optimize easyocr to operate in just one language ( e.g. english ) to do number plate recognition/anpr? can we make the ocr capability faster by stripping out other languages or is this controlled by passing it the language as a parameter? i think this has great potential for number plate recognition, but it needs to be faster. thank you for your help.",question,question
477,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/477,OSError: [WinError 126] The specified module could not be found NOT TORCH ISSUE,"`c:\users\denis\desktop\real-time-voice-cloning-master>python demoexecutor/platform/default/dso100.dll'; dlerror: cudart64executor/cuda/cudartcli.py"", line 4, in file ""c:\users\denis\desktop\real-time-voice-cloning-master\synthesizer\inference.py"", line 1, in file ""c:\users\denis\desktop\real-time-voice-cloning-master\synthesizer\tacotron2.py"", line 3, in file ""c:\users\denis\desktop\real-time-voice-cloning-master\synthesizer\models\_core\_handleload file ""c:\program files\windowsapps\pythonsoftwarefoundation.python.3.7x64_module file ""c:\users\denis\.platformio\penv\lib\site-packages\tensorflowcore\python\keras\_core\python\keras\_core\python\keras\activations.py"", line 23, in file ""c:\users\denis\.platformio\penv\lib\site-packages\tensorflowcore\python\keras\utils\multiutils.py"", line 22, in file ""c:\users\denis\.platformio\penv\lib\site-packages\tensorflowcore\python\keras\engine\trainingdistributor3.7.2288.0_ oserror: [winerror 126] the specified module could not be found` i tried every torch version, reinstalled cuda toolbox and checked al the requiremends same as ffmpeg",deployment,question
634,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/634,Synthesizer doesn't work in Python 3.8+,"the synthesizer uses tensorflow 1.x, which is not supported in python 3.8 or higher. we need help to upgrade the synthesizer model. the preferred approach is to replace it with a pytorch-based synthesizer. this is something we discussed in #447. i submitted a pr #472, which was not accepted due to bad voice quality. the heavy lifting is done but more work is needed to improve the model. please respond to this issue or submit a pull request to help out.",other,other
237,https://github.com/JaidedAI/EasyOCR/issues/237,"Pillow version warning,  Pillow<7.0, but you'll have pillow 7.2.0 which is incompatible.","this may be fixed... error: easyocr 1.1.3 has requirement pillow<7.0, but you'll have pillow 7.2.0 which is incompatible.",other,deployment
329,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/329,Running on a remote server,"hello, i am training the network on a remote server, which does not have any browser. is there a way to view the results on visdom from my local machine ?",question,question
111,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/111,can i define the --dataroot inside the test file?,"i am wondering how i can define my --dataroot inside the test.py when i am testing. the reason is that i have different sub-folders and need to change among them. so i want to kinda change the path from one folder to another while in a loop, that is why i need to define it in the file insted of using --dataroot. i appreciate it if you can let me know how it would be possible...",question,question
16,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/16," pix2pix without B side image,how to test","hi,sir: but i dont know how to do the test when i only have an a side image.should i conbine the a image with an empty image or something ? help wanted ,thank you!",question,question
563,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/563,what is the difference between cyclegan and pix2pix,i want to know the difference between cyclegan and pix2pix。 why you put it together。,other,question
1279,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1279,My test results doesn't give me any index.html,"i trained the model and everything ran smoothly. but when i tested the model, it ran but didn't give me any output, not even the index.html that was supposed to. i trained the model with the following options: and i'm trying to test it with the following options: this is what i'm getting when i run the test: thank you in advance.",question,question
1365,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1365,Discrepancy of dataset size in the two domains,"hello, i'm trying to adopt the idea of cyclegan on solving a domain mapping problem. say i have domain a and b, and i want a function mapping data from b to a. the problem is that the amount of available data in the two domains differ significantly: i have ~20k entries in domain a and only ~4k entries in domain b. it seems that most cyclegan-like implementations (this original one and others for different tasks) load the same amount of data (a batch) in two domains and train the two semi-cycles simultaneously, which implies that only the domain with smaller amount of data is exhaustively used for training, and a large proportion of data in the other domain are not used for training. that is, in my case, only ~4k out of ~20k entries in domain a are used for training. the mitigation currently in my mind is to duplicate entries in domain b so that the total amount of data in two domains are basically the same. is this a good idea? please give me some suggestions.",question,question
879,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/879,How to train models using Multilingual LibriSpeech,i want to train my own model using dataset from website. how can i adopt it to train my own model from it?,question,question
567,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/567,C++ version,could you please make a pure c++ code version without interpreting languages and cuda? so that it could be easily embedded into win32 and win64 applications.,other,other
657,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/657,ways to predict semantic label,"hi, thanks for the code. i have a question about the way to predict semantic label when doing *-to-semanticlabel experiments. say we have c kinds of semantic labels. did the network outputs hxwxc (i.e. each semantic label is a channel and is one hot) or it just outputs hxwx3, where each color represents some semantic label?",question,question
202,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/202,Extending pix2pix and CycleGan to Hyperspectral images,"hi, any advice for someone hoping to extend pix2pix and cyclegan to images with more than three channels in the input/output images ( e.g. : hyperspectral images ) ? thanks !",question,other
843,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/843,Is there TensorFlow implementation of  vocoder(waveRNN)?,"hi , i want to know that is there tensorflow implementation of vocoder(wavernn)? i ask this because that when i try to change the pytorch model of wavernn to tensorflow 2.x version, i succeed and can train it on dataset, but i find that the inference speed is 10 times slow that wavernn of pytorch.... it occurs in 'for loop' of [vocoder.generate()] function: def generate(self, mels, batched, target, overlap, mucallback=none): ................ ............. tensorflow2.3 version: ............... this part generate 1 sample using 5s for pytorch, but even 50s in tensorflow2.3! and i don't know why. could you please help me with the problem?",question,question
842,https://github.com/JaidedAI/EasyOCR/issues/842,About common words OCR to wrong words on traditional Chinese.,"i found that some words will ocr to other wrong words on traditional chinese. since there are common words ocr to wrong words, there are already 1500+ images with wrong ocr for one word. i keep these images, can i compress them and upload them to the cloud to send them to you? maybe you can retrain them?",question,question
734,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/734,Number of epochs and steps for encoder,i am new to machine learning and am exploring the encoder model code. i read the thesis behind the code where it is mentioned that the model is trained on 1 million steps. the code for the encoder model just iterates over dataset loader and i wonder if i have to set the number of epochs to 1 million seperately. any help would be appreciated.,other,question
1049,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1049,How to change the max epoch when training?,"the default epoch is 200, but how to change it? i can't find which parameter to adjust it. if i want to training 400 epoch? thank you",question,question
629,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/629,Error in macOS when trying to launch the toolbox,"traceback (most recent call last): file ""/users/luke/documents/real-time-voice-cloning-master/demo_.py"", line 1, in file ""/users/luke/documents/real-time-voice-cloning-master/toolbox/ui.py"", line 6, in modulenotfounderror: no module named 'encoder.inference'",question,question
911,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/911,Edges shifted and mirrored in edge images after batch_hed.py and PostprocessHED.m,"i am ** for convenience; - the images are form the edges2shoes dataset, and i cut the paired image in two: one edge image and one object; - image size is 256x256; - in postprocesshed.m it error frist, said can't find 'predict' variable, i changed the code in function getedge, from load(filepath) to load(filepath, 'predict'); i got stuck here for a long time, searched many issues and can not find a similar one, so i opened a new issue for it. could you please take a look at this issue ? thanks a lot ! @junyanz",Performance,question
621,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/621,Error: Model files not found. Follow these instructions to get and install the models:,"i have downloaded the files about encoder and so on. but when i run toolbox,it still show this: error: model files not found. follow these instructions to get and install the models:",question,question
298,https://github.com/JaidedAI/EasyOCR/issues/298,'charmap' codec can't encode character '\u2588' in position 12: character maps to <undefined>,"while installing reader = easyocr.reader(['en']) gets an error progress: -------------------------------------------------- 2.0% complete traceback (most recent call last): file ""f:\vscodeocr\test.py"", line 2, in file ""f:\anaconda3\lib\site-packages\easyocr\easyocr.py"", line 244, in _andhook file ""f:\anaconda3\lib\encodings\cp1252.py"", line 19, in encode unicodeencodeerror: 'charmap' codec can't encode character '\u2588' in position 12: character maps to",question,Error
238,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/238,How can I speak with cloned voice in realtime after cloning any voice.,"when use this toolbox, only text is spoken with cloned voice. but if the user wants to speak by using any cloned voice in real-time without text, only with his voice, how can perform that? i want to know how can do this.",question,question
831,https://github.com/JaidedAI/EasyOCR/issues/831,"KeyError: ""There is no item named 'cyrillic_g2.pth' in the archive""","hi, with new 1.6.0 version i got this error simply doing this: reader = easyocr.reader(['ru','en']) recognition and detection models are downloaded but there are no cyrillic_g2.pth in what is downloaded, because there are no second generation recognition model for cyrillic, so it look like from version 1.6.0 is trying to use the second generation model anyway, while it should fallback to the first generation if it does not exist. this is a bug because the same thing works fine in version 1.5.0.",question,other
700,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/700,NotADirectoryError occurs when using combine_A_and_B.py,i am preparing my own datasets and ** could anyone give me some advice? thanks a lot !,question,question
372,https://github.com/JaidedAI/EasyOCR/issues/372,Small mistake in group_text_box() method,hi there. there is a small mistake in . here we are comparing distance between boxes `abs(box[0]-xths *. so all that needs to be changed is to replace `box[3]-box[2]` with `box[1]-box[0]`.,other,other
168,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/168," decrease lines to 1000 on the train.txt , but it's still not effect","change to 16 batch-size have effect, but it's not good idea, ithink have any one tell me the skill on it ?",question,Error
25,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/25,Hang on audio file load,"i've tried to figure out why, but couldn't fix it: the process completely hangs (forever) in this line: pressing ctrl+c causes the ui to become responsive again but loading obviously fails. running the same command (`import librosa` and `librosa.load(""datasets/...."", 16000)` in a shell works fine) i thought it might be caused by something like : but setting this at the start of the process doesn't fix it: attaching a debugger to that line works as expected, but as soon as you step into that line it's impossible to pause the process again. running the same librosa.load command in the debug repl also causes it to hang.",Error,question
531,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/531,How to show images with different sizes using visdom?,"i would like to use visdom to show the results during training, however, how i can show images with different sizes, such as 256x256 (input images) and 128x128 (intermediate results) at the same time. when i am trying to show 256x256 and 128x128 images in one visdom screen, i got these error as follows: > traceback (most recent call last): file ""/home/csdept/projects/train.py"", line 36, in file ""/home/csdept/projects/util/visualizer.py"", line 100, in displayresults file ""/home/csdept/anaconda3/envs/py3/lib/python3.7/site-packages/visdom/_f file ""/home/csdept/anaconda3/envs/py3/lib/python3.7/site-packages/visdom/_to_numpy valueerror: could not broadcast input array from shape (3,128,128) into shape (3)",question,question
334,https://github.com/JaidedAI/EasyOCR/issues/334,French language support,"hi, the model is not detecting french language, how can i add the model or train it?",question,question
122,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/122,"test resnet_9blocks or resnet_6blocks then such error appears：KeyError: 'unexpected key ""model.10.conv_block.6.weight"" in state_dict'","when testing resnetblock.6.weight"" in state6blocks. why i trained resnet model,there was no bug ,but tested it unsuccessfully？ hoping your help.",Error,question
358,https://github.com/JaidedAI/EasyOCR/issues/358,"easyocr.recognize is significantly slower when given several boxes to estimate, rather than running it several time with one box each time","hello, thank you for this tool, it is great. i want to build on top of it, and execution time is a matter of importance for me (even on cpu). i don't know if it's a bug or a , but i've noticed that `easyocr.recognize` is significantly slower when called once and given `n` boxes to estimate the text in, rather than called `n` times with one box each time. how to reproduce ### 1/ download and ** is it an expected behavior? many thanks!",Performance,question
309,https://github.com/JaidedAI/EasyOCR/issues/309,can i merge multiple langague model?,"i want deteact kor, chtra (jpn) pretrained model?",question,question
80,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/80,"What are nThreads, resize_or_crop, and pool_size doing?","so i read the help part for the parameters that are used, but i am a little confused regarding nthreads, resizecrop, and poolorandandsize: it says pool_size is “the size of image buffer that stores previously generated images” and it is set to default=50, can anyone explain to me what it means? thanks",question,question
259,https://github.com/JaidedAI/EasyOCR/issues/259,Unable to detect english(latin) letters on indian number plates.,i want to use easyocr for number detection of vehicle. but easyocr was failing to detect. if possible can please guide me train the model using additional dataset? help will be appreciate. easyocr output : 'ka0319993' expected output : 'ka03mn9993' expected o/p : ka03ms7979 expected o/p: hr 26 bg 0383 easyocr o/p : b6?+ 3+5li0 expected o/p : wb62 b 5170,Performance,question
922,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/922,CycleGAN inference,"can i get multiple variants from one trained cyclegan in inference? for instance: i have one picture of a horse and i would like to have 4 different(!!!) pictures in style, trained in cyclegan. is it possible?",question,question
384,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/384,Improving repeatability of voice cloning,"how can i make the voice cloning results repeatable? for a given model with same input.wav + text to synthesize, is there a way to ensure that i get the same output every time? i see this code in synthesizer/train.py:",Performance,question
988,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/988,"If possible, change the title of the repository - w.r.t realtime","hello, i have asked a few of my ml friends for any reference to a ""real-time"" voice cloning software and they keep sending me the link to this repo. ""real time"" voice cloning is when i speak into a microphone, and someone else's voice comes out of a virtual audio interface in ""real-time"". this is a ""text-to-speach + clone"" and has nothing to do with a ""real-time"" voice cloning. am i wrong? if i am wrong, i apologize, as i am hoping that i am directed to the right documentation. if i am right, then please remove the ""real-time"" from the title of the repository, so all google searches don't end up here. thanks, val",other,other
611,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/611,How about adding a flag to decide whether to add SN to D and G ?,"sn has been applied to d to stabilize the training of adversarial models. and patchgan_sn has been verified its effectiveness in many works (e.g., gated conv, edgeconnect, also in spade) maybe we can add a flag to define whether to add this stuff to d/g.",other,other
547,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/547,Training net on just one voice?,"hi, i'd like to train the network just to clone one voice. can someone give me an advice on how to do this?",question,question
213,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/213,performance of batchSize > 1,"i ran cyclegan with batchsize = 2/4 and i got really bad results comparing to batchsize = 1, can you explain why it happens? also tried to update the network only by the error of one of the images but it didn't help.",question,question
305,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/305,audioread.exceptions.NoBackendError comes up whenever i try to load my own data :(,"pls help! whenever i try to load in my own data set to clone from, it says: exception: expected ""str, byte..."" on the toolbox itself. when i try to load from my root with a saved wav file (demo_toolbox.py -d ) it says: audioread.exceptions.nobackenderror on the terminal",question,question
875,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/875,vocoder precprocess,this error comes out when i try to run the vocodersynthesis valueerror: too many values to unpack (expected 3),question,other
420,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/420,Error when using scale_width and grayscale images,"i'm training on my own dataset and scaling the image widths to be 256 px, no cropping used, but in the 400th iteration of the 1st epoch i get the error: `valueerror: could not broadcast input array from shape (3,164,256) into shape (3)` i'm using this line to begin training: `python train.py --dataroot ./datasets/my-images/ --name mygan --inputnc 1 --resizecrop scale_width --finesize 256`",Error,question
71,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/71,RuntimeError: CUDA error: initialization error,"thanks for the great work and sharing the code with us. i was able to get the toolbox running and generating some interesting voices. however, when i tried to train my own model. i got some cuda initialization error when initializing the encoder model. the error is as below. i know my torch(1.0.1) and cuda(10) worked and gpu is 1080ti. any help will be greatly appreciated! file ""/app/encoder/inference.py"", line 32, in loadapply file ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 193, in _apply file ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 379, in convert runtimeerror: cuda error: initialization error",other,question
939,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/939,Can not find the three Pretrained model,"i intended to download these models, but find nothing. encoder\savedmodels\pretrained\pretrained.pt vocoder\saved_models\pretrained\pretrained.pt any guys know why? or can somebody just share them with me? thank you.",question,question
28,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/28,Link for youtube video wont work,"link to a youtube video in the readme wont work. when you click on picture in readme file youtube opens but it says that ""video is not available"".",other,other
640,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/640,Use venvs for program due to outdated dependancies,"i noticed this program has outdated dependancies. a few suggestions: - use a venv to isolate the packages from the main python instance ooooor... - update your dependancies thank you for your time, this looks awesome. (might be related to #638, #639)",deployment,other
231,https://github.com/JaidedAI/EasyOCR/issues/231,Can i lower the accurate in Easyocr?,"the easyocr is really a great job! but in my job ,i want to lower the accurate,how can i do that? my job is to extract subtitles from a movie,like this: thanks a lot!",question,question
1030,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1030,Citation Issue,"hi jun-yan, @junyanz i love your cyclegan paper and have used your code-base to create a cyclegan that can synthesize dolphin whistle spectrograms. also, i'm finishing up my thesis writing and wanted to cite your github repo specifically your . could you please guide me on how to cite this? thank you.",other,other
612,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/612,"error with ""--crop_size"" with pix2pix model","hello there, i have a paired dataset with just 400 images, many of them with a very high resolution (6000x6000 or more). training with cyclegan and ""--cropsize 300` i get this: `# crop_size should be smaller than the size of loaded image assertionerror` without this parameter my images are resized. my smallest image is 320x320px. what do you think it could be?",question,question
238,https://github.com/JaidedAI/EasyOCR/issues/238,How to train your own model,"hello author, i would like to ask how you trained the latin model",question,question
522,https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/522,Tanh in the Generator last activation,thanks for your work. i am wondering why are you using tanh in the last activation of the generator? thnks again,question,other
1070,https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1070,VoxCeleb 1/2 dataset gone (takendown?),"the dataset was available before february, went to ""temporary unavailable"" then the whole dataset vanished from the internet, no one talks about the disappearance of a such important dataset in the ml community, no records of it, some info i can find on the archive search engine was some deepfake related to facial impersonation with the fbi. seems its wiped out from the internet.",other,other
