{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i have something to share with you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"/home/notebook/data/group/privacy/models/pretrained/models/bert-base-uncased\"\n",
    "t5_path = \"/home/notebook/data/group/privacy/models/pretrained/models/t5-base\"\n",
    "gpt_path = \"/home/notebook/data/group/privacy/models/pretrained/models/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained(bert_path)\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(t5_path)\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(gpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have something to share with you. [SEP]  what a pity!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \" what a pity!\"\n",
    "concat_text = text + \" \" + tokenizer_bert.sep_token + \" \" + text1\n",
    "# concat_text = text + \" \" + text1\n",
    "concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2031,  2242,  2000,  3745,  2007,  2017,  1012,   102,\n",
       "          2054,  1037, 12063,   999,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_bert = tokenizer_bert(concat_text, truncation=True, max_length=64, padding='max_length',  return_tensors='pt', \n",
    "# add_special_tokens=True\n",
    "    # return_token_type_ids=True if \"token_type_ids\" in tokenizer_bert.model_input_names else False,\n",
    ")\n",
    "inputs_bert\n",
    "# inputs_bert[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/notebook/data/group/privacy/models/pretrained/models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2207, -0.3863,  0.1587, -0.3183,  0.1668]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(bert_path, config=bert_path+\"/config.json\", num_labels=5, ignore_mismatched_sizes=True)\n",
    "model.eval()\n",
    "model(**inputs_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have something to share with you. </s>  what a pity!'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \" what a pity!\"\n",
    "concat_text = text\n",
    "# concat_text = text + \" \" + tokenizer_bert.sep_token + \" \" + text1\n",
    "concat_text = text + \" </s> \" + text1\n",
    "# concat_text = text + \" \" + text1\n",
    "concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  3,  23,  43, 424,  12, 698,  28,  25,   5,   1, 125,   3,   9,   3,\n",
       "         102, 485,  55,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs_t5 = tokenizer_t5(text, truncation=True, max_length=28, padding='max_length',  return_tensors='pt',)\n",
    "inputs_t5 = tokenizer_t5(concat_text, truncation=True, max_length=28, padding='max_length',  return_tensors='pt')\n",
    "inputs_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have something to share with you. what a pity!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_t5.decode(inputs_t5.input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have something to share with you. <eos>  what a pity!'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \" what a pity!\"\n",
    "concat_text = text\n",
    "# concat_text = text + \" \" + tokenizer_bert.sep_token + \" \" + text1\n",
    "concat_text = text + \" </s> \" + text1\n",
    "concat_text = text + \" <eos> \" + text1\n",
    "# concat_text = text + \" \" + text1\n",
    "concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   72,   423,  1223,   284,  2648,   351,   345,    13,  1279,    68,\n",
       "           418,    29,   220,   644,   257, 26246,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_gpt2 = tokenizer_gpt2(concat_text, return_tensors='pt')\n",
    "inputs_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have something to share with you. <eos>  what a pity!'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt2.decode(inputs_gpt2.input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
